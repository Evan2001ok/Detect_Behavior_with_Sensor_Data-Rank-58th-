{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"},{"sourceId":12139340,"sourceType":"datasetVersion","datasetId":7645099},{"sourceId":12293285,"sourceType":"datasetVersion","datasetId":7748073},{"sourceId":12328761,"sourceType":"datasetVersion","datasetId":7771623},{"sourceId":12573306,"sourceType":"datasetVersion","datasetId":7932089},{"sourceId":12816527,"sourceType":"datasetVersion","datasetId":7899268},{"sourceId":12903166,"sourceType":"datasetVersion","datasetId":8164159},{"sourceId":240649816,"sourceType":"kernelVersion"},{"sourceId":246893721,"sourceType":"kernelVersion"},{"sourceId":251413288,"sourceType":"kernelVersion"},{"sourceId":470587,"sourceType":"modelInstanceVersion","modelInstanceId":379625,"modelId":398856},{"sourceId":517084,"sourceType":"modelInstanceVersion","modelInstanceId":407853,"modelId":400086}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# All sensor model","metadata":{}},{"cell_type":"markdown","source":"all sensor model1","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport kagglehub\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom tqdm.notebook import tqdm\nfrom torch.amp import autocast\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom scipy.spatial.transform import Rotation as R\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom collections import defaultdict\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import StratifiedGroupKFold\n\ndef remove_gravity_from_acc(acc_data, rot_data):\n    if isinstance(acc_data, pd.DataFrame):\n        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n    else:\n        acc_values = acc_data\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    else:\n        quat_values = rot_data\n    num_samples = acc_values.shape[0]\n    linear_accel = np.zeros_like(acc_values)\n    gravity_world = np.array([0, 0, 9.81])\n    for i in range(num_samples):\n        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n            linear_accel[i, :] = acc_values[i, :] \n            continue\n        try:\n            rotation = R.from_quat(quat_values[i])\n            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n        except ValueError:\n             linear_accel[i, :] = acc_values[i, :]\n    return linear_accel\n\ndef calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    else:\n        quat_values = rot_data\n    num_samples = quat_values.shape[0]\n    angular_vel = np.zeros((num_samples, 3))\n    for i in range(num_samples - 1):\n        q_t = quat_values[i]\n        q_t_plus_dt = quat_values[i+1]\n        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n            continue\n        try:\n            rot_t = R.from_quat(q_t)\n            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n            delta_rot = rot_t.inv() * rot_t_plus_dt\n            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n        except ValueError:\n            pass\n    return angular_vel\n\ndef calculate_angular_distance(rot_data):\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    else:\n        quat_values = rot_data\n    num_samples = quat_values.shape[0]\n    angular_dist = np.zeros(num_samples)\n    for i in range(num_samples - 1):\n        q1 = quat_values[i]\n        q2 = quat_values[i+1]\n        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n            angular_dist[i] = 0\n            continue\n        try:\n            r1 = R.from_quat(q1)\n            r2 = R.from_quat(q2)\n            relative_rotation = r1.inv() * r2\n            angle = np.linalg.norm(relative_rotation.as_rotvec())\n            angular_dist[i] = angle\n        except ValueError:\n            angular_dist[i] = 0 # В случае недействительных кватернионов\n            pass\n    return angular_dist\n\nclass CMIFeDataset(Dataset):\n    def __init__(self, data_path, config):\n        self.config = config\n        self.init_feature_names(data_path)\n        df = self.generate_features(pd.read_csv(data_path, usecols=set(self.use_cols) & set(self.raw_columns)))\n        self.generate_dataset(df)\n\n    def init_feature_names(self, data_path):\n        self.target_gestures = [\n            'Above ear - pull hair',\n            'Cheek - pinch skin',\n            'Eyebrow - pull hair',\n            'Eyelash - pull hair',\n            'Forehead - pull hairline',\n            'Forehead - scratch',\n            'Neck - pinch skin',\n            'Neck - scratch',\n        ]\n        self.non_target_gestures = [\n            'Write name on leg',\n            'Wave hello',\n            'Glasses on/off',\n            'Text on phone',\n            'Write name in air',\n            'Feel around in tray and pull out an object',\n            'Scratch knee/leg skin',\n            'Pull air toward your face',\n            'Drink from bottle/cup',\n            'Pinch knee/leg skin'\n        ]\n\n        self.acc_features = ['acc_mag', 'acc_mag_jerk', 'linear_acc_mag', 'linear_acc_mag_jerk']\n        self.rot_features = ['rot_angle', 'rot_angle_vel', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance']\n        self.old_imu_features = [\n            'acc_mag', 'rot_angle','acc_mag_jerk', 'rot_angle_vel',\n            'linear_acc_mag', 'linear_acc_mag_jerk',\n            'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance'\n        ]\n\n        self.extra_imu_features = self.config.get(\"imu_feats\", [])\n        self.imu_features = self.extra_imu_features.copy()\n        if self.config.get(\"add_imu_feat_default\", True):\n            if self.config.get(\"old_imu_feat\", True):\n                self.imu_features.extend(self.old_imu_features)\n            else:\n                self.imu_features.extend(self.acc_features)\n                self.imu_features.extend(self.rot_features)\n        self.er1_fearues = [\"er_x\", \"er_y\", \"er_z\"]\n        self.er2_fearues = ['er_r_xy', 'er_r_xz', 'er_r_yz', 'er_c_xy', 'er_c_xz', 'er_c_yz']\n        self.er_fearues = self.er1_fearues + self.er2_fearues\n        self.tof_mode = self.config.get(\"tof_mode\", \"stats\")\n        self.tof_region_stats = ['mean', 'std', 'min', 'max']\n        self.tof_cols = self.generate_tof_feature_names()\n\n        self.raw_columns = pd.read_csv(data_path, nrows=0).columns.tolist()\n        self.imu_acc_cols_base = ['acc_x', 'acc_y', 'acc_z', 'linear_acc_x', 'linear_acc_y', 'linear_acc_z'] if self.config.get(\"add_raw_acc\", False) else ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\n        self.imu_rot_cols_base = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n        self.imu_cols_base = self.imu_acc_cols_base + self.imu_rot_cols_base\n        self.imu_cols = list()\n        self.imu_channel_keys = defaultdict(list)\n        if self.config.get(\"add_imu_base\", True): \n            self.imu_cols.extend(self.imu_cols_base)\n            self.imu_channel_keys[\"acc\"] = self.imu_acc_cols_base\n            self.imu_channel_keys[\"rot\"] = self.imu_rot_cols_base\n        if self.config.get(\"add_imu_feats\", True): \n            self.imu_cols.extend(self.imu_features)\n            if self.config.get(\"split_imu_feat\", False):\n                if self.config.get(\"old_imu_feat\", True):\n                    assert False, \"split_imu_feat=True and old_imu_feat=True not supported\"\n                self.imu_channel_keys[\"acc_feat\"] = self.acc_features\n                self.imu_channel_keys[\"rot_feat\"] = self.rot_features\n            else:\n                if self.config.get(\"old_imu_feat\", True):\n                    self.imu_channel_keys[\"other\"].extend(self.old_imu_features)\n                else:\n                    self.imu_channel_keys[\"other\"].extend(self.acc_features)\n                    self.imu_channel_keys[\"other\"].extend(self.rot_features)\n        if self.config.get(\"add_imu_er_feats\", False): \n            self.imu_cols.extend(self.er_fearues)\n            if self.config.get(\"split_imu_feat\", False):\n                self.imu_channel_keys[\"er1_feat\"] = self.er1_fearues\n                self.imu_channel_keys[\"er2_feat\"] = self.er2_fearues\n            else:\n                self.imu_channel_keys[\"other\"].extend(self.er1_fearues)\n                self.imu_channel_keys[\"other\"].extend(self.er2_fearues)\n        self.flip_imu_cols = [f\"{col}_flip\" for col in self.imu_cols]\n        self.imu_channel_keys = {k: sorted(v) for k, v in self.imu_channel_keys.items()}\n        self.thm_cols = [c for c in self.raw_columns if c.startswith('thm_')]\n        self.thm_channel_keys = {k: [f\"thm_{k}\"] for k in range(1, 6)}\n        self.feature_cols = self.imu_cols + self.thm_cols + self.tof_cols\n        self.imu_dim = len(self.imu_cols)\n        self.thm_dim = len(self.thm_cols)\n        self.tof_dim = len(self.tof_cols)\n        self.base_cols = ['acc_x', 'acc_y', 'acc_z',\n                          'rot_x', 'rot_y', 'rot_z', 'rot_w',\n                          'sequence_id', 'subject', \n                          'sequence_type', 'gesture', 'orientation'] + [c for c in self.raw_columns if c.startswith('thm_')] + [f\"tof_{i}_v{p}\" for i in range(1, 6) for p in range(64)]\n        self.use_cols = self.base_cols + self.feature_cols\n        if self.config.get(\"return_flip_imu\", False):\n            self.use_cols.extend(self.flip_imu_cols)\n        self.fold_cols = ['subject', 'sequence_type', 'gesture', 'orientation', 'sequence_id']\n        if self.config.get(\"use_dg\", False):\n            self.dg_cols = ['adult_child', 'age', 'sex', 'handedness', 'shoulder_to_wrist_height', 'elbow_to_wrist_height']\n        self.global_imu_indices = {k: sorted([self.imu_cols.index(feat) for feat in feats]) for k, feats in self.imu_channel_keys.items()}\n        self.global_thm_indices = {k: sorted([self.thm_cols.index(key) for key in self.thm_channel_keys[k]]) for k in range(1, 6)}\n        self.global_tof_indices = {k: sorted([self.tof_cols.index(key) for key in self.tof_channel_keys[k]]) for k in range(1, 6)}\n            \n    def generate_tof_feature_names(self):\n        features = list()\n        self.tof_channel_keys = defaultdict(list)\n        if self.config.get(\"tof_raw\", False):\n            for i in range(1, 6):\n                features.extend([f\"tof_{i}_v{p}\" for p in range(64)])\n                self.tof_channel_keys[i].extend([f\"tof_{i}_v{p}\" for p in range(64)])\n        for i in range(1, 6):\n            if self.tof_mode != 0:\n                for stat in self.tof_region_stats:\n                    features.append(f'tof_{i}_{stat}')\n                    self.tof_channel_keys[i].append(f'tof_{i}_{stat}')\n                if self.tof_mode > 1:\n                    for r in range(self.tof_mode):\n                        for stat in self.tof_region_stats:\n                            features.append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n                            self.tof_channel_keys[i].append(f'tof{self.tof_mode}_{i}_region_{r}_{stat}')\n                if self.tof_mode == -1:\n                    for mode in [2, 4, 8, 16, 32]:\n                        for r in range(mode):\n                            for stat in self.tof_region_stats:\n                                features.append(f'tof{mode}_{i}_region_{r}_{stat}')\n                                self.tof_channel_keys[i].append(f'tof{mode}_{i}_region_{r}_{stat}')\n        return features\n\n    def compute_cross_axis_energy(self, df):\n        axes=['x', 'y', 'z']\n        features = {}\n        for axis in axes:\n            fft_result = fft(df[f'acc_{axis}'].values)\n            energy = np.sum(np.abs(fft_result)**2)\n            features[f\"er_{axis}\"] = energy\n        for i, axis1 in enumerate(axes):\n            for axis2 in axes[i+1:]:\n                features[f'er_r_{axis1}{axis2}'] = features[f'er_{axis1}'] / (features[f'er_{axis2}'] + 1e-6)\n        for i, axis1 in enumerate(axes):\n            for axis2 in axes[i+1:]:\n                features[f'er_c_{axis1}{axis2}'] = np.corrcoef(np.abs(fft(df[f'acc_{axis1}'].values)), np.abs(fft(df[f'acc_{axis2}'].values)))[0, 1]\n        return {k: v for k, v in features.items() if k in self.er_fearues}\n\n    def compute_imu_features(self, df):\n        if self.config.get(\"rot_fillna\", False):\n            df['rot_w'] = df['rot_w'].fillna(1)\n            df[['rot_x', 'rot_y', 'rot_z']] = df[['rot_x', 'rot_y', 'rot_z']].fillna(0)\n        df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n        df['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n        df['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n        df['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n            \n        linear_accel_list = []\n        for _, group in df.groupby('sequence_id'):\n            acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n            linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n            linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n        df_linear_accel = pd.concat(linear_accel_list)\n        df = pd.concat([df, df_linear_accel], axis=1)\n        df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n        df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n    \n        angular_vel_list = []\n        for _, group in df.groupby('sequence_id'):\n            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n            angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n            angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n        df_angular_vel = pd.concat(angular_vel_list)\n        df = pd.concat([df, df_angular_vel], axis=1)\n    \n        angular_distance_list = []\n        for _, group in df.groupby('sequence_id'):\n            rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n            angular_dist_group = calculate_angular_distance(rot_data_group)\n            angular_distance_list.append(pd.DataFrame(angular_dist_group, columns=['angular_distance'], index=group.index))\n        df_angular_distance = pd.concat(angular_distance_list)\n        df = pd.concat([df, df_angular_distance], axis=1)\n        return df\n\n    def compute_flip_features(self, df):\n        flip_df = df[['sequence_id', 'acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']].copy()\n        flip_df[['acc_x', 'acc_y', 'rot_x', 'rot_y']] *= -1\n        flip_df = self.compute_imu_features(flip_df)\n        for col in flip_df.columns:\n            if col != 'sequence_id':\n                df[f\"{col}_flip\"] = flip_df[col]\n        return df\n\n    def compute_features(self, df):\n        df = self.compute_imu_features(df)\n        if self.tof_mode != 0:\n            new_columns = {}\n            for i in range(1, 6):\n                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n                tof_data = df[pixel_cols].replace(-1, np.nan)\n                new_columns.update({\n                    f'tof_{i}_mean': tof_data.mean(axis=1),\n                    f'tof_{i}_std': tof_data.std(axis=1),\n                    f'tof_{i}_min': tof_data.min(axis=1),\n                    f'tof_{i}_max': tof_data.max(axis=1)\n                })\n                if self.tof_mode > 1:\n                    region_size = 64 // self.tof_mode\n                    for r in range(self.tof_mode):\n                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n                        new_columns.update({\n                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n                        })\n                if self.tof_mode == -1:\n                    for mode in [2, 4, 8, 16, 32]:\n                        region_size = 64 // mode\n                        for r in range(mode):\n                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n                            new_columns.update({\n                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n                            })\n            df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n            \n        def _calc_features(group):\n            return pd.DataFrame(self.compute_cross_axis_energy(group), index=[group.index[0]])\n        features_df = df.groupby('sequence_id', group_keys=False).apply(_calc_features)\n        df = df.join(features_df, how='left')\n        df[features_df.columns] = df.groupby('sequence_id')[features_df.columns].ffill()\n        \n        return df\n        \n    def generate_features(self, df):\n        self.le = LabelEncoder()\n        if self.config.get(\"one_neg\", False):\n            neg_other = \"Write name on leg\"\n            df['gesture'] = df['gesture'].apply(lambda x: x if x in self.target_gestures else neg_other)\n        df['gesture_int'] = self.le.fit_transform(df['gesture'])\n        self.class_num = len(self.le.classes_)\n        self.target_ints = np.array([self.le.classes_.tolist().index(name) for name in self.target_gestures])\n        self.non_target_ints = np.array([self.le.classes_.tolist().index(name) for name in self.non_target_gestures])\n        \n        if all(c in df.columns for c in self.feature_cols):\n            print(\"Features have precomputed, skip compute.\")\n        else:\n            print(\"Features not precomputed, do compute.\")\n            df = self.compute_features(df)\n\n        if self.config.get(\"return_flip_imu\", False):\n            if all(c in df.columns for c in self.flip_imu_cols):\n                print(\"Flip have precomputed, skip compute.\")\n            else:\n                print(\"Flip not precomputed, do compute.\")\n                df = self.compute_flip_features(df)\n\n        if self.config.get(\"use_dg\", False):\n            dg_df = pd.read_csv(self.config[\"dg_path\"])\n            df = pd.merge(df, dg_df, how='left', on='subject')\n            df['age'] /= 100\n            df['shoulder_to_wrist_height'] = df['shoulder_to_wrist_cm'] / df['height_cm']\n            df['elbow_to_wrist_height'] = df['elbow_to_wrist_cm'] / df['height_cm']\n        \n        if self.config.get(\"save_precompute\", False):\n            df.to_csv(self.config.get(\"save_filename\", \"train.csv\"))\n        return df\n\n    def scale(self, data_unscaled):\n        scaler_function = self.config.get(\"scaler_function\", StandardScaler())\n        scaler = scaler_function.fit(np.concatenate(data_unscaled, axis=0))\n        return [scaler.transform(x) for x in data_unscaled], scaler\n\n    def pad(self, data_scaled, cols):\n        pad_data = np.zeros((len(data_scaled), self.pad_len, len(cols)), dtype='float32')\n        for i, seq in enumerate(data_scaled):\n            seq_len = min(len(seq), self.pad_len)\n            pad_data[i, :seq_len] = seq[:seq_len]\n        return pad_data\n\n    def get_nan_value(self, data, ratio):\n        max_value = data.max().max()\n        nan_value = -max_value * ratio\n        print(f\"Max: {max_value}, set nan to {nan_value}\")\n        return nan_value\n\n    def generate_dataset(self, df):\n        seq_gp = df.groupby('sequence_id') \n        imu_unscaled, thm_unscaled, tof_unscaled = list(), list(), list()\n        if self.config.get(\"return_flip_imu\", False): flip_imu_unscaled = list()\n        classes, lens = list(), list()\n        self.imu_nan_value = self.get_nan_value(df[self.imu_cols], self.config[\"nan_ratio\"][\"imu\"])\n        self.thm_nan_value = self.get_nan_value(df[self.thm_cols], self.config[\"nan_ratio\"][\"thm\"])\n        self.tof_nan_value = self.get_nan_value(df[self.tof_cols], self.config[\"nan_ratio\"][\"tof\"])\n        if self.config.get(\"use_dg\", False):\n            self.dg = list()\n\n        self.fold_feats = defaultdict(list)\n        for seq_id, seq_df in seq_gp:\n            imu_data = seq_df[self.imu_cols]\n            if self.config[\"fbfill\"][\"imu\"]:\n                imu_data = imu_data.ffill().bfill()\n            imu_unscaled.append(imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n\n            if self.config.get(\"return_flip_imu\", False):\n                flip_imu_data = seq_df[self.flip_imu_cols]\n                if self.config[\"fbfill\"][\"imu\"]:\n                    flip_imu_data = flip_imu_data.ffill().bfill()\n                flip_imu_unscaled.append(flip_imu_data.fillna(self.imu_nan_value).values.astype('float32'))\n\n            thm_data = seq_df[self.thm_cols]\n            if self.config[\"fbfill\"][\"thm\"]:\n                thm_data = thm_data.ffill().bfill()\n            thm_unscaled.append(thm_data.fillna(self.thm_nan_value).values.astype('float32'))\n\n            tof_data = seq_df[self.tof_cols]\n            if self.config[\"fbfill\"][\"tof\"]:\n                tof_data = tof_data.ffill().bfill()\n            tof_unscaled.append(tof_data.fillna(self.tof_nan_value).values.astype('float32'))\n            \n            classes.append(seq_df['gesture_int'].iloc[0])\n            lens.append(len(imu_data))\n\n            for col in self.fold_cols:\n                self.fold_feats[col].append(seq_df[col].iloc[0])\n\n            if self.config.get(\"use_dg\", False):\n                self.dg.append(seq_df[self.dg_cols].iloc[0].values.astype('float32'))\n            \n        self.dataset_indices = classes\n        self.pad_len = int(np.percentile(lens, self.config.get(\"percent\", 95)))\n        if self.config.get(\"one_scale\", True):\n            x_unscaled = [np.concatenate([imu, thm, tof], axis=1) for imu, thm, tof in zip(imu_unscaled, thm_unscaled, tof_unscaled)]\n            x_scaled, self.x_scaler = self.scale(x_unscaled)\n            x = self.pad(x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n            self.imu = x[..., :self.imu_dim]\n            self.thm = x[..., self.imu_dim:self.imu_dim+self.thm_dim]\n            self.tof = x[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n\n            if self.config.get(\"return_flip_imu\", False):\n                flip_x_unscaled = [np.concatenate([flip_imu, thm, tof], axis=1) for flip_imu, thm, tof in zip(flip_imu_unscaled, thm_unscaled, tof_unscaled)]\n                flip_x_scaled = [self.x_scaler.transform(x) for x in flip_x_unscaled]\n                flip_x = self.pad(flip_x_scaled, self.imu_cols+self.thm_cols+self.tof_cols)\n                self.flip_imu = flip_x[..., :self.imu_dim]\n        else:\n            imu_scaled, self.imu_scaler = self.scale(imu_unscaled)\n            thm_scaled, self.thm_scaler = self.scale(thm_unscaled)\n            tof_scaled, self.tof_scaler = self.scale(tof_unscaled)\n            self.imu = self.pad(imu_scaled, self.imu_cols)\n            self.thm = self.pad(thm_scaled, self.thm_cols)\n            self.tof = self.pad(tof_scaled, self.tof_cols)\n\n            if self.config.get(\"return_flip_imu\", False):\n                flip_imu_scaled = [self.imu_scaler.transform(x) for x in flip_imu_unscaled]\n                self.flip_imu = self.pad(flip_imu_scaled, self.imu_cols)\n        self.precompute_scaled_nan_values()\n        self.class_ = F.one_hot(torch.from_numpy(np.array(classes)).long(), num_classes=len(self.le.classes_)).float().numpy()\n        self.binary_class_ = np.isin(np.array(classes), self.target_ints).astype(np.float32)\n        self.class_weight = torch.FloatTensor(compute_class_weight('balanced', classes=np.arange(len(self.le.classes_)), y=classes))\n\n    def precompute_scaled_nan_values(self):\n        dummy_df = pd.DataFrame(\n            np.array([[self.imu_nan_value]*len(self.imu_cols) + \n                     [self.thm_nan_value]*len(self.thm_cols) +\n                     [self.tof_nan_value]*len(self.tof_cols)]),\n            columns=self.imu_cols + self.thm_cols + self.tof_cols\n        )\n        \n        if self.config.get(\"one_scale\", True):\n            scaled = self.x_scaler.transform(dummy_df)\n            self.imu_scaled_nan = scaled[0, :self.imu_dim].mean()\n            self.thm_scaled_nan = scaled[0, self.imu_dim:self.imu_dim+self.thm_dim].mean()\n            self.tof_scaled_nan = scaled[0, self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim].mean()\n        else:\n            self.imu_scaled_nan = self.imu_scaler.transform(dummy_df[self.imu_cols])[0].mean()\n            self.thm_scaled_nan = self.thm_scaler.transform(dummy_df[self.thm_cols])[0].mean()\n            self.tof_scaled_nan = self.tof_scaler.transform(dummy_df[self.tof_cols])[0].mean()\n\n    def get_scaled_nan_tensors(self, imu, thm, tof):\n        return torch.full(imu.shape, self.imu_scaled_nan, device=imu.device), \\\n            torch.full(thm.shape, self.thm_scaled_nan, device=thm.device), \\\n            torch.full(tof.shape, self.tof_scaled_nan, device=tof.device)\n\n    def inference_process(self, sequence, demographics=None, reverse=False):\n        if self.config.get(\"use_dg\", False):\n            assert demographics is not None, \"Demographics needed\"\n            df_dg = demographics.to_pandas().copy()\n            df_dg['age'] /= 100\n            df_dg['shoulder_to_wrist_height'] = df_dg['shoulder_to_wrist_cm'] / df_dg['height_cm']\n            df_dg['elbow_to_wrist_height'] = df_dg['elbow_to_wrist_cm'] / df_dg['height_cm']\n        df_seq = sequence.to_pandas().copy()\n        if reverse:\n            df_seq[['acc_x', 'acc_y', 'rot_x', 'rot_y']] *= -1\n        if self.config.get(\"rot_fillna\", False):\n            df_seq['rot_w'] = df_seq['rot_w'].fillna(1)\n            df_seq[['rot_x', 'rot_y', 'rot_z']] = df_seq[['rot_x', 'rot_y', 'rot_z']].fillna(0)\n        if not all(c in df_seq.columns for c in self.imu_features):\n            df_seq['acc_mag'] = np.sqrt(df_seq['acc_x']**2 + df_seq['acc_y']**2 + df_seq['acc_z']**2)\n            df_seq['rot_angle'] = 2 * np.arccos(df_seq['rot_w'].clip(-1, 1))\n            df_seq['acc_mag_jerk'] = df_seq['acc_mag'].diff().fillna(0)\n            df_seq['rot_angle_vel'] = df_seq['rot_angle'].diff().fillna(0)\n            if all(col in df_seq.columns for col in ['acc_x', 'acc_y', 'acc_z', 'rot_x', 'rot_y', 'rot_z', 'rot_w']):\n                linear_accel = remove_gravity_from_acc(\n                    df_seq[['acc_x', 'acc_y', 'acc_z']], \n                    df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n                )\n                df_seq[['linear_acc_x', 'linear_acc_y', 'linear_acc_z']] = linear_accel\n            else:\n                df_seq['linear_acc_x'] = df_seq.get('acc_x', 0)\n                df_seq['linear_acc_y'] = df_seq.get('acc_y', 0)\n                df_seq['linear_acc_z'] = df_seq.get('acc_z', 0)\n            df_seq['linear_acc_mag'] = np.sqrt(df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2)\n            df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n                angular_vel = calculate_angular_velocity_from_quat(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = angular_vel\n            else:\n                df_seq[['angular_vel_x', 'angular_vel_y', 'angular_vel_z']] = 0\n            if all(col in df_seq.columns for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']):\n                df_seq['angular_distance'] = calculate_angular_distance(df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n            else:\n                df_seq['angular_distance'] = 0\n\n        if self.tof_mode != 0:\n            new_columns = {} \n            for i in range(1, 6):\n                pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n                tof_data = df_seq[pixel_cols].replace(-1, np.nan)\n                new_columns.update({\n                    f'tof_{i}_mean': tof_data.mean(axis=1),\n                    f'tof_{i}_std': tof_data.std(axis=1),\n                    f'tof_{i}_min': tof_data.min(axis=1),\n                    f'tof_{i}_max': tof_data.max(axis=1)\n                })\n                if self.tof_mode > 1:\n                    region_size = 64 // self.tof_mode\n                    for r in range(self.tof_mode):\n                        region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n                        new_columns.update({\n                            f'tof{self.tof_mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_std': region_data.std(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_min': region_data.min(axis=1),\n                            f'tof{self.tof_mode}_{i}_region_{r}_max': region_data.max(axis=1)\n                        })\n                if self.tof_mode == -1:\n                    for mode in [2, 4, 8, 16, 32]:\n                        region_size = 64 // mode\n                        for r in range(mode):\n                            region_data = tof_data.iloc[:, r*region_size : (r+1)*region_size]\n                            new_columns.update({\n                                f'tof{mode}_{i}_region_{r}_mean': region_data.mean(axis=1),\n                                f'tof{mode}_{i}_region_{r}_std': region_data.std(axis=1),\n                                f'tof{mode}_{i}_region_{r}_min': region_data.min(axis=1),\n                                f'tof{mode}_{i}_region_{r}_max': region_data.max(axis=1)\n                            })\n            df_seq = pd.concat([df_seq, pd.DataFrame(new_columns)], axis=1)\n        \n        imu_unscaled = df_seq[self.imu_cols]\n        if self.config[\"fbfill\"][\"imu\"]:\n            imu_unscaled = imu_unscaled.ffill().bfill()\n        imu_unscaled = imu_unscaled.fillna(self.imu_nan_value).values.astype('float32')\n\n        thm_unscaled = df_seq[self.thm_cols]\n        if self.config[\"fbfill\"][\"thm\"]:\n            thm_unscaled = thm_unscaled.ffill().bfill()\n        thm_unscaled = thm_unscaled.fillna(self.thm_nan_value).values.astype('float32')\n\n        tof_unscaled = df_seq[self.tof_cols]\n        if self.config[\"fbfill\"][\"tof\"]:\n            tof_unscaled = tof_unscaled.ffill().bfill()\n        tof_unscaled = tof_unscaled.fillna(self.tof_nan_value).values.astype('float32')\n        \n        if self.config.get(\"one_scale\", True):\n            x_unscaled = np.concatenate([imu_unscaled, thm_unscaled, tof_unscaled], axis=1)\n            x_scaled = self.x_scaler.transform(x_unscaled)\n            imu_scaled = x_scaled[..., :self.imu_dim]\n            thm_scaled = x_scaled[..., self.imu_dim:self.imu_dim+self.thm_dim]\n            tof_scaled = x_scaled[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n        else:\n            imu_scaled = self.imu_scaler.transform(imu_unscaled)\n            thm_scaled = self.thm_scaler.transform(thm_unscaled)\n            tof_scaled = self.tof_scaler.transform(tof_unscaled)\n\n        combined = np.concatenate([imu_scaled, thm_scaled, tof_scaled], axis=1)\n        padded = np.zeros((self.pad_len, combined.shape[1]), dtype='float32')\n        seq_len = min(combined.shape[0], self.pad_len)\n        padded[:seq_len] = combined[:seq_len]\n        imu = padded[..., :self.imu_dim]\n        thm = padded[..., self.imu_dim:self.imu_dim+self.thm_dim]\n        tof = padded[..., self.imu_dim+self.thm_dim:self.imu_dim+self.thm_dim+self.tof_dim]\n\n        ret = [torch.from_numpy(imu).float().unsqueeze(0), torch.from_numpy(thm).float().unsqueeze(0), torch.from_numpy(tof).float().unsqueeze(0)]\n        if self.config.get(\"use_dg\", False):\n            dg = df_dg[self.dg_cols].values.astype('float32')\n            ret.append(torch.from_numpy(dg).float())\n        return ret\n\n    def split5(self, imu, thm, tof):\n        imus = [imu[:, :, self.global_imu_indices[k]] for k in self.global_imu_indices]\n        thms = [thm[:, :, self.global_thm_indices[k]] for k in range(1, 6)]\n        tofs = [tof[:, :, self.global_tof_indices[k]] for k in range(1, 6)]\n        return imus, thms, tofs\n\n    def slide(self, imu, thm, tof, ratio=1.0):\n        def slide_tensor(tensor, nan_value, ratio):\n            b, l, d = tensor.shape\n            length = int(l * ratio)\n            if length > l:\n                pad = torch.full((b, length-l, d), nan_value, device=tensor.device)\n                tensor = torch.cat([tensor, pad], dim=1)\n            elif length < l:\n                tensor = tensor[:, :length, :] \n            return tensor\n        return slide_tensor(imu, self.imu_scaled_nan, ratio), slide_tensor(thm, self.thm_scaled_nan, ratio), slide_tensor(tof, self.tof_scaled_nan, ratio)\n\n    def __getitem__(self, idx):\n        ret = [self.imu[idx], self.thm[idx], self.tof[idx], self.class_[idx], self.binary_class_[idx]]\n        if self.config.get(\"return_extra\", False):\n            fold_feat_info = [self.fold_feats[col][idx] for col in self.fold_cols]\n            ret.append((idx, fold_feat_info))\n        if self.config.get(\"use_dg\", False):\n            ret.append(self.dg[idx])\n        if self.config.get(\"return_flip_imu\", False):\n            ret.append(self.flip_imu[idx])\n        return ret\n\n    def __len__(self):\n        return len(self.class_)\n\nclass CMIFoldDataset:\n    def __init__(self, data_path, config, full_dataset_function, n_folds=5, random_seed=0):\n        self.full_dataset = full_dataset_function(data_path=data_path, config=config)\n        self.imu_dim = self.full_dataset.imu_dim\n        self.thm_dim = self.full_dataset.thm_dim\n        self.tof_dim = self.full_dataset.tof_dim\n        self.le = self.full_dataset.le\n        self.class_names = self.full_dataset.le.classes_\n        self.class_weight = self.full_dataset.class_weight\n        self.n_folds = n_folds\n        self.sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n        self.fold_y = np.array(self.full_dataset.fold_feats[config.get(\"fold_y\", \"sequence_type\")])\n        self.fold_groups = np.array(self.full_dataset.fold_feats[config.get(\"fold_groups\", \"subject\")])\n        self.folds = list(self.sgkf.split(X=np.arange(len(self.full_dataset)), y=self.fold_y, groups=self.fold_groups))\n        self.exclude_subjects = set(config.get(\"exclude_subjects\", []))\n    \n    def get_fold_datasets(self, fold_idx):\n        if self.folds is None or fold_idx >= self.n_folds: return None, None\n        fold_train_idx, fold_valid_idx = self.folds[fold_idx]\n        subjects = np.array(self.full_dataset.fold_feats[\"subject\"])\n        train_subjects, valid_subjects = subjects[fold_train_idx], subjects[fold_valid_idx]\n        train_mask, valid_mask = ~np.isin(train_subjects, list(self.exclude_subjects)), ~np.isin(valid_subjects, list(self.exclude_subjects))\n        return Subset(self.full_dataset, np.array(fold_train_idx)[train_mask].tolist()), Subset(self.full_dataset, np.array(fold_valid_idx)[valid_mask].tolist())\n\n    def print_fold_stats(self):\n        def get_label_counts(subset):\n            counts = {name: 0 for name in self.class_names}\n            if subset is None: return counts\n            for idx in subset.indices:\n                label_idx = self.full_dataset.dataset_indices[idx]\n                counts[self.class_names[label_idx]] += 1\n            return counts\n        \n        print(\"\\n交叉验证折叠统计:\")\n        for fold_idx in range(self.n_folds):\n            train_fold, valid_fold = self.get_fold_datasets(fold_idx)\n            train_counts = get_label_counts(train_fold)\n            valid_counts = get_label_counts(valid_fold)\n            print(f\"\\nFold {fold_idx + 1}:\")\n            print(f\"{'类别':<50} {'训练集':<10} {'验证集':<10}\")\n            for name in self.class_names:\n                print(f\"{name:<50} {train_counts[name]:<10} {valid_counts[name]:<10}\")\n\n        for fold_idx, (train_idx, val_idx) in enumerate(self.folds):\n            train_subjects = set(self.fold_groups[train_idx])\n            val_subjects = set(self.fold_groups[val_idx])\n            print(f\"\\nFold {fold_idx + 1}:\")\n            print(\"训练集受试者:\", train_subjects)\n            print(\"验证集受试者:\", val_subjects)\n\n        self.print_filtered_stats()\n\n    def print_filtered_stats(self):\n        original_counts = defaultdict(int)\n        filtered_counts = defaultdict(int)\n        \n        for fold_idx in range(self.n_folds):\n            train_idx, val_idx = self.folds[fold_idx]\n            for idx in train_idx:\n                original_counts['train'] += 1\n            for idx in val_idx:\n                original_counts['valid'] += 1\n            train_set, val_set = self.get_fold_datasets(fold_idx)\n            filtered_counts['train'] += len(train_set)\n            filtered_counts['valid'] += len(val_set)\n        \n        print(f\"\\n排除subject {self.exclude_subjects} 后的数据量变化:\")\n        print(f\"原始训练集样本: {original_counts['train']}\")\n        print(f\"过滤后训练集样本: {filtered_counts['train']}\")\n        print(f\"原始验证集样本: {original_counts['valid']}\") \n        print(f\"过滤后验证集样本: {filtered_counts['valid']}\")\n\nclass SEBlock(nn.Module):\n    def __init__(self, channels, reduction = 8):\n        super().__init__()\n        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: (B, C, L)\n        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n        return x * se                \n\nclass ResNetSEBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, wd = 1e-4):\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_channels, out_channels,\n                               kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm1d(out_channels)\n        self.conv2 = nn.Conv1d(out_channels, out_channels,\n                               kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm1d(out_channels)\n        # SE\n        self.se = SEBlock(out_channels)\n        \n        if in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n                          padding=0, bias=False),\n                nn.BatchNorm1d(out_channels)\n            )\n        else:\n            self.shortcut = nn.Identity()\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x) :\n        identity = self.shortcut(x)              # (B, out, L)\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.se(out)                       # (B, out, L)\n        out = out + identity\n        return self.relu(out)\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, feature_dim):\n        super().__init__()\n        self.score_fn = nn.Linear(feature_dim, 1, bias=True)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        # x: (B, L, F)\n        score = torch.tanh(self.score_fn(x))     # (B, L, 1)\n        weights = self.softmax(score.squeeze(-1))# (B, L)\n        weights = weights.unsqueeze(-1)          # (B, L, 1)\n        context = x * weights                    # (B, L, F)\n        return context.sum(dim=1)                # (B, F)\n\nclass GaussianNoise(nn.Module):\n    \"\"\"Add Gaussian noise to input tensor\"\"\"\n    def __init__(self, stddev):\n        super().__init__()\n        self.stddev = stddev\n    \n    def forward(self, x):\n        if self.training:\n            noise = torch.randn_like(x) * self.stddev\n            return x + noise\n        return x\n\nclass CMIBackbone(nn.Module):\n    def __init__(self, imu_dim, thm_dim, tof_dim, **kwargs):\n        super().__init__()\n        self.imu_acc_branch = nn.Sequential(\n            self.residual_feature_block(3, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n        )\n        self.imu_rot_branch = nn.Sequential(\n            self.residual_feature_block(4, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n        )\n        self.imu_other_branch = nn.Sequential(\n            self.residual_feature_block(imu_dim-7, kwargs[\"imu1_channels\"], kwargs[\"imu1_layers\"], drop=kwargs[\"imu1_dropout\"]),\n            self.residual_feature_block(kwargs[\"imu1_channels\"], kwargs[\"imu2_channels\"], kwargs[\"imu2_layers\"], drop=kwargs[\"imu2_dropout\"])\n        )\n\n        self.thm_branch1, self.tof_branch1 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n        self.thm_branch2, self.tof_branch2 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n        self.thm_branch3, self.tof_branch3 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n        self.thm_branch4, self.tof_branch4 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n        self.thm_branch5, self.tof_branch5 = self.init_thm_tof_branch(thm_dim//5, tof_dim//5, **kwargs)\n\n        self.imu_proj = ResNetSEBlock(in_channels=3*kwargs[\"imu2_channels\"], out_channels=kwargs[\"imu2_channels\"])\n        self.thm_proj = ResNetSEBlock(in_channels=5*kwargs[\"thm2_channels\"], out_channels=kwargs[\"thm2_channels\"])\n        self.tof_proj = ResNetSEBlock(in_channels=5*kwargs[\"tof2_channels\"], out_channels=kwargs[\"tof2_channels\"])\n\n        self.lstm = nn.LSTM(\n            input_size=kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'],\n            hidden_size=kwargs['lstm_hidden_size'],\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.gru = nn.GRU(\n            input_size=kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'],\n            hidden_size=kwargs['gru_hidden_size'],\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True\n        )\n        \n        self.noise = GaussianNoise(kwargs['gaussian_noise_rate'])\n        self.dense = nn.Sequential(\n            nn.Linear(kwargs['imu2_channels']+kwargs['thm2_channels']+kwargs['tof2_channels'], kwargs['dense_channels']),\n            nn.ELU()\n        )\n        \n        self.attn = AttentionLayer(feature_dim=(kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'])  # lstm + gru + dense\n\n    def feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3):\n        return nn.Sequential(\n            *[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm1d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.MaxPool1d(pool_size, ceil_mode=True),\n            nn.Dropout(drop)\n        )\n\n    def residual_feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3):\n        return nn.Sequential(\n            *[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n            ResNetSEBlock(in_channels, out_channels, wd=1e-4),\n            nn.MaxPool1d(pool_size, ceil_mode=True),\n            nn.Dropout(drop)\n        )\n\n    def init_thm_tof_branch(self, thm_dim, tof_dim, **kwargs):\n        thm_branch = nn.Sequential(\n            self.feature_block(thm_dim, kwargs[\"thm1_channels\"], kwargs[\"thm1_layers\"], drop=kwargs[\"thm1_dropout\"]),\n            self.feature_block(kwargs[\"thm1_channels\"], kwargs[\"thm2_channels\"], kwargs[\"thm2_layers\"], drop=kwargs[\"thm2_dropout\"]),\n        )\n        tof_branch = nn.Sequential(\n            self.feature_block(tof_dim, kwargs[\"tof1_channels\"], kwargs[\"tof1_layers\"], drop=kwargs[\"tof1_dropout\"]),\n            self.feature_block(kwargs[\"tof1_channels\"], kwargs[\"tof2_channels\"], kwargs[\"tof2_layers\"], drop=kwargs[\"tof2_dropout\"]),\n        )\n        return thm_branch, tof_branch\n    \n    def forward(self, imus, thms, tofs):\n        imu_acc, imu_rot, imu_other = imus\n        imu_acc_feat = self.imu_acc_branch(imu_acc.permute(0, 2, 1))\n        imu_rot_feat = self.imu_rot_branch(imu_rot.permute(0, 2, 1))\n        imu_other_feat = self.imu_other_branch(imu_other.permute(0, 2, 1))\n        imu_feat = self.imu_proj(torch.cat([imu_acc_feat, imu_rot_feat, imu_other_feat], dim=1))\n        \n        thm1, thm2, thm3, thm4, thm5 = thms\n        tof1, tof2, tof3, tof4, tof5 = tofs\n        \n        thm1_feat = self.thm_branch1(thm1.permute(0, 2, 1))\n        thm2_feat = self.thm_branch2(thm2.permute(0, 2, 1))\n        thm3_feat = self.thm_branch3(thm3.permute(0, 2, 1))\n        thm4_feat = self.thm_branch4(thm4.permute(0, 2, 1))\n        thm5_feat = self.thm_branch5(thm5.permute(0, 2, 1))\n        thm_feat = self.thm_proj(torch.cat([thm1_feat, thm2_feat, thm3_feat, thm4_feat, thm5_feat], dim=1))\n        \n        tof1_feat = self.tof_branch1(tof1.permute(0, 2, 1))\n        tof2_feat = self.tof_branch2(tof2.permute(0, 2, 1))\n        tof3_feat = self.tof_branch3(tof3.permute(0, 2, 1))\n        tof4_feat = self.tof_branch4(tof4.permute(0, 2, 1))\n        tof5_feat = self.tof_branch5(tof5.permute(0, 2, 1))\n        tof_feat = self.tof_proj(torch.cat([tof1_feat, tof2_feat, tof3_feat, tof4_feat, tof5_feat], dim=1))\n        \n        feat = torch.cat([imu_feat, thm_feat, tof_feat], dim=1).permute(0, 2, 1)\n        lstm_out, _ = self.lstm(feat)\n        gru_out, _ = self.gru(feat)\n        dense_out = self.dense(self.noise(feat))\n        \n        return self.attn(torch.cat([lstm_out, gru_out, dense_out], dim=-1))\n\n\nCUDA0 = \"cuda:0\"\nseed = 0\nbatch_size = 64\nnum_workers = 4\nn_folds = 5\n\nroot_dir = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\nuniverse_csv_path = Path(\"/kaggle/input/cmi-precompute/pytorch/all/1/tof-1_raw.csv\")\n\nimu_only = False\n\ndeterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\ndeterministic.init_all(seed)\n\n\ndef init_dataset():\n    dataset_config = {\n        \"percent\": 99,\n        \"scaler_config\": StandardScaler(),\n        \"nan_ratio\": {\n            \"imu\": 0,\n            \"thm\": 0,\n            \"tof\": 0,\n        },\n        \"fbfill\": {\n            \"imu\": True,\n            \"thm\": True,\n            \"tof\": True,\n        },\n        \"one_scale\": False,\n        \"tof_raw\": True,\n        \"tof_mode\": 16,\n        \"save_precompute\": False,\n        \"fold_y\": \"gesture\",\n        \"fold_groups\": \"subject\",\n    }\n\n    dataset = CMIFoldDataset(universe_csv_path, dataset_config, full_dataset_function=CMIFeDataset, n_folds=n_folds, random_seed=seed)\n    dataset.print_fold_stats()\n    return dataset\n\ndef get_fold_dataset(dataset, fold):\n    _, valid_dataset = dataset.get_fold_datasets(fold)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n    return valid_loader\n\ndataset = init_dataset()\n\nclass CMIModel(nn.Module):\n    def __init__(self, target_classes_num, non_target_classes_num, **kwargs):\n        super().__init__()\n        self.backbone = CMIBackbone(dataset.imu_dim, dataset.thm_dim, dataset.tof_dim, **kwargs)\n        self.target_classifier = nn.Sequential(\n            nn.Linear((kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'], kwargs[\"cls_channels1\"]),\n            nn.BatchNorm1d(kwargs[\"cls_channels1\"]),\n            nn.ReLU(),\n            nn.Dropout(kwargs[\"cls_dropout1\"]),\n            nn.Linear(kwargs[\"cls_channels1\"], kwargs[\"cls_channels2\"]),\n            nn.BatchNorm1d(kwargs[\"cls_channels2\"]),\n            nn.ReLU(),\n            nn.Dropout(kwargs[\"cls_dropout2\"]),\n            nn.Linear(kwargs[\"cls_channels2\"], target_classes_num)\n        )\n        self.non_target_classifier = nn.Sequential(\n            nn.Linear((kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'], kwargs[\"cls_channels1\"]),\n            nn.BatchNorm1d(kwargs[\"cls_channels1\"]),\n            nn.ReLU(),\n            nn.Dropout(kwargs[\"cls_dropout1\"]),\n            nn.Linear(kwargs[\"cls_channels1\"], kwargs[\"cls_channels2\"]),\n            nn.BatchNorm1d(kwargs[\"cls_channels2\"]),\n            nn.ReLU(),\n            nn.Dropout(kwargs[\"cls_dropout2\"]),\n            nn.Linear(kwargs[\"cls_channels2\"], non_target_classes_num)\n        )\n    \n    def forward(self, imu, thm, tof):\n        feat = self.backbone(imu, thm, tof)\n        targets_y = self.target_classifier(feat)\n        non_targets_y = self.non_target_classifier(feat)\n        return torch.cat([targets_y, non_targets_y], dim=1)\n\nmodel_function = CMIModel\nmodel_args = {\"imu1_channels\": 128, \"imu2_channels\": 256, \"imu1_dropout\": 0.3, \"imu2_dropout\": 0.25,\n              \"imu1_layers\": 0, \"imu2_layers\": 0, \n              \"thm1_channels\": 32, \"thm2_channels\": 64, \"thm1_dropout\": 0.25, \"thm2_dropout\": 0.2,\n              \"thm1_layers\": 0, \"thm2_layers\": 0, \n              \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n              \"tof1_layers\": 0, \"tof2_layers\": 0, \n              \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n              \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,\n              \"target_classes_num\": 8, \"non_target_classes_num\": 10,}\nmodel_dir = Path(\"/kaggle/input/cmi-models-public/pytorch/base04/1\")\n\nmodel_dicts = [\n    {\n        \"model_function\": model_function,\n        \"model_args\": model_args,\n        \"model_path\": model_dir / f\"fold{fold}/best_ema.pt\",\n    } for fold in range(n_folds)\n]\n\ndef replace(k):\n    k = k.replace(\"_orig_mod.\", \"\")\n    return k\n\nmodels = list()\nfor model_dict in model_dicts:\n    model_function = model_dict[\"model_function\"]\n    model_args = model_dict[\"model_args\"]\n    model_path = model_dict[\"model_path\"]\n    model = model_function(**model_args).to(CUDA0)\n    state_dict = {replace(k): v for k,v in torch.load(model_path).items()}\n    model.load_state_dict(state_dict)\n    model = model.eval()\n    models.append(model)\n\n\nmetric_package = kagglehub.package_import('wasupandceacar/cmi-metric')\n\nmetric = metric_package.Metric()\nimu_only_metric = metric_package.Metric()\n\ndef to_cuda(*tensors):\n    return [tensor.to(CUDA0) for tensor in tensors]\n\ndef inference(model, imu, thm, tof):\n    imus, thms, tofs = dataset.full_dataset.split5(imu, thm, tof)\n    with autocast(device_type='cuda'):\n        pred_y = model(imus, thms, tofs)\n    return pred_y\n\ndef valid(model, valid_bar):\n    with torch.no_grad():\n        for imu, thm, tof, y, b in valid_bar:\n            imu, thm, tof, y = to_cuda(imu, thm, tof, y)\n            pred_y = inference(model, imu, thm, tof)\n            metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[pred_y.argmax(dim=1).cpu()])\n            _, thm, tof = dataset.full_dataset.get_scaled_nan_tensors(imu, thm, tof)\n            pred_y = inference(model, imu, thm, tof)\n            imu_only_metric.add(dataset.le.classes_[y.argmax(dim=1).cpu()], dataset.le.classes_[pred_y.argmax(dim=1).cpu()])\n\nfor fold, model in enumerate(models):\n    valid_loader = get_fold_dataset(dataset, fold)\n    valid_bar = tqdm(valid_loader, desc=f\"Valid\", leave=False)\n    valid(model, valid_bar)\n\nprint(f\"\"\"\nNormal score: {metric.score()}\nIMU only score: {imu_only_metric.score()}\n\"\"\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:33:40.360761Z","iopub.execute_input":"2025-09-02T14:33:40.361004Z","iopub.status.idle":"2025-09-02T14:36:53.263659Z","shell.execute_reply.started":"2025-09-02T14:33:40.360986Z","shell.execute_reply":"2025-09-02T14:36:53.26293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"PyTorch model's label classes:\")\nprint(dataset.le.classes_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:36:53.265157Z","iopub.execute_input":"2025-09-02T14:36:53.265408Z","iopub.status.idle":"2025-09-02T14:36:53.270372Z","shell.execute_reply.started":"2025-09-02T14:36:53.265383Z","shell.execute_reply":"2025-09-02T14:36:53.269663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======== BEGIN: drop-in replacement for avg_predict / predict2 / predict3 ========\nimport torch\nimport torch.nn.functional as F\nfrom contextlib import nullcontext\n\n\nAMP_CTX = (torch.amp.autocast('cuda') if torch.cuda.is_available() else nullcontext())\n\ndef _short_err(e, k=800):\n    s = f\"{type(e).__name__}: {str(e)}\"\n    return s[:k]\n\ndef avg_predict(models, imu, thm, tof):\n    \"\"\"\n    安全的集成平均：\n    - 不再硬编码 cuda；有 GPU 时自动使用 AMP；\n    - 统一每个子模型输出为 (1, C)；\n    - 子模型出错只打印短日志并用零 logits 兜底，不向外 raise（防止 gRPC metadata 超限）。\n    \"\"\"\n    outputs = []\n    try:\n        with AMP_CTX:\n            for model in models:\n                try:\n                    y = inference(model, imu, thm, tof)   # 你的原有前向函数\n                    # 兼容多输出/列表输出\n                    if isinstance(y, (list, tuple)):\n                        y = y[0]\n                    # 非 tensor -> tensor\n                    if not torch.is_tensor(y):\n                        y = torch.as_tensor(y, device=imu.device)\n                    # (C,) -> (1, C)\n                    if y.ndim == 1:\n                        y = y.unsqueeze(0)\n                    # 设备对齐\n                    if y.device != imu.device:\n                        y = y.to(imu.device, non_blocking=True)\n                    outputs.append(y)\n                except Exception as e:\n                    print(f\"[WARN] submodel failed: {_short_err(e)}\")\n                    C = len(getattr(dataset.le, 'classes_', [])) if hasattr(dataset, 'le') and hasattr(dataset.le, 'classes_') else 18\n                    outputs.append(torch.zeros((1, C), device=imu.device))\n        if not outputs:\n            C = len(getattr(dataset.le, 'classes_', [])) if hasattr(dataset, 'le') and hasattr(dataset.le, 'classes_') else 18\n            return torch.zeros((1, C), device=imu.device)\n        return torch.mean(torch.stack(outputs, dim=0), dim=0)  # (1, C)\n    except Exception as e:\n        print(f\"[ERROR] avg_predict failed: {_short_err(e)}\")\n        C = len(getattr(dataset.le, 'classes_', [])) if hasattr(dataset, 'le') and hasattr(dataset.le, 'classes_') else 18\n        return torch.zeros((1, C), device=imu.device)\n\ndef predict2(sequence: pl.DataFrame, demographics: pl.DataFrame):\n    \"\"\"第二个模型的预测函数（安全版）\"\"\"\n    try:\n        imu, thm, tof = dataset.full_dataset.inference_process(sequence)\n        with torch.no_grad():\n            # 仅在可用时移动到 GPU，避免 CPU 网关报错\n            if torch.cuda.is_available():\n                imu = imu.cuda(non_blocking=True); thm = thm.cuda(non_blocking=True); tof = tof.cuda(non_blocking=True)\n            # 可选 IMU-only 开关\n            if 'imu_only' in globals() and imu_only:\n                try:\n                    _, thm, tof = dataset.full_dataset.get_scaled_nan_tensors(imu, thm, tof)\n                except Exception as e:\n                    print(f\"[WARN] get_scaled_nan_tensors: {_short_err(e)}\")\n            # 集成预测\n            pred_y = avg_predict(models, imu, thm, tof)                  # (1, C)\n            probabilities = F.softmax(pred_y, dim=1).detach().cpu().numpy()\n        return probabilities  # (1, C)\n    except Exception as e:\n        print(f\"[ERROR] predict2 failed: {_short_err(e)}\")\n        C = len(getattr(dataset.le, 'classes_', [])) if hasattr(dataset, 'le') and hasattr(dataset.le, 'classes_') else 18\n        return np.ones((1, C), dtype=np.float32) / C\n\ndef predict3(sequence: pl.DataFrame, demographics: pl.DataFrame):\n    \"\"\"第三个模型：IMU-only 推理（安全版）\"\"\"\n    try:\n        imu, thm, tof = dataset.full_dataset.inference_process(sequence)\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                imu = imu.cuda(non_blocking=True); thm = thm.cuda(non_blocking=True); tof = tof.cuda(non_blocking=True)\n            # 强制 IMU-only：替换 THM/TOF 为占位 NaN 张量\n            try:\n                _, thm, tof = dataset.full_dataset.get_scaled_nan_tensors(imu, thm, tof)\n            except Exception as e:\n                print(f\"[WARN] get_scaled_nan_tensors: {_short_err(e)}\")\n            pred_y = avg_predict(models, imu, thm, tof)                  # (1, C)\n            probabilities = F.softmax(pred_y, dim=1).detach().cpu().numpy()\n        return probabilities  # (1, C)\n    except Exception as e:\n        print(f\"[ERROR] predict3 failed: {_short_err(e)}\")\n        C = len(getattr(dataset.le, 'classes_', [])) if hasattr(dataset, 'le') and hasattr(dataset.le, 'classes_') else 18\n        return np.ones((1, C), dtype=np.float32) / C\n# ======== END: drop-in replacement ========\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:36:53.271527Z","iopub.execute_input":"2025-09-02T14:36:53.271799Z","iopub.status.idle":"2025-09-02T14:36:53.290028Z","shell.execute_reply.started":"2025-09-02T14:36:53.27177Z","shell.execute_reply":"2025-09-02T14:36:53.289246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"all sensor model2","metadata":{}},{"cell_type":"code","source":"\"\"\"gated-gru-hybrid-ensemble-v02.ipynb\n\n    https://colab.research.google.com/drive/15f-PUIU6Tc6qYWYP6g7trekz1LypFFwW\n\"\"\"\n\nimport os\nimport json\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport warnings\nimport random\nimport math\nimport matplotlib.pyplot as plt\nimport polars as pl\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import (\n    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n    Bidirectional, GRU, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n    Lambda, Concatenate\n)\nfrom tensorflow.keras.optimizers import Adam as AdamTF\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers.schedules import CosineDecay\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam as AdamTorch\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom scipy.spatial.transform import Rotation as R\nfrom scipy.signal import firwin\n\n# 評価メトリクスはローカル検証/学習時にのみインポート\ntry:\n    from cmi_2025_metric_copy_for_import import CompetitionMetric\nexcept ImportError:\n    CompetitionMetric = None\n    print(\"CompetitionMetric could not be imported. OOF/CV score will not be calculated.\")\n\ndef seed_everything(seed=42):\n    \"\"\"\n    実行環境の乱数シードを統一的に設定する関数。\n    \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(2025)\n    tf.random.set_seed(seed)\n    tf.experimental.numpy.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    # torch.backends.cudnn.deterministic = True # パフォーマンスが低下する可能性があるためコメントアウト\n    # torch.backends.cudnn.benchmark = False\n\nseed_everything(seed=42)\nwarnings.filterwarnings(\"ignore\")\n\nTRAIN = False\n\n# --- パス設定 ---\nRAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n# YOUR_MODELS_DIRは自分の学習済みモデルが格納されているKaggleデータセットのパスに設定してください\nYOUR_MODELS_DIR = Path(\"/kaggle/input/cmi-data-gated-gru\") # ★★★ 自分のモデルパスに変更 ★★★\nPUBLIC_TF_MODEL_DIR = Path(\"/kaggle/input/lb-0-78-quaternions-tf-bilstm-gru-attention\")\nPUBLIC_PT_MODEL_DIR = Path(\"/kaggle/input/cmi3-models-p\")\nEXPORT_DIR = Path(\"./\") # 学習済みモデルやアーティファクトの保存先\n\n# --- モデル学習ハイパーパラメータ ---\nBATCH_SIZE = 64          # バッチサイズ\nPAD_PERCENTILE = 95      # シーケンス長のパディングを決めるためのパーセンタイル値\nLR_INIT = 4e-4           # 学習率の初期値 (微調整)\nWD = 3e-3                # Weight Decay（L2正則化）の係数\nMIXUP_ALPHA = 0.4        # Mixupのα値\nEPOCHS = 360             # 最大エポック数 (増加)\nPATIENCE = 50            # EarlyStoppingのpatience (増加)\nN_SPLITS = 10             # クロスバリデーションの分割数\nMASKING_PROB = 0.25      # 学習時にTOF/THMデータをマスクする確率\nGATE_LOSS_WEIGHT = 0.2   # Gatedモデルのゲート損失に対する重み\n\nprint(f\"▶ ライブラリのインポート完了\")\nprint(f\"  - TensorFlow: {tf.__version__}\")\nprint(f\"  - PyTorch: {torch.__version__}\")\nprint(f\"▶ TRAINモード: {TRAIN}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# PyTorchモデル用の標準化パラメータ\nmean_pt = torch.tensor([\n    0, 0, 0, 0, 0, 0, 9.0319e-03, 1.0849e+00, -2.6186e-03, 3.7651e-03,\n    -5.3660e-03, -2.8177e-03, 1.3318e-03, -1.5876e-04, 6.3495e-01,\n    6.2877e-01, 6.0607e-01, 6.2142e-01, 6.3808e-01, 6.5420e-01,\n    7.4102e-03, -3.4159e-03, -7.5237e-03, -2.6034e-02, 2.9704e-02,\n    -3.1546e-02, -2.0610e-03, -4.6986e-03, -4.7216e-03, -2.6281e-02,\n    1.5799e-02, 1.0016e-02\n], dtype=torch.float32).view(1, -1, 1).to(device)\n\nstd_pt = torch.tensor([\n    1, 1, 1, 1, 1, 1, 0.2067, 0.8583, 0.3162,\n    0.2668, 0.2917, 0.2341, 0.3023, 0.3281, 1.0264, 0.8838, 0.8686, 1.0973,\n    1.0267, 0.9018, 0.4658, 0.2009, 0.2057, 1.2240, 0.9535, 0.6655, 0.2941,\n    0.3421, 0.8156, 0.6565, 1.1034, 1.5577\n], dtype=torch.float32).view(1, -1, 1).to(device) + 1e-8\n\nclass ImuFeatureExtractor(nn.Module):\n    \"\"\"\n    ★★★ PyTorchモデル用の特徴量抽出器 ★★★\n    公開モデルの重みと一致させるため、元の正しい定義に修正。\n    \"\"\"\n    def __init__(self, fs=100., add_quaternion=False):\n        super().__init__()\n        self.fs = fs\n        self.add_quaternion = add_quaternion\n\n        k = 15\n\n        # ▼▼▼【ここが修正点】▼▼▼\n        # 公開モデルの重みファイルに存在する 'self.lpf' 層を再度追加する\n        self.lpf = nn.Conv1d(6, 6, kernel_size=k, padding=k//2,\n                                 groups=6, bias=False)\n        nn.init.kaiming_uniform_(self.lpf.weight, a=math.sqrt(5))\n        # ▲▲▲【ここまでが修正点】▲▲▲\n\n        self.lpf_acc  = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n        self.lpf_gyro = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n\n    def forward(self, imu):\n        acc  = imu[:, 0:3, :]\n        gyro = imu[:, 3:6, :]\n\n        # 1) magnitude\n        acc_mag  = torch.norm(acc,  dim=1, keepdim=True)\n        gyro_mag = torch.norm(gyro, dim=1, keepdim=True)\n\n        # 2) jerk\n        jerk = F.pad(acc[:, :, 1:] - acc[:, :, :-1], (1,0))\n        gyro_delta = F.pad(gyro[:, :, 1:] - gyro[:, :, :-1], (1,0))\n\n        # 3) energy\n        acc_pow  = acc ** 2\n        gyro_pow = gyro ** 2\n\n        # 4) LPF / HPF\n        # self.lpf は forwardパスでは使われていないが、重み読み込みのために定義が必要\n        acc_lpf  = self.lpf_acc(acc)\n        acc_hpf  = acc - acc_lpf\n        gyro_lpf = self.lpf_gyro(gyro)\n        gyro_hpf = gyro - gyro_lpf\n\n        features = [\n            acc, gyro,\n            acc_mag, gyro_mag,\n            jerk, gyro_delta,\n            acc_pow, gyro_pow,\n            acc_lpf, acc_hpf,\n            gyro_lpf, gyro_hpf,\n        ]\n        return torch.cat(features, dim=1)\n\nclass SEBlock(nn.Module):\n    def __init__(self, channels, reduction=8):\n        super().__init__()\n        self.squeeze = nn.AdaptiveAvgPool1d(1)\n        self.excitation = nn.Sequential(\n            nn.Linear(channels, channels // reduction, bias=False), nn.ReLU(inplace=True),\n            nn.Linear(channels // reduction, channels, bias=False), nn.Sigmoid()\n        )\n    def forward(self, x):\n        b, c, _ = x.size()\n        y = self.squeeze(x).view(b, c)\n        y = self.excitation(y).view(b, c, 1)\n        return x * y.expand_as(x)\n\nclass ResidualSECNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, pool_size=2, dropout=0.3):\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n        self.bn1 = nn.BatchNorm1d(out_channels)\n        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n        self.bn2 = nn.BatchNorm1d(out_channels)\n        self.se = SEBlock(out_channels)\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut = nn.Sequential(nn.Conv1d(in_channels, out_channels, 1, bias=False), nn.BatchNorm1d(out_channels))\n        self.pool = nn.MaxPool1d(pool_size)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out = self.se(out)\n        out += self.shortcut(x)\n        return self.dropout(self.pool(F.relu(out)))\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.attention = nn.Linear(hidden_dim, 1)\n    def forward(self, x):\n        scores = torch.tanh(self.attention(x))\n        weights = F.softmax(scores.squeeze(-1), dim=1)\n        return torch.sum(x * weights.unsqueeze(-1), dim=1)\n\nclass TwoBranchModel(nn.Module):\n    def __init__(self, pad_len, imu_dim_raw, tof_dim, n_classes, dropouts=[0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.3], feature_engineering=True, **kwargs):\n        super().__init__()\n        self.feature_engineering = feature_engineering\n        imu_dim = 32 if feature_engineering else imu_dim_raw\n        self.imu_fe = ImuFeatureExtractor(**kwargs) if feature_engineering else nn.Identity()\n        self.fir_nchan = 7\n        numtaps = 33\n        fir_kernel = torch.tensor(firwin(numtaps, cutoff=1.0, fs=10.0, pass_zero=False), dtype=torch.float32).view(1, 1, -1).repeat(self.fir_nchan, 1, 1)\n        self.register_buffer(\"fir_kernel\", fir_kernel)\n        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=dropouts[0])\n        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=dropouts[1])\n        self.tof_conv1 = nn.Conv1d(tof_dim, 64, 3, padding=1, bias=False)\n        self.tof_bn1, self.tof_pool1, self.tof_drop1 = nn.BatchNorm1d(64), nn.MaxPool1d(2), nn.Dropout(dropouts[2])\n        self.tof_conv2 = nn.Conv1d(64, 128, 3, padding=1, bias=False)\n        self.tof_bn2, self.tof_pool2, self.tof_drop2 = nn.BatchNorm1d(128), nn.MaxPool1d(2), nn.Dropout(dropouts[3])\n        self.bilstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n        self.lstm_dropout = nn.Dropout(dropouts[4])\n        self.attention = AttentionLayer(256)\n        self.dense1, self.bn_dense1, self.drop1 = nn.Linear(256, 256, bias=False), nn.BatchNorm1d(256), nn.Dropout(dropouts[5])\n        self.dense2, self.bn_dense2, self.drop2 = nn.Linear(256, 128, bias=False), nn.BatchNorm1d(128), nn.Dropout(dropouts[6])\n        self.classifier = nn.Linear(128, n_classes)\n\n    def forward(self, x):\n        imu_raw = x[:, :, :self.fir_nchan].transpose(1, 2)\n        tof = x[:, :, self.fir_nchan:].transpose(1, 2)\n        imu_fe = self.imu_fe(imu_raw)\n        filtered = F.conv1d(imu_fe[:, :self.fir_nchan, :], self.fir_kernel, padding=self.fir_kernel.shape[-1] // 2, groups=self.fir_nchan)\n        imu = (torch.cat([filtered, imu_fe[:, self.fir_nchan:, :]], dim=1) - mean_pt) / std_pt\n        x1 = self.imu_block1(imu); x1 = self.imu_block2(x1)\n        x2 = self.tof_drop1(self.tof_pool1(F.relu(self.tof_bn1(self.tof_conv1(tof)))))\n        x2 = self.tof_drop2(self.tof_pool2(F.relu(self.tof_bn2(self.tof_conv2(x2)))))\n        merged = torch.cat([x1, x2], dim=1).transpose(1, 2)\n        lstm_out, _ = self.bilstm(merged); lstm_out = self.lstm_dropout(lstm_out)\n        attended = self.attention(lstm_out)\n        x = self.drop1(F.relu(self.bn_dense1(self.dense1(attended))))\n        x = self.drop2(F.relu(self.bn_dense2(self.dense2(x))))\n        return self.classifier(x)\n\nclass PublicTwoBranchModel(nn.Module):\n    \"\"\"\n    ★★★ 公開されているPyTorchモデル（モデル群C）を読み込むための、元のアーキテクチャを持つクラス ★★★\n    \"\"\"\n    def __init__(self, pad_len, imu_dim_raw, tof_dim, n_classes, dropouts=[0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.3], feature_engineering=True, **kwargs):\n        super().__init__()\n        self.feature_engineering = feature_engineering\n        imu_dim = 32 if feature_engineering else imu_dim_raw\n        self.imu_fe = ImuFeatureExtractor(**kwargs) if feature_engineering else nn.Identity()\n        self.fir_nchan = 7\n        numtaps = 33\n        fir_kernel = torch.tensor(firwin(numtaps, cutoff=1.0, fs=10.0, pass_zero=False), dtype=torch.float32).view(1, 1, -1).repeat(self.fir_nchan, 1, 1)\n        self.register_buffer(\"fir_kernel\", fir_kernel)\n        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=dropouts[0])\n        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=dropouts[1])\n        self.tof_conv1 = nn.Conv1d(tof_dim, 64, 3, padding=1, bias=False)\n        self.tof_bn1, self.tof_pool1, self.tof_drop1 = nn.BatchNorm1d(64), nn.MaxPool1d(2), nn.Dropout(dropouts[2])\n        self.tof_conv2 = nn.Conv1d(64, 128, 3, padding=1, bias=False)\n        self.tof_bn2, self.tof_pool2, self.tof_drop2 = nn.BatchNorm1d(128), nn.MaxPool1d(2), nn.Dropout(dropouts[3])\n        self.bilstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True) # GRUではなくLSTM\n        self.lstm_dropout = nn.Dropout(dropouts[4])\n        self.attention = AttentionLayer(256) # 128*2 for bidirectional\n        self.dense1, self.bn_dense1, self.drop1 = nn.Linear(256, 256, bias=False), nn.BatchNorm1d(256), nn.Dropout(dropouts[5])\n        self.dense2, self.bn_dense2, self.drop2 = nn.Linear(256, 128, bias=False), nn.BatchNorm1d(128), nn.Dropout(dropouts[6])\n        self.classifier = nn.Linear(128, n_classes)\n\n    def forward(self, x):\n        imu_raw = x[:, :, :self.fir_nchan].transpose(1, 2)\n        tof = x[:, :, self.fir_nchan:].transpose(1, 2)\n        imu_fe = self.imu_fe(imu_raw)\n        filtered = F.conv1d(imu_fe[:, :self.fir_nchan, :], self.fir_kernel, padding=self.fir_kernel.shape[-1] // 2, groups=self.fir_nchan)\n        # mean_pt, std_pt は事前に定義されているグローバル変数\n        imu = (torch.cat([filtered, imu_fe[:, self.fir_nchan:, :]], dim=1) - mean_pt) / std_pt\n        x1 = self.imu_block1(imu); x1 = self.imu_block2(x1)\n        x2 = self.tof_drop1(self.tof_pool1(F.relu(self.tof_bn1(self.tof_conv1(tof)))))\n        x2 = self.tof_drop2(self.tof_pool2(F.relu(self.tof_bn2(self.tof_conv2(x2)))))\n        merged = torch.cat([x1, x2], dim=1).transpose(1, 2)\n        lstm_out, _ = self.bilstm(merged); lstm_out = self.lstm_dropout(lstm_out)\n        attended = self.attention(lstm_out)\n        x = self.drop1(F.relu(self.bn_dense1(self.dense1(attended))))\n        x = self.drop2(F.relu(self.bn_dense2(self.dense2(x))))\n        return self.classifier(x)\n\ndef pad_sequences_torch3(sequences, maxlen, padding='post', truncating='post', value=0.0):\n    result = []\n    for seq in sequences:\n        if len(seq) >= maxlen: seq = seq[:maxlen] if truncating == 'post' else seq[-maxlen:]\n        else:\n            pad_len = maxlen - len(seq)\n            pad_array = np.full((pad_len, seq.shape[1]), value)\n            seq = np.concatenate([seq, pad_array]) if padding == 'post' else np.concatenate([pad_array, seq])\n        result.append(seq)\n    return np.array(result, dtype=np.float32)\n\n# =============================================================================\n# ## 特徴量エンジニアリング関数\n# =============================================================================\ndef remove_gravity_from_acc3(acc_data, rot_data):\n    \"\"\"加速度データから重力成分を除去する\"\"\"\n    acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    linear_accel = np.zeros_like(acc_values)\n    gravity_world = np.array([0, 0, 9.81])\n    for i in range(len(acc_values)):\n        if np.all(np.isnan(quat_values[i])):\n            linear_accel[i, :] = acc_values[i, :]\n            continue\n        try:\n            rotation = R.from_quat(quat_values[i])\n            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n        except (ValueError, IndexError):\n            linear_accel[i, :] = acc_values[i, :]\n    return linear_accel\n\ndef calculate_angular_velocity_from_quat3(rot_data, time_delta=1/200):\n    \"\"\"クォータニオンから角速度を計算する\"\"\"\n    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    angular_vel = np.zeros((len(quat_values), 3))\n    for i in range(len(quat_values) - 1):\n        q_t, q_t_plus_dt = quat_values[i], quat_values[i+1]\n        if np.all(np.isnan(q_t)) or np.all(np.isnan(q_t_plus_dt)): continue\n        try:\n            rot_t = R.from_quat(q_t)\n            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n            delta_rot = rot_t.inv() * rot_t_plus_dt\n            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n        except (ValueError, IndexError): pass\n    return angular_vel\n\ndef calculate_angular_distance3(rot_data):\n    \"\"\"クォータニオンから角距離を計算する\"\"\"\n    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    angular_dist = np.zeros(len(quat_values))\n    for i in range(len(quat_values) - 1):\n        q1, q2 = quat_values[i], quat_values[i+1]\n        if np.all(np.isnan(q1)) or np.all(np.isnan(q2)): continue\n        try:\n            r1 = R.from_quat(q1)\n            r2 = R.from_quat(q2)\n            relative_rotation = r1.inv() * r2\n            angular_dist[i] = np.linalg.norm(relative_rotation.as_rotvec())\n        except (ValueError, IndexError): pass\n    return angular_dist\n\ndef time_sum(x): return K.sum(x, axis=1)\ndef squeeze_last_axis(x): return tf.squeeze(x, axis=-1)\ndef expand_last_axis(x): return tf.expand_dims(x, axis=-1)\n\ndef se_block(x, reduction=8):\n    \"\"\"Squeeze-and-Excitationブロック\"\"\"\n    ch = x.shape[-1]\n    se = GlobalAveragePooling1D()(x)\n    se = Dense(ch // reduction, activation='relu')(se)\n    se = Dense(ch, activation='sigmoid')(se)\n    se = Reshape((1, ch))(se)\n    return Multiply()([x, se])\n\ndef residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n    \"\"\"Residual SE-CNNブロック\"\"\"\n    shortcut = x\n    # 2層のConv1D\n    for _ in range(2):\n        x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n    # SEブロック\n    x = se_block(x)\n    # ショートカット接続\n    if shortcut.shape[-1] != filters:\n        shortcut = Conv1D(filters, 1, padding='same', use_bias=False, kernel_regularizer=l2(wd))(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n    x = add([x, shortcut])\n    x = Activation('relu')(x)\n    x = MaxPooling1D(pool_size)(x)\n    x = Dropout(drop)(x)\n    return x\n\ndef attention_layer(inputs):\n    \"\"\"アテンション層\"\"\"\n    score = Dense(1, activation='tanh')(inputs)\n    score = Lambda(squeeze_last_axis)(score)\n    weights = Activation('softmax')(score)\n    weights = Lambda(expand_last_axis)(weights)\n    context = Multiply()([inputs, weights])\n    context = Lambda(time_sum)(context)\n    return context\n\nclass GatedMixupGenerator(Sequence):\n    \"\"\"Mixupとセンサーマスキングを適用するデータジェネレータ\"\"\"\n    def __init__(self, X, y, batch_size, imu_dim, class_weight=None, alpha=0.2, masking_prob=0.0):\n        self.X, self.y, self.batch, self.imu_dim = X, y, batch_size, imu_dim\n        self.class_weight, self.alpha, self.masking_prob = class_weight, alpha, masking_prob\n        self.indices = np.arange(len(X))\n\n    def __len__(self):\n        return int(np.ceil(len(self.X) / self.batch))\n\n    def __getitem__(self, i):\n        idx = self.indices[i*self.batch:(i+1)*self.batch]\n        Xb, yb = self.X[idx].copy(), self.y[idx].copy()\n\n        sample_weights = np.ones(len(Xb), dtype='float32')\n        if self.class_weight:\n            sample_weights = np.array([self.class_weight.get(i, 1.0) for i in yb.argmax(axis=1)])\n\n        gate_target = np.ones(len(Xb), dtype='float32')\n        if self.masking_prob > 0:\n            for j in range(len(Xb)):\n                if np.random.rand() < self.masking_prob:\n                    Xb[j, :, self.imu_dim:] = 0\n                    gate_target[j] = 0.0\n\n        if self.alpha > 0:\n            lam = np.random.beta(self.alpha, self.alpha)\n            perm = np.random.permutation(len(Xb))\n            X_mix = lam * Xb + (1 - lam) * Xb[perm]\n            y_mix = lam * yb + (1 - lam) * yb[perm]\n            gate_target_mix = lam * gate_target + (1 - lam) * gate_target[perm]\n            sample_weights_mix = lam * sample_weights + (1 - lam) * sample_weights[perm]\n            return X_mix, {'main_output': y_mix, 'tof_gate': gate_target_mix}, sample_weights_mix\n\n        return Xb, {'main_output': yb, 'tof_gate': gate_target}, sample_weights\n\n    def on_epoch_end(self):\n        np.random.shuffle(self.indices)\n\ndef build_gated_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n    \"\"\"\n    自作のGated Two-Branchモデルを構築する関数。\n    [改良点] LSTMをGRUに変更、全結合層を1層追加。\n    \"\"\"\n    inp = Input(shape=(pad_len, imu_dim + tof_dim))\n    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n\n    # IMUブランチ (Deep)\n    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n\n    # TOF/THMブランチ (Light) with Gating\n    x2_base = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n    x2_base = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2_base)\n    x2_base = BatchNormalization()(x2_base); x2_base = Activation('relu')(x2_base)\n    x2_base = MaxPooling1D(2)(x2_base); x2_base = Dropout(0.2)(x2_base)\n\n    # Gating機構\n    gate_input = GlobalAveragePooling1D()(tof)\n    gate_input = Dense(16, activation='relu')(gate_input)\n    gate = Dense(1, activation='sigmoid', name='tof_gate')(gate_input)\n    x2 = Multiply()([x2_base, gate])\n\n    # ブランチのマージと後続層\n    merged = Concatenate()([x1, x2])\n    # ★改良点: LSTM -> GRU\n    x = Bidirectional(GRU(256, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n    x = Dropout(0.45)(x)\n    x = attention_layer(x)\n\n    # ★改良点: 全結合層を1層追加して表現力を向上\n    for units, drop in [(512, 0.5), (256, 0.4), (128, 0.3)]:\n        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Dropout(drop)(x)\n\n    out = Dense(n_classes, activation='softmax', name='main_output', kernel_regularizer=l2(wd))(x)\n\n    return Model(inputs=inp, outputs=[out, gate])\n\n# -----------------------------------------------------------------------------\n# ### 推論モード (`TRAIN = False`)\n# -----------------------------------------------------------------------------\n\nprint(\"▶ 推論モード開始 – 学習済みモデルとアーティファクトを読み込みます...\")\n\n# --- モデル群A (自作TF/Kerasモデル) の読み込み ---\nprint(\"  モデル群A (自作5-Fold Gated GRUモデル) を読み込み中...\")\nfinal_feature_cols_A = np.load(YOUR_MODELS_DIR / \"final_feature_cols.npy\", allow_pickle=True).tolist()\npad_len_A = int(np.load(YOUR_MODELS_DIR / \"sequence_maxlen.npy\"))\nscaler_A = joblib.load(YOUR_MODELS_DIR / \"scaler.pkl\")\ngesture_classes = np.load(YOUR_MODELS_DIR / \"gesture_classes.npy\", allow_pickle=True)\ncustom_objs_A = {'time_sum': time_sum, 'squeeze_last_axis': squeeze_last_axis, 'expand_last_axis': expand_last_axis,\n                 'se_block': se_block, 'residual_se_cnn_block': residual_se_cnn_block, 'attention_layer': attention_layer}\nmodels_A = [load_model(YOUR_MODELS_DIR / f\"final_model_fold_{f}.h5\", compile=False, custom_objects=custom_objs_A) for f in range(N_SPLITS)]\nprint(f\"  > {len(models_A)}個のモデルを正常に読み込みました。\")\n\n# --- モデル群B (公開TF/Kerasモデル) の読み込み ---\nprint(\"\\n  モデル群B (公開TF/Kerasモデル) を読み込み中...\")\nfinal_feature_cols_B = np.load(PUBLIC_TF_MODEL_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\npad_len_B = int(np.load(PUBLIC_TF_MODEL_DIR / \"sequence_maxlen.npy\"))\nscaler_B = joblib.load(PUBLIC_TF_MODEL_DIR / \"scaler.pkl\")\ncustom_objs_B = custom_objs_A # public modelも同じカスタムオブジェクトを使用\nmodel_B = load_model(PUBLIC_TF_MODEL_DIR / \"gesture_two_branch_mixup.h5\", compile=False, custom_objects=custom_objs_B)\nprint(\"  > 1個のモデルを正常に読み込みました。\")\n\n# --- モデル群C (公開PyTorchモデル) の読み込み ---\nprint(\"\\n  モデル群C (公開PyTorchモデル) を読み込み中...\")\nfinal_feature_cols_C = np.load(PUBLIC_PT_MODEL_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\npad_len_C = int(np.load(PUBLIC_PT_MODEL_DIR / \"sequence_maxlen.npy\"))\nscaler_C = joblib.load(PUBLIC_PT_MODEL_DIR / \"scaler.pkl\")\n\npt_models = []\nfor f in range(5):\n    checkpoint = torch.load(PUBLIC_PT_MODEL_DIR / f\"gesture_two_branch_fold{f}.pth\", map_location=device)\n    cfg = {'pad_len': checkpoint['pad_len'], 'imu_dim_raw': checkpoint['imu_dim'],\n           'tof_dim': checkpoint['tof_dim'], 'n_classes': checkpoint['n_classes']}\n    m = PublicTwoBranchModel(**cfg).to(device)\n    m.load_state_dict(checkpoint['model_state_dict'])\n    m.eval()\n    pt_models.append(m)\nprint(f\"  > {len(pt_models)}個のモデルを正常に読み込みました。\")\n\n# predict_4\n\n# --- `predict`関数の定義 ---\ndef predict4(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n    df_seq_orig = sequence.to_pandas()\n    df_seq_A = df_seq_orig.copy()\n    \n    linear_accel_A = remove_gravity_from_acc3(df_seq_A[['acc_x','acc_y','acc_z']], df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n    df_seq_A['linear_acc_x'], df_seq_A['linear_acc_y'], df_seq_A['linear_acc_z'] = linear_accel_A[:,0], linear_accel_A[:,1], linear_accel_A[:,2]\n    df_seq_A['linear_acc_mag'] = np.linalg.norm(linear_accel_A, axis=1)\n    df_seq_A['linear_acc_mag_jerk'] = df_seq_A['linear_acc_mag'].diff().fillna(0)\n    angular_vel_A = calculate_angular_velocity_from_quat3(df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n    df_seq_A['angular_vel_x'], df_seq_A['angular_vel_y'], df_seq_A['angular_vel_z'] = angular_vel_A[:,0], angular_vel_A[:,1], angular_vel_A[:,2]\n    df_seq_A['angular_distance'] = calculate_angular_distance3(df_seq_A[['rot_x','rot_y','rot_z','rot_w']])\n    for col in ['rot_x', 'rot_y', 'rot_z', 'rot_w']:\n        df_seq_A[f'{col}_diff'] = df_seq_A[col].diff().fillna(0)\n    cols_for_stats=['linear_acc_mag','linear_acc_mag_jerk','angular_distance']\n    for col in cols_for_stats:\n        df_seq_A[f'{col}_skew'], df_seq_A[f'{col}_kurt'] = df_seq_A[col].skew(), df_seq_A[col].kurtosis()\n    for i in range(1,6):\n        if f'tof_{i}_v0' in df_seq_A.columns:\n            pixel_cols=[f\"tof_{i}_v{p}\" for p in range(64)]; tof_data=df_seq_A[pixel_cols].replace(-1,np.nan)\n            df_seq_A[f'tof_{i}_mean'], df_seq_A[f'tof_{i}_std'], df_seq_A[f'tof_{i}_min'], df_seq_A[f'tof_{i}_max'] = tof_data.mean(axis=1),tof_data.std(axis=1),tof_data.min(axis=1),tof_data.max(axis=1)\n    tof_mean_cols=[f'tof_{i}_mean' for i in range(1,6) if f'tof_{i}_mean' in df_seq_A.columns]\n    if tof_mean_cols:\n        df_seq_A['tof_std_across_sensors']=df_seq_A[tof_mean_cols].std(axis=1)\n        df_seq_A['tof_range_across_sensors']=df_seq_A[tof_mean_cols].max(axis=1)-df_seq_A[tof_mean_cols].min(axis=1)\n    thm_cols=[f'thm_{i}' for i in range(1,6) if f'thm_{i}' in df_seq_A.columns]\n    if thm_cols:\n        df_seq_A['thm_std_across_sensors']=df_seq_A[thm_cols].std(axis=1)\n        df_seq_A['thm_range_across_sensors']=df_seq_A[thm_cols].max(axis=1)-df_seq_A[thm_cols].min(axis=1)\n    # (推論 A)\n    mat_A = df_seq_A[final_feature_cols_A].ffill().bfill().fillna(0).values.astype('float32')\n    mat_A = scaler_A.transform(mat_A)\n    pad_input_A = pad_sequences([mat_A], maxlen=pad_len_A, padding='post', dtype='float32')\n    preds_A_folds = [model.predict(pad_input_A, verbose=0)[0] for model in models_A]\n    avg_pred_A = np.mean(preds_A_folds, axis=0)\n\n    # --- 2. モデル群B (公開TFモデル) の予測 ---\n    df_seq_B = df_seq_orig.copy()\n    # (特徴量生成 B)\n    df_seq_B['acc_mag']=np.sqrt(df_seq_B['acc_x']**2+df_seq_B['acc_y']**2+df_seq_B['acc_z']**2)\n    df_seq_B['rot_angle']=2*np.arccos(df_seq_B['rot_w'].clip(-1,1))\n    df_seq_B['acc_mag_jerk']=df_seq_B['acc_mag'].diff().fillna(0)\n    df_seq_B['rot_angle_vel']=df_seq_B['rot_angle'].diff().fillna(0)\n    linear_accel_B=remove_gravity_from_acc3(df_seq_B,df_seq_B)\n    df_seq_B['linear_acc_x'],df_seq_B['linear_acc_y'],df_seq_B['linear_acc_z']=linear_accel_B[:,0],linear_accel_B[:,1],linear_accel_B[:,2]\n    df_seq_B['linear_acc_mag']=np.sqrt(df_seq_B['linear_acc_x']**2+df_seq_B['linear_acc_y']**2+df_seq_B['linear_acc_z']**2)\n    df_seq_B['linear_acc_mag_jerk']=df_seq_B['linear_acc_mag'].diff().fillna(0)\n    angular_vel_B=calculate_angular_velocity_from_quat3(df_seq_B)\n    df_seq_B['angular_vel_x'],df_seq_B['angular_vel_y'],df_seq_B['angular_vel_z']=angular_vel_B[:,0],angular_vel_B[:,1],angular_vel_B[:,2]\n    df_seq_B['angular_distance']=calculate_angular_distance3(df_seq_B)\n    for i in range(1,6):\n        if f'tof_{i}_v0' in df_seq_B.columns:\n            pixel_cols=[f\"tof_{i}_v{p}\" for p in range(64)]; tof_data=df_seq_B[pixel_cols].replace(-1,np.nan)\n            df_seq_B[f\"tof_{i}_mean\"],df_seq_B[f\"tof_{i}_std\"],df_seq_B[f\"tof_{i}_min\"],df_seq_B[f\"tof_{i}_max\"]=tof_data.mean(axis=1),tof_data.std(axis=1),tof_data.min(axis=1),tof_data.max(axis=1)\n    # (推論 B)\n    mat_B = df_seq_B[final_feature_cols_B].ffill().bfill().fillna(0).values.astype('float32')\n    mat_B = scaler_B.transform(mat_B)\n    pad_input_B = pad_sequences([mat_B], maxlen=pad_len_B, padding='post', dtype='float32')\n    pred_B = model_B.predict(pad_input_B, verbose=0)\n    if isinstance(pred_B, list): pred_B = pred_B[0]\n\n    # --- 3. モデル群C (公開PyTorchモデル) の予測 ---\n    df_seq_C = df_seq_orig.copy() # Cは特徴量生成が不要なため、コピーのみ\n    mat_C = df_seq_C[final_feature_cols_C].ffill().bfill().fillna(0).values.astype('float32')\n    mat_C = scaler_C.transform(mat_C)\n    pad_input_C = pad_sequences_torch3([mat_C], maxlen=pad_len_C, padding='pre', truncating='pre')\n    with torch.no_grad():\n        pt_input = torch.from_numpy(pad_input_C).to(device)\n        preds_C_folds = [model(pt_input) for model in pt_models]\n        avg_pred_C_logits = torch.mean(torch.stack(preds_C_folds), dim=0)\n        avg_pred_C = torch.softmax(avg_pred_C_logits, dim=1).cpu().numpy()\n\n    # --- 4. 加重平均による最終決定 ---\n\n    weights = {'A': 0.50, 'B': 0.20, 'C': 0.30}\n\n    final_pred_proba = (weights['A'] * avg_pred_A + weights['B'] * pred_B + weights['C'] * avg_pred_C)\n\n    return final_pred_proba","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:36:53.290958Z","iopub.execute_input":"2025-09-02T14:36:53.291212Z","iopub.status.idle":"2025-09-02T14:37:02.438412Z","shell.execute_reply.started":"2025-09-02T14:36:53.291195Z","shell.execute_reply":"2025-09-02T14:37:02.437724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"all sensor model3","metadata":{}},{"cell_type":"code","source":"import os, random, math\nimport pandas as pd, numpy as np, polars as pl\nfrom pathlib import Path\nfrom scipy.spatial.transform import Rotation as R\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import f1_score\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress ALL TensorFlow logs\nimport tensorflow as tf\ntf.get_logger().setLevel('ERROR')  # Only show errors \nfrom tensorflow.keras.utils import to_categorical, pad_sequences\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import (\n    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, \n    Dropout, Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply,\n    Reshape, Lambda, Concatenate, GRU, GaussianNoise\n)\nfrom tensorflow.keras.optimizers.schedules import CosineDecay, CosineDecayRestarts, ExponentialDecay\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nTRAIN = False\nBASE_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\nPRETRAINED_DIR = Path(\"/kaggle/input/artifact0\")\nEXPORT_DIR = Path(\"/kaggle/working\")\nSEED = 42\nBATCH_SIZE = 64\nPAD_PERCENTILE = 95\nLR_INIT = 5e-4\nWD = 3e-3\nMIXUP_ALPHA = 0.4\nEPOCHS = 160\nPATIENCE = 40\nN_SPLITS = 10\nMASKING_PROB = 0.35\nGATE_LOSS_WEIGHT = 0.2\nUSE_LR_SCHEDULER = True\nLR_SCHEDULE_TYPE = \"cosine_decay_restarts\"  # Options: \"cosine_decay\", \"cosine_decay_restarts\", \"exponential_decay\"\n\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    tf.experimental.numpy.random.seed(seed)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed_everything(SEED)\n\ndef remove_gravity_from_acc(acc_data, rot_data):\n    acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    linear_accel = np.zeros_like(acc_values)\n    gravity_world = np.array([0, 0, 9.81])\n    \n    for i in range(len(acc_values)):\n        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n            linear_accel[i, :] = acc_values[i, :]\n            continue\n        try:\n            rotation = R.from_quat(quat_values[i])\n            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n        except ValueError:\n            linear_accel[i, :] = acc_values[i, :]\n    return linear_accel\n\ndef calculate_angular_velocity_from_quat(rot_data, time_delta=1/200):\n    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    angular_vel = np.zeros((len(quat_values), 3))\n    \n    for i in range(len(quat_values) - 1):\n        q_t, q_t_plus_dt = quat_values[i], quat_values[i+1]\n        if np.all(np.isnan(q_t)) or np.all(np.isnan(q_t_plus_dt)):\n            continue\n        try:\n            rot_t = R.from_quat(q_t)\n            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n            delta_rot = rot_t.inv() * rot_t_plus_dt\n            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n        except ValueError:\n            pass\n    return angular_vel\n\ndef calculate_angular_distance(rot_data):\n    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    angular_dist = np.zeros(len(quat_values))\n    \n    for i in range(len(quat_values) - 1):\n        q1, q2 = quat_values[i], quat_values[i+1]\n        if np.all(np.isnan(q1)) or np.all(np.isnan(q2)):\n            continue\n        try:\n            r1, r2 = R.from_quat(q1), R.from_quat(q2)\n            relative_rotation = r1.inv() * r2\n            angular_dist[i] = np.linalg.norm(relative_rotation.as_rotvec())\n        except ValueError:\n            pass\n    return angular_dist\n\ndef cmi_metric(y_true_gestures, y_pred_gestures, bfrb_gestures=None):\n    y_true_gestures = np.array(y_true_gestures)\n    y_pred_gestures = np.array(y_pred_gestures)\n    \n    y_true_binary = np.array(['target' if gesture in bfrb_gestures else 'non_target' \n                             for gesture in y_true_gestures])\n    y_pred_binary = np.array(['target' if gesture in bfrb_gestures else 'non_target' \n                             for gesture in y_pred_gestures])\n    \n    binary_f1 = f1_score(y_true_binary, y_pred_binary, pos_label='target')\n    \n    y_true_collapsed = []\n    y_pred_collapsed = []\n    \n    for true_gesture, pred_gesture in zip(y_true_gestures, y_pred_gestures):\n        if true_gesture in bfrb_gestures:\n            y_true_collapsed.append(true_gesture)\n        else:\n            y_true_collapsed.append('non_target')\n            \n        if pred_gesture in bfrb_gestures:\n            y_pred_collapsed.append(pred_gesture)\n        else:\n            y_pred_collapsed.append('non_target')\n    \n    y_true_collapsed = np.array(y_true_collapsed)\n    y_pred_collapsed = np.array(y_pred_collapsed)\n    \n    macro_f1 = f1_score(y_true_collapsed, y_pred_collapsed, average='macro')\n    composite_score = (binary_f1 + macro_f1) / 2.0\n    \n    return {\n        'binary_f1': binary_f1,\n        'macro_f1': macro_f1, \n        'composite_score': composite_score\n    }\n\ndef evaluate_with_cmi_metric(model, X_val, y_val_gestures, gesture_classes, bfrb_gestures):\n    predictions = model.predict(X_val, verbose=0)[0]\n    pred_gesture_indices = predictions.argmax(axis=1)\n    pred_gestures = gesture_classes[pred_gesture_indices]\n    scores = cmi_metric(y_val_gestures, pred_gestures, bfrb_gestures)\n    return scores\n\n\ndef get_individual_gesture_scores(model, X_val, y_val_gestures, gesture_classes, bfrb_gestures):\n    \"\"\"Calculate F1 score for each individual gesture\"\"\"\n    predictions = model.predict(X_val, verbose=0)[0]\n    pred_gesture_indices = predictions.argmax(axis=1)\n    pred_gestures = gesture_classes[pred_gesture_indices]\n    \n    gesture_scores = {}\n    unique_gestures = np.unique(y_val_gestures)\n    \n    for gesture in unique_gestures:\n        y_true_binary = (y_val_gestures == gesture).astype(int)\n        y_pred_binary = (pred_gestures == gesture).astype(int)\n        \n        if y_true_binary.sum() > 0:\n            f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n            gesture_scores[gesture] = f1\n    \n    return gesture_scores\n\ndef evaluate_dual_cmi_metric(model, X_val, y_val_gestures, gesture_classes, bfrb_gestures, imu_dim):\n    \n    cmi_full = evaluate_with_cmi_metric(model, X_val, y_val_gestures, gesture_classes, bfrb_gestures)\n    \n    \n    X_val_imu_only = X_val.copy()\n    X_val_imu_only[:, :, imu_dim:] = 0.0\n    \n    cmi_imu = evaluate_with_cmi_metric(model, X_val_imu_only, y_val_gestures, gesture_classes, bfrb_gestures)\n    \n    realistic_composite = (cmi_full['composite_score'] + cmi_imu['composite_score']) / 2.0\n    realistic_binary = (cmi_full['binary_f1'] + cmi_imu['binary_f1']) / 2.0\n    realistic_macro = (cmi_full['macro_f1'] + cmi_imu['macro_f1']) / 2.0\n    \n    sensor_dependency = cmi_full['composite_score'] - cmi_imu['composite_score']\n    \n    return {\n        'composite_score': realistic_composite,\n        'binary_f1': realistic_binary, \n        'macro_f1': realistic_macro,\n        'full_sensor_composite': cmi_full['composite_score'],\n        'full_sensor_binary': cmi_full['binary_f1'],\n        'full_sensor_macro': cmi_full['macro_f1'],\n        'imu_only_composite': cmi_imu['composite_score'],\n        'imu_only_binary': cmi_imu['binary_f1'],\n        'imu_only_macro': cmi_imu['macro_f1'],\n        'sensor_dependency': sensor_dependency,\n        'performance_stability': 1.0 - (sensor_dependency / max(cmi_full['composite_score'], 0.01))\n    }\n\nclass IMUSpecificScaler:\n    def __init__(self):\n         self.imu_scaler = StandardScaler()\n         self.tof_scaler = StandardScaler()\n         self.imu_dim = None\n            \n    def fit(self, X, imu_dim):\n         self.imu_dim = imu_dim\n         self.imu_scaler.fit(X[:, :imu_dim])\n         self.tof_scaler.fit(X[:, imu_dim:])\n         return self\n            \n    def transform(self, X):\n         X_imu = self.imu_scaler.transform(X[:, :self.imu_dim])\n         X_tof = self.tof_scaler.transform(X[:, self.imu_dim:])\n         return np.concatenate([X_imu, X_tof], axis=1)\n        \nclass EnhancedCMIMetricCallback(tf.keras.callbacks.Callback):\n    def __init__(self, X_val, y_val_gestures, gesture_classes, bfrb_gestures, imu_dim, patience=40, verbose=1):\n        super().__init__()\n        self.X_val = X_val\n        self.y_val_gestures = y_val_gestures\n        self.gesture_classes = gesture_classes\n        self.bfrb_gestures = bfrb_gestures\n        self.imu_dim = imu_dim\n        self.patience = patience\n        self.verbose = verbose\n        self.best_score = -np.inf\n        self.wait = 0\n        self.best_weights = None\n        \n    def on_epoch_end(self, epoch, logs=None):\n        dual_scores = evaluate_dual_cmi_metric(\n            self.model, self.X_val, self.y_val_gestures, \n            self.gesture_classes, self.bfrb_gestures, self.imu_dim\n        )\n        \n        realistic_composite = dual_scores['composite_score']\n        \n        logs = logs or {}\n        logs['val_realistic_composite'] = realistic_composite\n        logs['val_full_sensor_composite'] = dual_scores['full_sensor_composite']\n        logs['val_imu_only_composite'] = dual_scores['imu_only_composite']\n        logs['val_sensor_dependency'] = dual_scores['sensor_dependency']\n        \n        # Silent progress\n        if self.verbose > 0:\n            print('.', end='', flush=True)\n        \n        if realistic_composite > self.best_score:\n            self.best_score = realistic_composite\n            self.wait = 0\n            self.best_weights = self.model.get_weights()\n        else:\n            self.wait += 1\n            \n        if self.wait >= self.patience:\n            self.model.stop_training = True\n            \n    def on_train_end(self, logs=None):\n        if self.best_weights is not None:\n            self.model.set_weights(self.best_weights)\n        \ndef create_detailed_confusion_analysis(models, X_val_all, y_val_all, le_classes, bfrb_gestures):\n    \"\"\"Create detailed confusion matrix and misclassification analysis\"\"\"\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.metrics import confusion_matrix\n    \n    # Get ensemble predictions\n    all_predictions = []\n    for model in models:\n        pred = model.predict(X_val_all, verbose=0)[0]\n        all_predictions.append(pred)\n    \n    ensemble_pred = np.mean(all_predictions, axis=0)\n    pred_classes = ensemble_pred.argmax(axis=1)\n    true_classes = y_val_all.argmax(axis=1)\n    \n    # Create confusion matrix\n    cm = confusion_matrix(true_classes, pred_classes)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"DETAILED CONFUSION MATRIX ANALYSIS\")\n    print(\"=\"*80)\n    \n    # 1. Overall confusion matrix visualization\n    plt.figure(figsize=(15, 12))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=[cls[:20] for cls in le_classes],\n                yticklabels=[cls[:20] for cls in le_classes])\n    plt.title('Gesture Confusion Matrix')\n    plt.ylabel('True Gesture')\n    plt.xlabel('Predicted Gesture')\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.savefig(EXPORT_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # 2. Detailed misclassification analysis\n    print(\"\\n1. TOP MISCLASSIFICATIONS:\")\n    print(\"-\" * 50)\n    \n    misclassifications = []\n    for i, true_class in enumerate(le_classes):\n        for j, pred_class in enumerate(le_classes):\n            if i != j and cm[i][j] > 0:\n                misclassifications.append({\n                    'true': true_class,\n                    'predicted': pred_class, \n                    'count': cm[i][j],\n                    'true_is_bfrb': true_class in bfrb_gestures,\n                    'pred_is_bfrb': pred_class in bfrb_gestures\n                })\n    \n    # Sort by count\n    misclassifications.sort(key=lambda x: x['count'], reverse=True)\n    \n    print(\"Most frequent misclassifications:\")\n    for i, mc in enumerate(misclassifications[:15]):\n        true_marker = \" \" if mc['true_is_bfrb'] else \"  \"\n        pred_marker = \" \" if mc['pred_is_bfrb'] else \"  \"\n        print(f\"{i+1:2d}. {true_marker} {mc['true'][:25]:25} → {pred_marker} {mc['predicted'][:25]:25} ({mc['count']} times)\")\n    \n    # 3. BFRB-specific confusion analysis\n    print(\"\\n2. BFRB GESTURE CONFUSION PATTERNS:\")\n    print(\"-\" * 50)\n    \n    bfrb_confusions = [mc for mc in misclassifications if mc['true_is_bfrb']]\n    \n    print(\"BFRB gestures confused with other BFRB gestures:\")\n    bfrb_to_bfrb = [mc for mc in bfrb_confusions if mc['pred_is_bfrb']]\n    for mc in bfrb_to_bfrb[:10]:\n        print(f\"   {mc['true'][:30]:30} →  {mc['predicted'][:30]:30} ({mc['count']}x)\")\n    \n    print(f\"\\nBFRB gestures confused with Non-BFRB gestures:\")\n    bfrb_to_non = [mc for mc in bfrb_confusions if not mc['pred_is_bfrb']]\n    for mc in bfrb_to_non[:10]:\n        print(f\"   {mc['true'][:30]:30} →    {mc['predicted'][:30]:30} ({mc['count']}x)\")\n    \n    # 4. Gesture-specific accuracy\n    print(\"\\n3. INDIVIDUAL GESTURE ACCURACY:\")\n    print(\"-\" * 50)\n    \n    gesture_accuracy = {}\n    for i, gesture in enumerate(le_classes):\n        total_true = cm[i].sum()\n        correct = cm[i][i]\n        accuracy = correct / total_true if total_true > 0 else 0\n        gesture_accuracy[gesture] = accuracy\n    \n    print(\"BFRB Gesture Accuracy:\")\n    bfrb_acc = {g: a for g, a in gesture_accuracy.items() if g in bfrb_gestures}\n    for gesture, acc in sorted(bfrb_acc.items(), key=lambda x: x[1]):\n        print(f\"  {gesture[:35]:35} {acc:.3f}\")\n    \n    print(f\"\\nNon-BFRB Gesture Accuracy (top 10):\")\n    non_bfrb_acc = {g: a for g, a in gesture_accuracy.items() if g not in bfrb_gestures}\n    for gesture, acc in sorted(non_bfrb_acc.items(), key=lambda x: x[1], reverse=True)[:10]:\n        print(f\"  {gesture[:35]:35} {acc:.3f}\")\n    \n    # 5. Actionable insights\n    print(\"\\n4. ACTIONABLE INSIGHTS:\")\n    print(\"-\" * 50)\n    \n    worst_bfrb = sorted(bfrb_acc.items(), key=lambda x: x[1])[:3]\n    print(\"PRIORITY: Worst performing BFRB gestures:\")\n    for gesture, acc in worst_bfrb:\n        main_confusions = [mc for mc in misclassifications if mc['true'] == gesture][:3]\n        print(f\"\\n  • {gesture} (accuracy: {acc:.3f})\")\n        print(\"    Most confused with:\")\n        for mc in main_confusions:\n            conf_type = \"BFRB\" if mc['pred_is_bfrb'] else \"Non-BFRB\"\n            print(f\"      - {mc['predicted']} ({conf_type}, {mc['count']}x)\")\n    \n    return cm, misclassifications, gesture_accuracy\ndef time_sum(x):\n    return tf.reduce_sum(x, axis=1)\n\ndef squeeze_last_axis(x):\n    return tf.squeeze(x, axis=-1)\n\ndef expand_last_axis(x):\n    return tf.expand_dims(x, axis=-1)\n\ndef se_block(x, reduction=8):\n    ch = x.shape[-1]\n    se = GlobalAveragePooling1D()(x)\n    se = Dense(ch // reduction, activation='relu')(se)\n    se = Dense(ch, activation='sigmoid')(se)\n    se = Reshape((1, ch))(se)\n    return Multiply()([x, se])\n\ndef residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n    shortcut = x\n    for _ in range(2):\n        x = Conv1D(filters, kernel_size, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n    x = se_block(x)\n    \n    if shortcut.shape[-1] != filters:\n        shortcut = Conv1D(filters, 1, padding='same', use_bias=False, kernel_regularizer=l2(wd))(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n    \n    x = add([x, shortcut])\n    x = Activation('relu')(x)\n    x = MaxPooling1D(pool_size)(x)\n    x = Dropout(drop)(x)\n    return x\n\ndef attention_layer(inputs):\n    score = Dense(1, activation='tanh')(inputs)\n    score = Lambda(squeeze_last_axis)(score)\n    weights = Activation('softmax')(score)\n    weights = Lambda(expand_last_axis)(weights)\n    context = Multiply()([inputs, weights])\n    context = Lambda(time_sum)(context)\n    return context\n\nclass GatedMixupGenerator(tf.keras.utils.Sequence):\n    def __init__(self, X, y, batch_size, imu_dim, class_weight=None, alpha=0.2, masking_prob=0.0):\n        self.X, self.y = X, y\n        self.batch = batch_size\n        self.imu_dim = imu_dim\n        self.class_weight = class_weight\n        self.alpha = alpha\n        self.masking_prob = masking_prob\n        self.indices = np.arange(len(X))\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.ceil(len(self.X) / self.batch))\n\n    def __getitem__(self, i):\n        idx = self.indices[i*self.batch:(i+1)*self.batch]\n        Xb, yb = self.X[idx].copy(), self.y[idx].copy()\n        \n        sample_weights = np.ones(len(Xb), dtype='float32')\n        if self.class_weight:\n            y_integers = yb.argmax(axis=1)\n            sample_weights = np.array([self.class_weight[i] for i in y_integers])\n        \n        gate_target = np.ones(len(Xb), dtype='float32')\n        if self.masking_prob > 0:\n            for i in range(len(Xb)):\n                if np.random.rand() < self.masking_prob:\n                    Xb[i, :, self.imu_dim:] = 0\n                    gate_target[i] = 0.0\n        \n        if self.alpha > 0:\n            lam = np.random.beta(self.alpha, self.alpha)\n            perm = np.random.permutation(len(Xb))\n            X_mix = lam * Xb + (1 - lam) * Xb[perm]\n            y_mix = lam * yb + (1 - lam) * yb[perm]\n            gate_target_mix = lam * gate_target + (1 - lam) * gate_target[perm]\n            sample_weights_mix = lam * sample_weights + (1 - lam) * sample_weights[perm]\n            return X_mix, {'main_output': y_mix, 'tof_gate': gate_target_mix}, sample_weights_mix\n        \n        return Xb, {'main_output': yb, 'tof_gate': gate_target}, sample_weights\n\n    def on_epoch_end(self):\n        np.random.shuffle(self.indices)\n\ndef build_gated_two_branch_model(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n    \n    x1 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n    \n    x2_base = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n    x2_base = BatchNormalization()(x2_base)\n    x2_base = Activation('relu')(x2_base)\n    x2_base = MaxPooling1D(2)(x2_base)\n    x2_base = Dropout(0.2)(x2_base)\n    \n    x2_base = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x2_base)\n    x2_base = BatchNormalization()(x2_base)\n    x2_base = Activation('relu')(x2_base)\n    x2_base = MaxPooling1D(2)(x2_base)\n    x2_base = Dropout(0.2)(x2_base)\n    \n    gate_input = GlobalAveragePooling1D()(tof)\n    gate_input = Dense(16, activation='relu')(gate_input)\n    gate = Dense(1, activation='sigmoid', name='tof_gate')(gate_input)\n    x2 = Multiply()([x2_base, gate])\n    \n    merged = Concatenate()([x1, x2])\n    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(merged)\n    xc = GaussianNoise(0.09)(merged)\n    xc = Dense(16, activation='elu')(xc)\n    x = Concatenate()([xa, xb, xc])\n    x = Dropout(0.4)(x)\n    x = attention_layer(x)\n    \n    for units, drop in [(256, 0.5), (128, 0.3)]:\n        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Dropout(drop)(x)\n    \n    out = Dense(n_classes, activation='softmax', name='main_output', kernel_regularizer=l2(wd))(x)\n    return Model(inputs=inp, outputs=[out, gate])\n \nif TRAIN:\n    print(\"----------TRAINING MODE---------\")\n    \n    train = pd.read_csv(BASE_DIR / \"train.csv\")\n    train_dem = pd.read_csv(BASE_DIR / \"train_demographics.csv\")\n    df = pd.merge(train, train_dem, on='subject', how='left')\n    \n    le = LabelEncoder()\n    df['gesture_int'] = le.fit_transform(df['gesture'])\n    np.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\n    print(\"Data loaded | Unique gestures:\", len(le.classes_))\n    \n    bfrb_gestures = [\n        'Above ear - pull hair',\n        'Forehead - pull hairline', \n        'Forehead - scratch',\n        'Eyebrow - pull hair',\n        'Eyelash - pull hair',\n        'Neck - pinch skin',\n        'Neck - scratch',\n        'Cheek - pinch skin'\n    ]\n    \n    print(\"Calculating physics-based features with sequence grouping...\")\n    \n    linear_accel_list = []\n    for _, group in df.groupby('sequence_id'):\n        linear_accel = remove_gravity_from_acc(\n            group[['acc_x', 'acc_y', 'acc_z']], \n            group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n        )\n        linear_accel_df = pd.DataFrame(\n            linear_accel,\n            columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'],\n            index=group.index\n        )\n        linear_accel_list.append(linear_accel_df)\n    \n    df = pd.concat([df, pd.concat(linear_accel_list)], axis=1)\n    df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n    df['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n    \n    angular_vel_list = []\n    for _, group in df.groupby('sequence_id'):\n        angular_vel = calculate_angular_velocity_from_quat(\n            group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n        )\n        angular_vel_df = pd.DataFrame(\n            angular_vel,\n            columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'],\n            index=group.index\n        )\n        angular_vel_list.append(angular_vel_df)\n    \n    df = pd.concat([df, pd.concat(angular_vel_list)], axis=1)\n    df['angular_vel_mag'] = np.sqrt(df['angular_vel_x']**2 + df['angular_vel_y']**2 + df['angular_vel_z']**2)\n    df['angular_vel_mag_jerk'] = df.groupby('sequence_id')['angular_vel_mag'].diff().fillna(0)\n    \n    angular_dist_list = []\n    for _, group in df.groupby('sequence_id'):\n        angular_dist = calculate_angular_distance(\n            group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n        )\n        angular_dist_df = pd.DataFrame(\n            angular_dist,\n            columns=['angular_distance'],\n            index=group.index\n        )\n        angular_dist_list.append(angular_dist_df)\n    \n    df = pd.concat([df, pd.concat(angular_dist_list)], axis=1)\n    df['gesture_rhythm_signature'] = df.groupby('sequence_id')['linear_acc_mag'].transform(\n        lambda x: x.rolling(5, min_periods=1).std() / (x.rolling(5, min_periods=1).mean() + 1e-6)\n    )\n    imu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z'] + [c for c in df.columns if c.startswith('rot_')]\n    imu_engineered = ['linear_acc_mag', 'linear_acc_mag_jerk', 'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_distance','angular_vel_mag','angular_vel_mag_jerk','gesture_rhythm_signature']\n    imu_cols = list(dict.fromkeys(imu_cols_base + imu_engineered))\n    \n    thm_cols = [c for c in df.columns if c.startswith('thm_')]\n    tof_agg_cols = []\n    for i in range(1, 6):\n        tof_agg_cols.extend([f'tof_{i}_mean', f'tof_{i}_std', f'tof_{i}_min', f'tof_{i}_max'])\n    \n    final_feature_cols = imu_cols + thm_cols + tof_agg_cols\n    imu_dim = len(imu_cols)\n    tof_thm_dim = len(thm_cols) + len(tof_agg_cols)\n    \n    print(f\"Feature dimensions: IMU={imu_dim} | TOF/THM={tof_thm_dim} | Total={len(final_feature_cols)}\")\n    np.save(EXPORT_DIR / \"feature_cols.npy\", np.array(final_feature_cols))\n    \n    print(\"Building sequences...\")\n    seq_gp = df.groupby('sequence_id')\n    X_list_unscaled, y_list, groups_list, lens = [], [], [], []\n    \n    for seq_id, seq_df in seq_gp:\n        seq_df_copy = seq_df.copy()\n        for i in range(1, 6):\n            pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n            tof_sensor_data = seq_df_copy[pixel_cols].replace(-1, np.nan)\n            seq_df_copy[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n            seq_df_copy[f'tof_{i}_std'] = tof_sensor_data.std(axis=1)\n            seq_df_copy[f'tof_{i}_min'] = tof_sensor_data.min(axis=1)\n            seq_df_copy[f'tof_{i}_max'] = tof_sensor_data.max(axis=1)\n        \n        mat_unscaled = seq_df_copy[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n        X_list_unscaled.append(mat_unscaled)\n        y_list.append(seq_df_copy['gesture_int'].iloc[0])\n        groups_list.append(seq_df_copy['subject'].iloc[0])\n        lens.append(len(mat_unscaled))\n    \n    print(\"Fitting IMU-Specific StandardScalers...\")\n    all_steps_concatenated = np.concatenate(X_list_unscaled, axis=0)\n    scaler = IMUSpecificScaler().fit(all_steps_concatenated, imu_dim)\n    joblib.dump(scaler, EXPORT_DIR / \"imu_specific_scaler.pkl\")\n    \n    print(\"Scaling and padding sequences with IMU-specific normalization...\")\n    X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n    del X_list_unscaled\n    \n    pad_len = int(np.percentile(lens, PAD_PERCENTILE))\n    np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n    \n    X = pad_sequences(X_scaled_list, maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n    del X_scaled_list\n    \n    y_stratify = np.array(y_list)\n    y = to_categorical(y_list, num_classes=len(le.classes_))\n    groups = np.array(groups_list)\n    \n    print(f\"Starting realistic {N_SPLITS}-fold training with dual evaluation...\")\n    sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n    fold_realistic_scores = []\n    fold_gesture_scores = []\n\n    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X, y_stratify, groups)):\n        print(f\"\\n{'='*50}\")\n        print(f\"FOLD {fold+1}/{N_SPLITS} - Training in progress\", end='')\n        \n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        y_val_gestures = le.classes_[y_stratify[val_idx]]\n\n        model = build_gated_two_branch_model(pad_len, imu_dim, tof_thm_dim, len(le.classes_), wd=WD)\n        # Learning rate scheduler setup\n        if USE_LR_SCHEDULER:\n            steps_per_epoch = len(X_train) // BATCH_SIZE\n            total_steps = steps_per_epoch * EPOCHS\n            \n            if LR_SCHEDULE_TYPE == \"cosine_decay\":\n                lr_schedule = CosineDecay(\n                    initial_learning_rate=LR_INIT,\n                    decay_steps=total_steps,\n                    alpha=0.01  # End at 1% of initial LR\n                )\n            elif LR_SCHEDULE_TYPE == \"cosine_decay_restarts\":\n                lr_schedule = CosineDecayRestarts(\n                    initial_learning_rate=LR_INIT,\n                    first_decay_steps=steps_per_epoch * 20,  # Restart every 20 epochs\n                    t_mul=1.2,  # Increase restart period by 20% each time\n                    m_mul=0.8,  # Reduce max LR by 20% each restart\n                    alpha=0.01  # Minimum LR as fraction of initial\n                )\n            elif LR_SCHEDULE_TYPE == \"exponential_decay\":\n                lr_schedule = ExponentialDecay(\n                    initial_learning_rate=LR_INIT,\n                    decay_steps=steps_per_epoch * 10,  # Decay every 10 epochs\n                    decay_rate=0.9,\n                    staircase=False\n                )\n            else:\n                lr_schedule = LR_INIT  # No scheduling\n            \n            optimizer = Adam(learning_rate=lr_schedule)\n        else:\n            optimizer = Adam(LR_INIT)\n        \n        \n        model.compile(\n            optimizer = optimizer,\n            loss={\n                'main_output': tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),  \n                'tof_gate': 'binary_crossentropy'\n            },\n            loss_weights={'main_output': 1.0, 'tof_gate': GATE_LOSS_WEIGHT},\n            metrics={'main_output': 'accuracy'}\n        )\n        \n        class_weights = compute_class_weight(\n            'balanced', \n            classes=np.arange(len(le.classes_)), \n            y=y_train.argmax(1)\n        )\n        class_weight_dict = dict(enumerate(class_weights))\n        \n        train_gen = GatedMixupGenerator(\n            X_train, y_train, BATCH_SIZE, imu_dim,\n            class_weight=class_weight_dict, alpha=MIXUP_ALPHA, masking_prob=MASKING_PROB\n        )\n        val_gen = GatedMixupGenerator(X_val, y_val, BATCH_SIZE, imu_dim)\n        enhanced_callback = EnhancedCMIMetricCallback(\n            X_val, y_val_gestures, le.classes_, bfrb_gestures, imu_dim,\n            patience=PATIENCE, verbose=1\n        )\n        \n        model.fit(\n            train_gen, validation_data=val_gen, epochs=EPOCHS,\n            callbacks=[enhanced_callback], verbose=0\n        )\n        \n        final_scores = evaluate_dual_cmi_metric(\n            model, X_val, y_val_gestures, le.classes_, bfrb_gestures, imu_dim\n        )\n        \n        gesture_scores = get_individual_gesture_scores(\n            model, X_val, y_val_gestures, le.classes_, bfrb_gestures\n        )\n        \n        print(f\"\\nFOLD {fold+1} COMPLETED ✓\")\n        print(f\"Composite: {final_scores['composite_score']:.4f} | \"\n              f\"Binary: {final_scores['binary_f1']:.4f} | \"\n              f\"Macro: {final_scores['macro_f1']:.4f}\")\n        print(f\"IMU-only: {final_scores['imu_only_composite']:.4f} | \"\n              f\"Full: {final_scores['full_sensor_composite']:.4f} | \"\n              f\"Gap: {final_scores['sensor_dependency']:.4f}\")\n        \n        print(\"\\nIndividual Gesture F1 Scores:\")\n        for gesture, score in sorted(gesture_scores.items(), key=lambda x: x[1], reverse=True):\n            print(f\"  {gesture[:30]:30} {score:.3f}\")\n        \n        fold_realistic_scores.append(final_scores)\n        fold_gesture_scores.append(gesture_scores)\n        \n        model.save(EXPORT_DIR / f\"gesture_model_fold_{fold}.h5\")\n        print(f\"Model saved: fold_{fold}.h5\")\n\n    print(\"\\n----Training Complete----\")\n    print(\"\\nAverage Results Across All Folds:\")\n    avg_scores = {\n        'composite_score': np.mean([s['composite_score'] for s in fold_realistic_scores]),\n        'macro_f1': np.mean([s['macro_f1'] for s in fold_realistic_scores]),\n        'binary_f1': np.mean([s['binary_f1'] for s in fold_realistic_scores]),\n        'imu_only_composite': np.mean([s['imu_only_composite'] for s in fold_realistic_scores]),\n        'full_sensor_composite': np.mean([s['full_sensor_composite'] for s in fold_realistic_scores]),\n        'sensor_dependency': np.mean([s['sensor_dependency'] for s in fold_realistic_scores])\n    }\n    print(f\"Actual Composite: {avg_scores['composite_score']:.4f}\")\n    print(f\"Macro: {avg_scores['macro_f1']:.4f}\")\n    print(f\"Binary: {avg_scores['binary_f1']:.4f}\")\n    print(f\"Imu: {avg_scores['imu_only_composite']:.4f}\")\n    print(f\"Full: {avg_scores['full_sensor_composite']:.4f}\")\n    print(f\"Sensor Gap: {avg_scores['sensor_dependency']:.4f}\")\n    print(\"\\nGenerating detailed confusion analysis...\")\n    X_val_all = []\n    y_val_all = []\n    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X, y_stratify, groups)):\n        X_val_all.append(X[val_idx])\n        y_val_all.append(y[val_idx])\n    X_val_combined = np.concatenate(X_val_all, axis=0)\n    y_val_combined = np.concatenate(y_val_all, axis=0)\n    \n    models = []\n    for fold in range(N_SPLITS):\n        model = load_model(EXPORT_DIR / f\"gesture_model_fold_{fold}.h5\", \n                           custom_objects={\n                               'time_sum': time_sum,\n                               'squeeze_last_axis': squeeze_last_axis,\n                               'expand_last_axis': expand_last_axis\n                           })\n        models.append(model)\n    cm, misclassifications, gesture_accuracy = create_detailed_confusion_analysis(\n        models, X_val_combined, y_val_combined, le.classes_, bfrb_gestures\n    )\n\nelse:\n    print(\"▶ INFERENCE MODE – loading artifacts from\", PRETRAINED_DIR)\n    \n    # Load all saved artifacts - UPDATED FOR NEW SCALER\n    final_feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n    pad_len = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n    scaler = joblib.load(PRETRAINED_DIR / \"imu_specific_scaler.pkl\")  # Updated scaler\n    gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n    \n    print(f\"  Loaded feature columns: {len(final_feature_cols)}\")\n    print(f\"  Sequence padding length: {pad_len}\")\n    print(f\"  Gesture classes: {len(gesture_classes)}\")\n    \n    # Define custom objects for model loading\n    custom_objects = {\n        'time_sum': time_sum,\n        'squeeze_last_axis': squeeze_last_axis, \n        'expand_last_axis': expand_last_axis,\n        'se_block': se_block,\n        'residual_se_cnn_block': residual_se_cnn_block,\n        'attention_layer': attention_layer,\n    }\n    \n    # Load ensemble of models\n    # Load ensemble of TF (Keras) models — keep them away from PyTorch `models`\n    tf_models = []\n    print(f\"  Loading {N_SPLITS} TF models for ensemble inference...\")\n    for fold in range(N_SPLITS):\n        model_path = PRETRAINED_DIR / f\"gesture_model_fold_{fold}.h5\"\n        if model_path.exists():\n            m = load_model(model_path, compile=False, custom_objects=custom_objects)\n            tf_models.append(m)                                   # ← 只放进 tf_models\n            print(f\"    ✓ Loaded TF fold {fold} model\")\n        else:\n            print(f\"    ✗ TF model fold {fold} not found at {model_path}\")\n\n    print(f\"  Successfully loaded {len(tf_models)} TF models\")    # ← 统计 tf_models\n\n\ndef _get_main_logits(model, x, n_classes):\n    \"\"\"\n    统一从单输出/多输出模型取出主 logits，并标准化为形状 (1, n_classes)。\n    任何不匹配都抛出短错误，由上层 try/except 兜底。\n    \"\"\"\n    out = model.predict(x, verbose=0)\n\n    # 多输出模型：取第一个（main_output）\n    if isinstance(out, (list, tuple)):\n        out = out[0]\n\n    out = np.asarray(out)\n\n    # 单输出模型但被切成 (18,) 的情况，补回 batch 维\n    if out.ndim == 1:\n        out = out.reshape(1, -1)\n\n    # 再保险：严格检查类别维度\n    if out.shape[-1] != n_classes:\n        raise ValueError(f\"model logits dim {out.shape[-1]} != expected {n_classes}\")\n\n    # 现在一定是 (1, n_classes)\n    return out\n\ndef predict_pf_allsensor(sequence: pl.DataFrame, demographics: pl.DataFrame) -> np.ndarray:\n    \"\"\"\n    使用已加载的 Keras 折叠模型做集成预测。\n    保证：任意异常仅打印短信息，不向外抛；返回 (1, C) 概率。\n    \"\"\"\n    try:\n        df_seq = sequence.to_pandas()\n\n        # ----- feature engineering -----\n        linear_accel = remove_gravity_from_acc(\n            df_seq[['acc_x', 'acc_y', 'acc_z']],\n            df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n        )\n        df_seq['linear_acc_x'] = linear_accel[:, 0]\n        df_seq['linear_acc_y'] = linear_accel[:, 1]\n        df_seq['linear_acc_z'] = linear_accel[:, 2]\n        df_seq['linear_acc_mag'] = np.sqrt(\n            df_seq['linear_acc_x']**2 + df_seq['linear_acc_y']**2 + df_seq['linear_acc_z']**2\n        )\n        df_seq['linear_acc_mag_jerk'] = df_seq['linear_acc_mag'].diff().fillna(0)\n\n        angular_vel = calculate_angular_velocity_from_quat(\n            df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n        )\n        df_seq['angular_vel_x'] = angular_vel[:, 0]\n        df_seq['angular_vel_y'] = angular_vel[:, 1]\n        df_seq['angular_vel_z'] = angular_vel[:, 2]\n        df_seq['angular_vel_mag'] = np.sqrt(\n            df_seq['angular_vel_x']**2 + df_seq['angular_vel_y']**2 + df_seq['angular_vel_z']**2\n        )\n        df_seq['angular_vel_mag_jerk'] = df_seq['angular_vel_mag'].diff().fillna(0)\n\n        angular_dist = calculate_angular_distance(\n            df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n        )\n        df_seq['angular_distance'] = angular_dist\n\n        df_seq['gesture_rhythm_signature'] = (\n            df_seq['linear_acc_mag'].rolling(5, min_periods=1).std()\n            / (df_seq['linear_acc_mag'].rolling(5, min_periods=1).mean() + 1e-6)\n        )\n\n        for i in range(1, 6):\n            pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n            tof_sensor_data = df_seq[pixel_cols].replace(-1, np.nan)\n            df_seq[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n            df_seq[f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n            df_seq[f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n            df_seq[f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n\n        # ----- scale & pad -----\n        mat_unscaled = df_seq[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n        mat_scaled = scaler.transform(mat_unscaled)\n        padded_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n\n        # ----- TF ensemble only -----\n        C = len(gesture_classes)\n        if not tf_models:\n            return np.ones((1, C), dtype=np.float32) / C\n\n        all_predictions = []\n        for m in tf_models:\n            try:\n                pred_main = _get_main_logits(m, padded_input, n_classes=C)  # (1, C)\n                all_predictions.append(pred_main.astype(np.float32, copy=False))\n            except Exception as e:\n                short = f\"{type(e).__name__}: {str(e)[:200]}\"\n                print(f\"[WARN] TF model {getattr(m,'name','?')} prediction failed: {short}\")\n                all_predictions.append(np.ones((1, C), dtype=np.float32) / C)\n\n        ensemble_pred = np.mean(np.stack(all_predictions, axis=0), axis=0).astype(np.float32, copy=False)\n        return ensemble_pred\n\n    except Exception as e:\n        short = f\"{type(e).__name__}: {str(e)[:800]}\"\n        print(f\"[ERROR] predict_pf_allsensor failed: {short}\")\n        C = len(gesture_classes) if 'gesture_classes' in globals() else 18\n        return np.ones((1, C), dtype=np.float32) / C\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:37:02.440243Z","iopub.execute_input":"2025-09-02T14:37:02.440752Z","iopub.status.idle":"2025-09-02T14:37:08.362437Z","shell.execute_reply.started":"2025-09-02T14:37:02.440732Z","shell.execute_reply":"2025-09-02T14:37:08.361368Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# imu only model","metadata":{}},{"cell_type":"code","source":"# ==================== TensorFlow SE-1DCNN 模型集成模块 ====================\n#\n# 此模块提供 predict_tensorflow_se1dcnn 函数用于模型集成\n# 使用类封装避免命名空间冲突\n# 核心逻辑: SE-1DCNN + Attention模型, 带有四元数安全处理\n#\n# ------------------------------------------------------------------\nimport os\nimport joblib\nimport json\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom pathlib import Path\nfrom scipy.spatial.transform import Rotation as R\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 设置随机种子\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(42)\n\nclass TensorFlowSE1DCNNModel:\n    \"\"\"封装TensorFlow SE-1DCNN模型,避免命名空间冲突\"\"\"\n    \n    def __init__(self):\n        # 配置路径 - 需要根据实际情况修改\n        self.PREPROCESS_DIR = Path(\"/kaggle/input/imuonly-process-model/imuonly_porcess/kaggle/working/processed_data_selected_features_v1\")  # 修改为实际路径\n        self.MODEL_DIR = Path(\"/kaggle/input/imuonly-process-model/imuonly1_model/kaggle/working/saved_models_keras_fixed_test\")  # 修改为实际路径\n        self.N_FOLDS = 5\n        self.MAX_SEQ_LENGTH = 128\n        \n        print(f\"TensorFlow SE-1DCNN模块: 正在加载配置...\")\n        \n        # 加载特征和标签配置\n        with open(self.PREPROCESS_DIR / \"feature_names.json\", 'r') as f:\n            feature_info = json.load(f)\n            self.ALL_FEATURE_NAMES = feature_info['all_features']\n        \n        with open(self.PREPROCESS_DIR / \"label_map.json\", 'r') as f:\n            self.LABEL2IDX = json.load(f)\n            self.IDX2LABEL = {v: k for k, v in self.LABEL2IDX.items()}\n            self.N_CLASSES = len(self.LABEL2IDX)\n            \n        # 统一的标签顺序(与所有模型对齐)\n        self.UNIFIED_LABELS = [\n            'Above ear - pull hair', 'Cheek - pinch skin', 'Drink from bottle/cup',\n            'Eyebrow - pull hair', 'Eyelash - pull hair',\n            'Feel around in tray and pull out an object', 'Forehead - pull hairline',\n            'Forehead - scratch', 'Glasses on/off', 'Neck - pinch skin',\n            'Neck - scratch', 'Pinch knee/leg skin', 'Pull air toward your face',\n            'Scratch knee/leg skin', 'Text on phone', 'Wave hello',\n            'Write name in air', 'Write name on leg'\n        ]\n\n        # 创建从本模型索引到统一索引的映射\n        self.model_to_unified_indices = [0] * self.N_CLASSES\n        for label, model_index in self.LABEL2IDX.items():\n            try:\n                unified_index = self.UNIFIED_LABELS.index(label)\n                self.model_to_unified_indices[model_index] = unified_index\n            except ValueError:\n                print(f\"警告: 标签 '{label}' 不在统一列表中。\")\n\n        # 选择的特征(与训练时完全一致)\n        self.SELECTED_FEATURES = [\n            'rot_w', 'rot_x', 'rot_y', 'rot_z',           # 四元数\n            'linear_acc_x', 'linear_acc_y', 'linear_acc_z', # 线性加速度\n            'linear_acc_mag',                               # 线性加速度模长\n            'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # 角速度\n            'angular_distance',                             # 角距离\n            'acc_mag'                                       # 加速度模长\n        ]\n        self.FEATURE_INDICES = [self.ALL_FEATURE_NAMES.index(f) for f in self.SELECTED_FEATURES]\n\n        # 模型和缩放器\n        self.models = []\n        self.scalers = []\n        self._loaded = False\n        \n    @staticmethod\n    def remove_gravity_from_acc(acc_values, quat_values):\n        \"\"\"去除重力影响\"\"\"\n        linear_accel = np.zeros_like(acc_values)\n        gravity_world = np.array([0, 0, 9.81])\n        for i in range(len(acc_values)):\n            if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n                linear_accel[i, :] = acc_values[i, :]\n                continue\n            try:\n                rotation = R.from_quat(quat_values[i])\n                gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n                linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n            except ValueError:\n                linear_accel[i, :] = acc_values[i, :]\n        return linear_accel\n\n    @staticmethod\n    def calculate_angular_velocity_from_quat(quat_values, time_delta=1/200):\n        \"\"\"计算角速度\"\"\"\n        angular_vel = np.zeros((len(quat_values), 3))\n        for i in range(len(quat_values) - 1):\n            q_t, q_t_plus_dt = quat_values[i], quat_values[i+1]\n            if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n               np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n                continue\n            try:\n                rot_t = R.from_quat(q_t)\n                rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n                delta_rot = rot_t.inv() * rot_t_plus_dt\n                angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n            except ValueError:\n                pass\n        return angular_vel\n    \n    @staticmethod\n    def compute_angular_distance_xyzw(quat_values):\n        \"\"\"计算相邻帧的角距离\"\"\"\n        n = len(quat_values)\n        ang = np.zeros(n, dtype=np.float32)\n        if n <= 1:\n            return ang\n        # 归一化\n        norm = np.linalg.norm(quat_values, axis=1, keepdims=True)\n        mask = norm[:,0] > 1e-8\n        quat_values[mask] = quat_values[mask] / norm[mask]\n        # 计算角距离\n        dot = np.sum(quat_values[:-1] * quat_values[1:], axis=1)\n        dot = np.clip(np.abs(dot), -1.0, 1.0)\n        ang[1:] = 2.0 * np.arccos(dot).astype(np.float32)\n        return ang\n\n    def fix_zero_quaternions(self, features):\n        \"\"\"修复零四元数\"\"\"\n        quat_features = features[:, :4]\n        quat_norm = np.linalg.norm(quat_features, axis=1)\n        zero_mask = quat_norm < 1e-8\n        \n        if np.any(zero_mask):\n            # 将零四元数替换为单位四元数 [1,0,0,0] (w,x,y,z格式)\n            features[zero_mask, :4] = [1.0, 0.0, 0.0, 0.0]\n        \n        return features\n\n    def feature_engineering(self, df):\n        \"\"\"特征工程\"\"\"\n        # 加速度模长\n        df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n        \n        # 线性加速度（去除重力）\n        acc_values = df[['acc_x', 'acc_y', 'acc_z']].values\n        quat_values = df[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n        \n        linear_accel = self.remove_gravity_from_acc(acc_values, quat_values)\n        df['linear_acc_x'] = linear_accel[:, 0]\n        df['linear_acc_y'] = linear_accel[:, 1]\n        df['linear_acc_z'] = linear_accel[:, 2]\n        df['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\n        \n        # 角速度\n        angular_vel = self.calculate_angular_velocity_from_quat(quat_values)\n        df['angular_vel_x'] = angular_vel[:, 0]\n        df['angular_vel_y'] = angular_vel[:, 1]\n        df['angular_vel_z'] = angular_vel[:, 2]\n        \n        # 角距离\n        df['angular_distance'] = self.compute_angular_distance_xyzw(quat_values)\n\n        # 填充缺失值\n        for feat in self.SELECTED_FEATURES:\n            if feat in df.columns:\n                df[feat] = df[feat].ffill().bfill().fillna(0.0).astype('float32')\n        \n        return df\n\n    def load_models(self):\n        \"\"\"延迟加载模型\"\"\"\n        if self._loaded:\n            return\n        \n        print(\"TensorFlow SE-1DCNN模块: 正在加载模型和scalers...\")\n        \n        # 启用不安全的反序列化\n        tf.keras.config.enable_unsafe_deserialization()\n        \n        # 定义自定义层\n        class SumPooling1D(Layer):\n            def __init__(self, **kwargs):\n                super(SumPooling1D, self).__init__(**kwargs)\n            def call(self, inputs):\n                return tf.reduce_sum(inputs, axis=1)\n            def get_config(self):\n                return super(SumPooling1D, self).get_config()\n\n        class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n            def __init__(self, base_lr, warmup_steps, total_steps, min_lr=1e-5):\n                super().__init__()\n                self.base_lr = base_lr\n                self.warmup_steps = warmup_steps\n                self.total_steps = total_steps\n                self.min_lr = min_lr\n            def __call__(self, step):\n                step = tf.cast(step, tf.float32)\n                warm = self.base_lr * (step / tf.cast(self.warmup_steps, tf.float32))\n                progress = (step - self.warmup_steps) / tf.maximum(1.0, self.total_steps - self.warmup_steps)\n                cosine = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + tf.cos(np.pi * tf.clip_by_value(progress, 0.0, 1.0)))\n                return tf.where(step < self.warmup_steps, warm, cosine)\n            def get_config(self):\n                return {\n                    \"base_lr\": self.base_lr,\n                    \"warmup_steps\": self.warmup_steps,\n                    \"total_steps\": self.total_steps,\n                    \"min_lr\": self.min_lr\n                }\n\n        custom_objects = {\n            'SumPooling1D': SumPooling1D,\n            'WarmupCosine': WarmupCosine\n        }\n        \n        # 加载模型和scaler\n        for fold in range(1, self.N_FOLDS + 1):\n            scaler_path = self.MODEL_DIR / f\"fold_{fold}_scaler.joblib\"\n            model_path = self.MODEL_DIR / f\"fold_{fold}_model.keras\"\n            \n            if scaler_path.exists() and model_path.exists():\n                try:\n                    scaler = joblib.load(scaler_path)\n                    model = tf.keras.models.load_model(model_path, custom_objects=custom_objects, compile=False)\n                    self.scalers.append(scaler)\n                    self.models.append(model)\n                    print(f\"  ✓ 加载 Fold {fold}\")\n                except Exception as e:\n                    print(f\"  ✗ 加载 Fold {fold} 失败: {e}\")\n            else:\n                print(f\"  ✗ 找不到 Fold {fold} 的文件\")\n            \n        self._loaded = True\n        print(f\"TensorFlow SE-1DCNN模块: 成功加载 {len(self.models)}/{self.N_FOLDS} 个模型。\")\n\n    def _standardize_with_quaternion_simple(self, features, scaler):\n        \"\"\"四元数安全的标准化 - 简化版本\"\"\"\n        quat_features = features[:, :4]\n        other_features = features[:, 4:]\n        \n        # 四元数保持原值，不归一化\n        # 只标准化非四元数特征\n        if other_features.shape[1] > 0:\n            other_scaled = scaler.transform(other_features)\n            features_scaled = np.concatenate([quat_features, other_scaled], axis=1)\n        else:\n            features_scaled = quat_features\n        \n        return features_scaled\n\n    def predict_proba(self, sequence: pl.DataFrame, demographics: pl.DataFrame) -> np.ndarray:\n        \"\"\"返回概率分布,已对齐到统一标签顺序\"\"\"\n        self.load_models()\n        \n        try:\n            # 数据预处理\n            df = sequence.to_pandas()\n            df = self.feature_engineering(df)\n            \n            # 提取选定特征\n            features = df[self.SELECTED_FEATURES].values\n            \n            # 修复零四元数\n            features = self.fix_zero_quaternions(features)\n            \n            # 集成预测\n            all_predictions = []\n            for model, scaler in zip(self.models, self.scalers):\n                # 四元数安全的标准化\n                features_scaled = self._standardize_with_quaternion_simple(features, scaler)\n                \n                # 截断或填充到max_seq_length\n                if len(features_scaled) > self.MAX_SEQ_LENGTH:\n                    features_scaled = features_scaled[:self.MAX_SEQ_LENGTH]\n                else:\n                    pad_length = self.MAX_SEQ_LENGTH - len(features_scaled)\n                    # 填充：四元数用[1,0,0,0]，其他特征用0\n                    quat_pad = np.array([[1.0, 0.0, 0.0, 0.0]] * pad_length)\n                    other_pad = np.zeros((pad_length, features_scaled.shape[1] - 4))\n                    pad_values = np.concatenate([quat_pad, other_pad], axis=1)\n                    features_scaled = np.vstack([features_scaled, pad_values])\n\n                # 预测\n                features_scaled = features_scaled[np.newaxis, ...]\n                pred = model.predict(features_scaled, verbose=0)[0]\n                all_predictions.append(pred)\n            \n            # 平均集成并转换为概率\n            ensemble_logits = np.mean(all_predictions, axis=0)\n            \n            # 使用softmax转换为概率\n            exp_logits = np.exp(ensemble_logits - np.max(ensemble_logits))\n            raw_probabilities = exp_logits / np.sum(exp_logits)\n            \n            # 重新排列概率以匹配统一标签顺序\n            unified_probabilities = np.zeros((1, len(self.UNIFIED_LABELS)))\n            for model_index, unified_index in enumerate(self.model_to_unified_indices):\n                unified_probabilities[0, unified_index] = raw_probabilities[model_index]\n                \n            return unified_probabilities\n            \n        except Exception as e:\n            print(f\"TensorFlowSE1DCNNModel.predict_proba 出错: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            # 返回均匀分布作为默认\n            return np.full((1, len(self.UNIFIED_LABELS)), 1.0 / len(self.UNIFIED_LABELS))\n\n# ==================== 创建全局模型实例 ====================\n_tensorflow_se1dcnn_model = TensorFlowSE1DCNNModel()\n\n# ==================== 导出的预测函数 ====================\ndef predict_tensorflow_se1dcnn(sequence: pl.DataFrame, demographics: pl.DataFrame) -> np.ndarray:\n    \"\"\"\n    TensorFlow SE-1DCNN模型预测函数(用于集成)\n    \n    参数:\n        sequence: 传感器序列数据\n        demographics: 人口统计数据 (未使用)\n    \n    返回:\n        np.ndarray: 形状为 (1, 18) 的概率数组,已对齐到统一标签顺序\n    \"\"\"\n    return _tensorflow_se1dcnn_model.predict_proba(sequence, demographics)\n\n\n# ==================== 本地测试代码 ====================\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"=\"*60)\n    print(\"         TensorFlow SE-1DCNN模型本地测试\")\n    print(\"=\"*60 + \"\\n\")\n    \n    TEST_CSV = '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv'\n    TEST_DEMOGRAPHICS = '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv'\n    \n    try:\n        print(f\"正在加载测试数据...\")\n        test_sequences = pl.read_csv(TEST_CSV)\n        test_demographics = pl.read_csv(TEST_DEMOGRAPHICS)\n        \n        sequence_ids = test_sequences.get_column(\"sequence_id\").unique().to_list()\n        print(f\"找到 {len(sequence_ids)} 个测试序列\")\n        \n        test_count = min(3, len(sequence_ids))\n        print(f\"\\n将测试前 {test_count} 个序列:\")\n        print(\"-\" * 40)\n        \n        for i, seq_id in enumerate(sequence_ids[:test_count]):\n            print(f\"\\n测试序列 {i+1}/{test_count}: {seq_id}\")\n            print(\"-\" * 30)\n            \n            sequence = test_sequences.filter(pl.col(\"sequence_id\") == seq_id)\n            print(f\"序列长度: {len(sequence)} 个时间步\")\n            \n            print(\"正在进行预测...\")\n            probabilities = predict_tensorflow_se1dcnn(sequence, test_demographics)\n            \n            print(f\"\\n预测结果:\")\n            print(f\"  - 概率数组形状: {probabilities.shape}\")\n            print(f\"  - 概率和: {probabilities.sum():.6f} (应该 ≈ 1.0)\")\n            print(f\"  - 最小概率: {probabilities.min():.6f}\")\n            print(f\"  - 最大概率: {probabilities.max():.6f}\")\n            \n            top3_indices = np.argsort(probabilities[0])[-3:][::-1]\n            \n            print(f\"\\nTop-3 预测:\")\n            for rank, idx in enumerate(top3_indices, 1):\n                label = _tensorflow_se1dcnn_model.UNIFIED_LABELS[idx]\n                prob = probabilities[0, idx]\n                print(f\"  {rank}. {label:<40} {prob:.4f}\")\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"✅ 测试成功完成！\")\n        print(\"模块已准备好用于集成。\")\n        print(\"=\"*60)\n        \n    except FileNotFoundError as e:\n        print(f\"\\n❌ 错误：找不到测试文件\")\n        print(f\"请确保以下文件存在：\")\n        print(f\"  - {TEST_CSV}\")\n        print(f\"  - {TEST_DEMOGRAPHICS}\")\n        \n    except Exception as e:\n        print(f\"\\n❌ 测试过程中出错: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:37:08.363719Z","iopub.execute_input":"2025-09-02T14:37:08.364059Z","iopub.status.idle":"2025-09-02T14:37:25.539901Z","shell.execute_reply.started":"2025-09-02T14:37:08.364032Z","shell.execute_reply":"2025-09-02T14:37:25.539197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"import polars as pl\nimport numpy as np\n\n# 更严格的数据质量门控\n\ndef predict(sequence: pl.DataFrame, demographics):\n    \"\"\"\n    Evaluates sensor data quality (ToF & THM) to dynamically route to the best model.\n    Uses a conservative QC for THM so that multi-sensor is used only when\n    the extra sensors are truly informative.\n    \n    路由逻辑:\n    - 如果 THM 和 TOF 都不可用: 0.4*IMU-THM-TOF + 0.6*IMU-only\n    - 否则 (至少有一个可用): 0.7*IMU-THM-TOF + 0.3*IMU-only\n    \"\"\"\n\n    # ---------------- Parameters ----------------\n    # ToF (kept as your current settings)\n    TOF_FRAME_VALID_RATIO_THRESHOLD = 0.25\n    TOF_MIN_GOOD_SENSORS            = 2\n    TOF_SPATIAL_COV_HIGH_THRESHOLD  = 0.50\n    TOF_SPATIAL_COV_LOW_THRESHOLD   = 0.30\n    TOF_TIME_COVERAGE_THRESHOLD     = 0.60\n\n    # THM (strict gates; you can tune on CV)\n    THM_FRAME_VALID_RATIO_THRESHOLD = 0.60   # was 0.25\n    THM_MIN_ACTIVE_RATIO            = 0.20   # %frames with |diff| > eps\n    THM_ACTIVE_DELTA_EPS            = 0.05   # activity threshold for diff\n    THM_MIN_GOOD_SENSORS            = 2      # at least 2 THM channels pass\n    THM_MIN_STD_SUM                 = 0.0    # keep 0 as you prefer (can later change to a quantile)\n\n    CONFIDENCE_MARGIN = 0.0  # kept for future use if you compare confidences\n\n    # ---------------- Column Identification ----------------\n    all_tof_cols = [c for c in sequence.columns if c.startswith(\"tof_\")]\n    all_thm_cols = [c for c in sequence.columns if c.startswith(\"thm_\")]\n\n    # ---------------- Preprocessing for QC ----------------\n    # For QC, treat sentinel -1 as null to compute valid coverage correctly.\n    sequence_for_qc = sequence\n    if all_tof_cols:\n        sequence_for_qc = sequence_for_qc.with_columns(\n            [pl.col(c).replace(-1, None).alias(c) for c in all_tof_cols]\n        )\n    # If your THM ever uses -1 as sentinel, uncomment the following:\n    # if all_thm_cols:\n    #     sequence_for_qc = sequence_for_qc.with_columns(\n    #         [pl.col(c).replace(-1, None).alias(c) for c in all_thm_cols]\n    #     )\n\n    # ---------------- ToF Quality Assessment ----------------\n    is_tof_system_ok = False\n    if all_tof_cols:\n        overall_tof_frame_ratio = sequence_for_qc.select(\n            pl.any_horizontal([pl.col(c).is_not_null() for c in all_tof_cols]).cast(pl.Int8).mean()\n        ).item()\n\n        if overall_tof_frame_ratio is None:\n            overall_tof_frame_ratio = 0.0\n\n        if overall_tof_frame_ratio >= TOF_FRAME_VALID_RATIO_THRESHOLD:\n            # group ToF pixels by sensor 1..5\n            tof_sensor_groups: dict[int, list[str]] = {}\n            for sensor_id in range(1, 6):\n                cols_for_sensor = [c for c in all_tof_cols if c.startswith(f\"tof_{sensor_id}_\")]\n                if cols_for_sensor:\n                    tof_sensor_groups[sensor_id] = cols_for_sensor\n\n            num_good_tof_sensors = 0\n            for sensor_id, sensor_cols in tof_sensor_groups.items():\n                n_pix = len(sensor_cols)\n                spatial_cov_series = sequence_for_qc.select(\n                    (pl.sum_horizontal([pl.col(c).is_not_null() for c in sensor_cols]) / n_pix).alias(\"cov\")\n                )[\"cov\"]\n\n                avg_spatial_coverage = float(spatial_cov_series.mean())\n                time_coverage = float(\n                    (spatial_cov_series >= TOF_SPATIAL_COV_LOW_THRESHOLD).cast(pl.Int8).mean()\n                )\n\n                is_high_quality = avg_spatial_coverage >= TOF_SPATIAL_COV_HIGH_THRESHOLD\n                is_medium_stable = (\n                    (avg_spatial_coverage >= TOF_SPATIAL_COV_LOW_THRESHOLD) and\n                    (time_coverage >= TOF_TIME_COVERAGE_THRESHOLD)\n                )\n\n                if is_high_quality or is_medium_stable:\n                    num_good_tof_sensors += 1\n\n            is_tof_system_ok = (num_good_tof_sensors >= TOF_MIN_GOOD_SENSORS)\n\n    # ---------------- THM Quality Assessment (strict gates) ----------------\n    is_thm_system_ok = False\n    if all_thm_cols:\n        # Gate 1) Frame coverage across all THM columns\n        thm_frame_ratio = sequence_for_qc.select(\n            pl.any_horizontal([pl.col(c).is_not_null() for c in all_thm_cols]).cast(pl.Int8).mean()\n        ).item()\n        if thm_frame_ratio is None:\n            thm_frame_ratio = 0.0\n\n        if thm_frame_ratio >= THM_FRAME_VALID_RATIO_THRESHOLD:\n            good_thm_flags = []\n            thm_std_list = []\n\n            for c in all_thm_cols:\n                # per-channel frame coverage\n                valid_ratio = sequence_for_qc.select(\n                    pl.col(c).is_not_null().cast(pl.Int8).mean()\n                ).item() or 0.0\n\n                # per-channel activity ratio: fraction of frames with |diff| > eps\n                active_ratio = sequence_for_qc.select(\n                    (pl.col(c).diff().abs() > THM_ACTIVE_DELTA_EPS).cast(pl.Int8).mean()\n                ).item() or 0.0\n\n                # std for std_sum (not a hard gate since you chose to keep 0.0)\n                ch_std = sequence_for_qc.select(pl.col(c).std()).item()\n                ch_std = 0.0 if (ch_std is None or np.isnan(ch_std)) else float(ch_std)\n                thm_std_list.append(ch_std)\n\n                good_thm_flags.append(\n                    (valid_ratio >= THM_FRAME_VALID_RATIO_THRESHOLD) and\n                    (active_ratio >= THM_MIN_ACTIVE_RATIO)\n                )\n\n            num_good_thm = sum(1 for f in good_thm_flags if f)\n            thm_std_sum  = float(np.sum(thm_std_list))\n\n            # Gate 2 & 3: need >= 2 good channels AND (optionally) std_sum >= threshold (kept 0.0)\n            is_thm_system_ok = (num_good_thm >= THM_MIN_GOOD_SENSORS) and (thm_std_sum >= THM_MIN_STD_SUM)\n\n    # ---------------- 新的路由决策逻辑 ----------------\n    # 判断是否至少有一个传感器系统可用\n    at_least_one_sensor_ok = is_tof_system_ok or is_thm_system_ok\n    \n    # ---------------- 推理部分 ----------------\n    # 获取两类模型的预测\n    # Multi-sensor models (IMU-THM-TOF)\n    probabilities_model1 = predict2(sequence, demographics)[0]\n    probabilities_model2 = predict4(sequence, demographics)[0]\n    probabilities_model3 = predict_pf_allsensor(sequence, demographics)[0]\n    multi_sensor_probabilities = 0.4 * probabilities_model1 + 0.35 * probabilities_model2 + 0.25 * probabilities_model3\n    \n    # IMU-only models\n    probabilities_imu1 = predict_tensorflow_se1dcnn(sequence, demographics)[0]\n    probabilities_imu2 = predict3(sequence, demographics)[0]\n    imu_only_probabilities = 0.6 * probabilities_imu1 + 0.4 * probabilities_imu2\n    \n    # 根据传感器可用性决定最终权重\n    if at_least_one_sensor_ok:\n        # 至少有一个传感器可用: 0.7*IMU-THM-TOF + 0.3*IMU-only\n        final_probabilities = 0.7 * multi_sensor_probabilities + 0.3 * imu_only_probabilities\n    else:\n        # THM和TOF都不可用: 0.4*IMU-THM-TOF + 0.6*IMU-only\n        final_probabilities = 0.4 * multi_sensor_probabilities + 0.6 * imu_only_probabilities\n\n    predicted_index = int(np.argmax(final_probabilities))\n    return dataset.le.classes_[predicted_index]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:37:25.540685Z","iopub.execute_input":"2025-09-02T14:37:25.540911Z","iopub.status.idle":"2025-09-02T14:37:25.557913Z","shell.execute_reply.started":"2025-09-02T14:37:25.540893Z","shell.execute_reply":"2025-09-02T14:37:25.557151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#imumonly model\n    #1 secmm bert imuonly lb  = 0.810\n    #2 secnn + attention imuonly lb = 0.813\n    #1 and 2 average esamble = 0.814\n\n#all sensor model\n    #secnn bert allsensor lb = 0.841\n    #secnn gated-gru-hybrid-ensemble esamble lb = 0.835\n    #secnn + bigru,bilstm lb = 0.821\n    #secnn+(bilstm,bigru),secnn+bert,secnn+\n    \n\n#esamble type\n    #allsensor weight esamble lb = 0.845\n    #imuonly esamble + allsensor esamble data quality Switching Model lb = 0.853\n    #imuonly esamble + allsensor esamble data quality switching cross esamble lb = 0.855(now choice)\n\n#performance improvement \n    #directional model integration\n    #post-processing\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:37:25.558772Z","iopub.execute_input":"2025-09-02T14:37:25.558954Z","iopub.status.idle":"2025-09-02T14:37:25.571962Z","shell.execute_reply.started":"2025-09-02T14:37:25.55894Z","shell.execute_reply":"2025-09-02T14:37:25.571347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kaggle_evaluation.cmi_inference_server\ninference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n        )\n    )\n\nif not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print(pd.read_parquet(\"submission.parquet\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T14:37:25.572699Z","iopub.execute_input":"2025-09-02T14:37:25.57295Z","iopub.status.idle":"2025-09-02T14:37:49.779783Z","shell.execute_reply.started":"2025-09-02T14:37:25.572925Z","shell.execute_reply":"2025-09-02T14:37:49.779055Z"}},"outputs":[],"execution_count":null}]}