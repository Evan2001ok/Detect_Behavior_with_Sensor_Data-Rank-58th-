{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "imuonly-dataprocoss"
      ],
      "metadata": {
        "id": "AcTGU0DYX7eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "#  æ•°æ®å¤„ç†è„šæœ¬\n",
        "#  CMI Behavior Detection - ç²¾ç®€ç‰¹å¾ç‰ˆ\n",
        "#\n",
        "#  æ’é™¤å·²çŸ¥é—®é¢˜åºåˆ—: SEQ_011975 (å­˜åœ¨æµ‹é‡é—®é¢˜)\n",
        "#\n",
        "#  è®­ç»ƒç‰¹å¾: rot, linear_acc, linear_acc_mag, angular_vel, angular_distance, acc_mag\n",
        "# ===================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suppress noisy warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# ===================================================================\n",
        "# 1. Configuration & Global Settings\n",
        "# ===================================================================\n",
        "\n",
        "DATA_ROOT = '/kaggle/input/cmi-detect-behavior-with-sensor-data'\n",
        "OUTPUT_DIR = './processed_data_selected_features_v1'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# è®­ç»ƒä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨\n",
        "SELECTED_FEATURES = [\n",
        "    'rot_w', 'rot_x', 'rot_y', 'rot_z',           # å››å…ƒæ•°\n",
        "    'linear_acc_x', 'linear_acc_y', 'linear_acc_z', # çº¿æ€§åŠ é€Ÿåº¦\n",
        "    'linear_acc_mag',                               # çº¿æ€§åŠ é€Ÿåº¦æ¨¡é•¿\n",
        "    'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # è§’é€Ÿåº¦\n",
        "    'angular_distance',                             # è§’è·ç¦»\n",
        "    'acc_mag'                                       # åŠ é€Ÿåº¦æ¨¡é•¿\n",
        "]\n",
        "\n",
        "# Labels\n",
        "LABEL_NAMES = [\n",
        "    'Forehead - pull hairline', 'Neck - pinch skin', 'Forehead - scratch',\n",
        "    'Eyelash - pull hair', 'Text on phone', 'Eyebrow - pull hair',\n",
        "    'Neck - scratch', 'Above ear - pull hair', 'Cheek - pinch skin',\n",
        "    'Wave hello', 'Write name in air', 'Pull air toward your face',\n",
        "    'Feel around in tray and pull out an object', 'Write name on leg',\n",
        "    'Pinch knee/leg skin', 'Scratch knee/leg skin', 'Drink from bottle/cup',\n",
        "    'Glasses on/off'\n",
        "]\n",
        "LABEL2IDX = {x: i for i, x in enumerate(LABEL_NAMES)}\n",
        "\n",
        "# ===================================================================\n",
        "# 2. æ ¸å¿ƒç‰¹å¾å·¥ç¨‹å‡½æ•°\n",
        "# ===================================================================\n",
        "\n",
        "def remove_gravity_from_acc(acc_data, rot_data):\n",
        "    \"\"\"å»é™¤é‡åŠ›å½±å“ï¼Œè®¡ç®—çº¿æ€§åŠ é€Ÿåº¦\"\"\"\n",
        "    acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
        "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    linear_accel = np.zeros_like(acc_values)\n",
        "    gravity_world = np.array([0, 0, 9.81])\n",
        "\n",
        "    for i in range(len(acc_values)):\n",
        "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
        "            linear_accel[i, :] = acc_values[i, :]\n",
        "            continue\n",
        "        try:\n",
        "            rotation = R.from_quat(quat_values[i])\n",
        "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
        "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
        "        except ValueError:\n",
        "            linear_accel[i, :] = acc_values[i, :]\n",
        "    return linear_accel\n",
        "\n",
        "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200):\n",
        "    \"\"\"ä»å››å…ƒæ•°è®¡ç®—è§’é€Ÿåº¦\"\"\"\n",
        "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    angular_vel = np.zeros((len(quat_values), 3))\n",
        "\n",
        "    for i in range(len(quat_values) - 1):\n",
        "        q_t, q_t_plus_dt = quat_values[i], quat_values[i+1]\n",
        "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
        "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
        "            continue\n",
        "        try:\n",
        "            rot_t = R.from_quat(q_t)\n",
        "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
        "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
        "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
        "        except ValueError:\n",
        "            pass\n",
        "    return angular_vel\n",
        "\n",
        "def compute_angular_distance_xyzw(rot_df):\n",
        "    \"\"\"è®¡ç®—ç›¸é‚»å¸§çš„è§’è·ç¦»\"\"\"\n",
        "    q = rot_df[['rot_x','rot_y','rot_z','rot_w']].values.astype(np.float32)\n",
        "    n = len(q)\n",
        "    ang = np.zeros(n, dtype=np.float32)\n",
        "    if n <= 1:\n",
        "        return ang\n",
        "    # å½’ä¸€åŒ–\n",
        "    norm = np.linalg.norm(q, axis=1, keepdims=True)\n",
        "    mask = norm[:,0] > 1e-8\n",
        "    q[mask] = q[mask] / norm[mask]\n",
        "    # è®¡ç®—è§’è·ç¦»\n",
        "    dot = np.sum(q[:-1] * q[1:], axis=1)\n",
        "    dot = np.clip(np.abs(dot), -1.0, 1.0)\n",
        "    ang[1:] = 2.0 * np.arccos(dot).astype(np.float32)\n",
        "    return ang\n",
        "\n",
        "def feature_engineering(df):\n",
        "    \"\"\"ç²¾ç®€ç‰ˆç‰¹å¾å·¥ç¨‹\"\"\"\n",
        "    print(\"å¼€å§‹ç‰¹å¾å·¥ç¨‹...\")\n",
        "\n",
        "    # 1. åŠ é€Ÿåº¦æ¨¡é•¿\n",
        "    df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
        "\n",
        "    # 2. çº¿æ€§åŠ é€Ÿåº¦ï¼ˆå»é™¤é‡åŠ›ï¼‰\n",
        "    print(\"è®¡ç®—çº¿æ€§åŠ é€Ÿåº¦...\")\n",
        "    tqdm.pandas(desc=\"å»é™¤é‡åŠ›\")\n",
        "    linear_accel_df = df.groupby('sequence_id', group_keys=False).progress_apply(\n",
        "        lambda g: pd.DataFrame(\n",
        "            remove_gravity_from_acc(\n",
        "                g[['acc_x', 'acc_y', 'acc_z']],\n",
        "                g[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "            ),\n",
        "            columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'],\n",
        "            index=g.index\n",
        "        )\n",
        "    )\n",
        "    df = df.join(linear_accel_df)\n",
        "\n",
        "    # çº¿æ€§åŠ é€Ÿåº¦æ¨¡é•¿\n",
        "    df['linear_acc_mag'] = np.sqrt(\n",
        "        df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2\n",
        "    )\n",
        "\n",
        "    # 3. è§’é€Ÿåº¦\n",
        "    print(\"è®¡ç®—è§’é€Ÿåº¦...\")\n",
        "    tqdm.pandas(desc=\"è®¡ç®—è§’é€Ÿåº¦\")\n",
        "    angular_velocity_df = df.groupby('sequence_id', group_keys=False).progress_apply(\n",
        "        lambda g: pd.DataFrame(\n",
        "            calculate_angular_velocity_from_quat(g[['rot_x', 'rot_y', 'rot_z', 'rot_w']]),\n",
        "            columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'],\n",
        "            index=g.index\n",
        "        )\n",
        "    )\n",
        "    df = df.join(angular_velocity_df)\n",
        "\n",
        "    # 4. è§’è·ç¦»\n",
        "    print(\"è®¡ç®—è§’è·ç¦»...\")\n",
        "    tqdm.pandas(desc=\"è®¡ç®—è§’è·ç¦»\")\n",
        "    angdist_df = df.groupby('sequence_id', group_keys=False).progress_apply(\n",
        "        lambda g: pd.Series(compute_angular_distance_xyzw(g[['rot_x','rot_y','rot_z','rot_w']].reset_index(drop=True)),\n",
        "                            index=g.index, name='angular_distance')\n",
        "    ).to_frame()\n",
        "    df = df.join(angdist_df)\n",
        "\n",
        "    # å¡«å……ç¼ºå¤±å€¼ï¼ˆæŒ‰åºåˆ—åˆ†ç»„ï¼‰\n",
        "    print(\"å¡«å……ç¼ºå¤±å€¼...\")\n",
        "    df[SELECTED_FEATURES] = (\n",
        "        df.groupby('sequence_id')[SELECTED_FEATURES]\n",
        "        .apply(lambda g: g.ffill().bfill())\n",
        "        .reset_index(level=0, drop=True)\n",
        "        .fillna(0.0)\n",
        "        .astype('float32')\n",
        "    )\n",
        "\n",
        "    print(f\"ç‰¹å¾å·¥ç¨‹å®Œæˆã€‚ç‰¹å¾æ•°: {len(SELECTED_FEATURES)}\")\n",
        "    return df\n",
        "\n",
        "# ===================================================================\n",
        "# 3. ä¸»æ‰§è¡Œæµç¨‹\n",
        "# ===================================================================\n",
        "\n",
        "PROBLEMATIC_SEQUENCES = ['SEQ_011975']\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"åŠ è½½åŸå§‹æ•°æ®...\")\n",
        "    train_df = pd.read_csv(f'{DATA_ROOT}/train.csv')\n",
        "    train_demo_df = pd.read_csv(f'{DATA_ROOT}/train_demographics.csv')\n",
        "    train_df = pd.merge(train_df, train_demo_df, how='left', on='subject')\n",
        "\n",
        "    # æ’é™¤é—®é¢˜åºåˆ—\n",
        "    print(f\"æ’é™¤é—®é¢˜åºåˆ—: {PROBLEMATIC_SEQUENCES}\")\n",
        "    original_count = len(train_df)\n",
        "    train_df = train_df[~train_df['sequence_id'].isin(PROBLEMATIC_SEQUENCES)].copy()\n",
        "    excluded_count = original_count - len(train_df)\n",
        "    print(f\"å·²æ’é™¤ {excluded_count} æ¡æ•°æ®è®°å½•\")\n",
        "\n",
        "    # è¿è¡Œç‰¹å¾å·¥ç¨‹\n",
        "    train_df = feature_engineering(train_df)\n",
        "\n",
        "    print(\"èšåˆæ•°æ®ä¸ºåºåˆ—...\")\n",
        "    agg_train_df = train_df.groupby(['sequence_id', 'subject', 'gesture']).apply(\n",
        "        lambda df: df[SELECTED_FEATURES].values,\n",
        "        include_groups=False,\n",
        "    ).reset_index()\n",
        "    agg_train_df.columns = ['sequence_id', 'subject', 'gesture', 'sequence']\n",
        "    agg_train_df['label'] = agg_train_df.gesture.map(LABEL2IDX)\n",
        "\n",
        "    # ä¿å­˜å¤„ç†åçš„æ•°æ®\n",
        "    output_path = f'{OUTPUT_DIR}/processed_train_data_raw.joblib'\n",
        "    print(f\"ä¿å­˜èšåˆæ•°æ®åˆ° {output_path}\")\n",
        "    joblib.dump(agg_train_df, output_path)\n",
        "\n",
        "    # ä¿å­˜ç‰¹å¾é…ç½®\n",
        "    print(f\"ä¿å­˜ç‰¹å¾é…ç½®...\")\n",
        "    feature_info = {\n",
        "        'all_features': SELECTED_FEATURES,\n",
        "        'time_features': SELECTED_FEATURES,\n",
        "        'psd_features': [],\n",
        "        'stat_features': [],\n",
        "        'feature_count': len(SELECTED_FEATURES)\n",
        "    }\n",
        "    with open(f'{OUTPUT_DIR}/feature_names.json', 'w') as f:\n",
        "        json.dump(feature_info, f, indent=2)\n",
        "\n",
        "    with open(f'{OUTPUT_DIR}/label_map.json', 'w') as f:\n",
        "        json.dump(LABEL2IDX, f, indent=2)\n",
        "\n",
        "    print(\"\\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆ!\")\n",
        "    print(f\"ç‰¹å¾æ•°: {len(SELECTED_FEATURES)}\")\n",
        "    print(\"\\nç‰¹å¾åˆ—è¡¨:\")\n",
        "    for i, feat in enumerate(SELECTED_FEATURES, 1):\n",
        "        print(f\"  {i:2d}. {feat}\")\n",
        "\n",
        "# ===================================================================\n",
        "# 4. æµ‹è¯•é›†é¢„å¤„ç†å‡½æ•°\n",
        "# ===================================================================\n",
        "\n",
        "def preprocess_test_data(test_csv_path, demographics_csv_path=None, output_dir=OUTPUT_DIR):\n",
        "    \"\"\"å¤„ç†æµ‹è¯•æ•°æ®\"\"\"\n",
        "    print(\"å¤„ç†æµ‹è¯•æ•°æ®...\")\n",
        "\n",
        "    test_df = pd.read_csv(test_csv_path)\n",
        "    if demographics_csv_path:\n",
        "        test_demo_df = pd.read_csv(demographics_csv_path)\n",
        "        test_df = pd.merge(test_df, test_demo_df, how='left', on='subject')\n",
        "\n",
        "    # æ’é™¤é—®é¢˜åºåˆ—\n",
        "    if 'sequence_id' in test_df.columns:\n",
        "        test_df = test_df[~test_df['sequence_id'].isin(PROBLEMATIC_SEQUENCES)].copy()\n",
        "\n",
        "    # ç‰¹å¾å·¥ç¨‹\n",
        "    test_df = feature_engineering(test_df)\n",
        "\n",
        "    # èšåˆæ•°æ®\n",
        "    agg_test_df = test_df.groupby(['sequence_id', 'subject']).apply(\n",
        "        lambda df: df[SELECTED_FEATURES].values,\n",
        "        include_groups=False,\n",
        "    ).reset_index()\n",
        "    agg_test_df.columns = ['sequence_id', 'subject', 'sequence']\n",
        "\n",
        "    # ä¿å­˜\n",
        "    test_output_path = f'{output_dir}/processed_test_data_raw.joblib'\n",
        "    print(f\"ä¿å­˜æµ‹è¯•æ•°æ®åˆ° {test_output_path}\")\n",
        "    joblib.dump(agg_test_df, test_output_path)\n",
        "\n",
        "    print(\"âœ… æµ‹è¯•æ•°æ®é¢„å¤„ç†å®Œæˆ!\")\n",
        "    return agg_test_df"
      ],
      "metadata": {
        "id": "YqSOlAvpc2Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training-model"
      ],
      "metadata": {
        "id": "Dhmd2MmIYBmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "#\n",
        "# å®Œæ•´çš„ TensorFlow/Keras è®­ç»ƒä»£ç  - CMI è¡Œä¸ºæ£€æµ‹\n",
        "# se1dcnn+attentionæ¨¡å‹æ¶æ„ - å››å…ƒæ•°å®‰å…¨ç‰ˆæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼šä»…ä¿®å¤é›¶å››å…ƒæ•°ï¼‰\n",
        "# cv = 0.8094, lb = 0.812\n",
        "# è¿‡æ‹Ÿåˆå·²è§£å†³\n",
        "#\n",
        "# ===================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Your input ran out of data.*\")\n",
        "\n",
        "# æŠ‘åˆ¶ TensorFlow çš„ä¸€äº›æ—¥å¿—è¾“å‡º\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# ===================================================================\n",
        "# 1. é…ç½®ä¸å…¨å±€è®¾ç½®\n",
        "# ===================================================================\n",
        "DEBUG = False\n",
        "TRAIN = True\n",
        "MAX_SEQ_LENGTH = 128\n",
        "PROCESSED_DATA_DIR = '/kaggle/input/imuonly-process/kaggle/working/processed_data_selected_features_v1' # è¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…è·¯å¾„\n",
        "OUTPUT_DIR = './saved_models_keras_fixed_test'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# ä»é¢„å¤„ç†é˜¶æ®µç”Ÿæˆçš„æ–‡ä»¶ä¸­åŠ è½½æ ‡ç­¾æ˜ å°„\n",
        "with open(f'{PROCESSED_DATA_DIR}/label_map.json', 'r') as f:\n",
        "    LABEL2IDX = json.load(f)\n",
        "IDX2LABEL = {v: k for k, v in LABEL2IDX.items()}\n",
        "N_CLASSES = len(LABEL2IDX)\n",
        "\n",
        "# åŠ è½½ç‰¹å¾é…ç½®\n",
        "with open(f'{PROCESSED_DATA_DIR}/feature_names.json', 'r') as f:\n",
        "    feature_info = json.load(f)\n",
        "    ALL_FEATURE_NAMES = feature_info['all_features']\n",
        "    TIME_FEATURE_NAMES = feature_info['time_features']\n",
        "    PSD_FEATURE_NAMES = feature_info['psd_features']\n",
        "    STAT_FEATURE_NAMES = feature_info['stat_features']\n",
        "\n",
        "# ===================================================================\n",
        "# ç‰¹å¾é€‰æ‹©é…ç½® - æ³¨æ„å‰4ä¸ªå¿…é¡»æ˜¯å››å…ƒæ•°\n",
        "# ===================================================================\n",
        "SELECTED_FEATURES = [\n",
        "    'rot_w', 'rot_x', 'rot_y', 'rot_z',           # å››å…ƒæ•°\n",
        "    'linear_acc_x', 'linear_acc_y', 'linear_acc_z', # çº¿æ€§åŠ é€Ÿåº¦\n",
        "    'linear_acc_mag',                               # çº¿æ€§åŠ é€Ÿåº¦æ¨¡é•¿\n",
        "    'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # è§’é€Ÿåº¦\n",
        "    'angular_distance',                             # è§’è·ç¦»\n",
        "    'acc_mag'                                       # åŠ é€Ÿåº¦æ¨¡é•¿\n",
        "]\n",
        "\n",
        "if SELECTED_FEATURES is not None:\n",
        "    FEATURE_NAMES = SELECTED_FEATURES\n",
        "else:\n",
        "    FEATURE_NAMES = ALL_FEATURE_NAMES\n",
        "\n",
        "FEATURE_INDICES = [ALL_FEATURE_NAMES.index(f) for f in FEATURE_NAMES]\n",
        "\n",
        "print(f\"ä½¿ç”¨çš„ç‰¹å¾æ•°é‡: {len(FEATURE_NAMES)}\")\n",
        "print(f\"ä½¿ç”¨çš„ç‰¹å¾: {FEATURE_NAMES}\")\n",
        "print(f\"å‰4ä¸ªç‰¹å¾æ˜¯å››å…ƒæ•°: {FEATURE_NAMES[:4]}\")\n",
        "\n",
        "tf.keras.utils.set_random_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ===================================================================\n",
        "# 2. åŠ è½½é¢„å¤„ç†å¥½çš„æ•°æ®ï¼ˆåŸå§‹æ•°æ®ï¼Œæœªæ ‡å‡†åŒ–ï¼‰\n",
        "# ===================================================================\n",
        "print(\"\\nåŠ è½½é¢„å¤„ç†å¥½çš„æ•°æ®ï¼ˆåŸå§‹ç‰¹å¾ï¼‰...\")\n",
        "agg_train_df = joblib.load(f'{PROCESSED_DATA_DIR}/processed_train_data_raw.joblib')\n",
        "\n",
        "if DEBUG:\n",
        "    agg_train_df = agg_train_df.head(2000)\n",
        "\n",
        "sequences_full = agg_train_df['sequence'].tolist()\n",
        "sequences = [seq[:, FEATURE_INDICES] for seq in sequences_full]\n",
        "\n",
        "# ===================================================================\n",
        "# ä¿®å¤é›¶å››å…ƒæ•°ï¼ˆç¼ºå¤±å€¼å¡«å……å¯¼è‡´çš„é—®é¢˜ï¼‰- ä¿ç•™\n",
        "# ===================================================================\n",
        "def fix_zero_quaternions(sequences):\n",
        "    \"\"\"\n",
        "    ä¿®å¤é›¶å››å…ƒæ•°ï¼Œå°†å…¶æ›¿æ¢ä¸ºå•ä½å››å…ƒæ•° [1,0,0,0] (w,x,y,zæ ¼å¼)\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤é›¶å››å…ƒæ•°...\")\n",
        "    total_frames = 0\n",
        "    zero_frames = 0\n",
        "    problematic_sequences = []\n",
        "\n",
        "    for seq_idx, seq in enumerate(sequences):\n",
        "        # è®¡ç®—æ¯å¸§å››å…ƒæ•°çš„èŒƒæ•°\n",
        "        quat_norms = np.linalg.norm(seq[:, :4], axis=1)\n",
        "        total_frames += len(quat_norms)\n",
        "\n",
        "        # æ‰¾å‡ºé›¶å››å…ƒæ•°ï¼ˆèŒƒæ•°æ¥è¿‘0ï¼‰\n",
        "        zero_mask = quat_norms < 1e-8\n",
        "        num_zeros = np.sum(zero_mask)\n",
        "\n",
        "        if num_zeros > 0:\n",
        "            # ä¿®å¤ï¼šå°†é›¶å››å…ƒæ•°æ›¿æ¢ä¸ºå•ä½å››å…ƒæ•° [1,0,0,0]\n",
        "            seq[zero_mask, :4] = [1.0, 0.0, 0.0, 0.0]\n",
        "            zero_frames += num_zeros\n",
        "            problematic_sequences.append(seq_idx)\n",
        "\n",
        "    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯\n",
        "    if zero_frames > 0:\n",
        "        print(f\"   âœ… ä¿®å¤äº† {zero_frames}/{total_frames} ä¸ªé›¶å››å…ƒæ•°å¸§ \"\n",
        "              f\"({100*zero_frames/total_frames:.2f}%)\")\n",
        "        print(f\"   ğŸ“Š æ¶‰åŠ {len(problematic_sequences)} ä¸ªåºåˆ—\")\n",
        "    else:\n",
        "        print(f\"   âœ… æœªå‘ç°é›¶å››å…ƒæ•°ï¼Œæ•°æ®è´¨é‡è‰¯å¥½\")\n",
        "\n",
        "    # éªŒè¯ä¿®å¤åçš„ç»“æœ\n",
        "    print(\"\\n   éªŒè¯ä¿®å¤åçš„å››å…ƒæ•°èŒƒæ•°:\")\n",
        "    all_quats = np.vstack([seq[:, :4] for seq in sequences])\n",
        "    fixed_norms = np.linalg.norm(all_quats, axis=1)\n",
        "    print(f\"     å‡å€¼: {np.mean(fixed_norms):.6f}\")\n",
        "    print(f\"     æ ‡å‡†å·®: {np.std(fixed_norms):.6f}\")\n",
        "    print(f\"     èŒƒå›´: [{np.min(fixed_norms):.6f}, {np.max(fixed_norms):.6f}]\")\n",
        "\n",
        "    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é—®é¢˜\n",
        "    remaining_zeros = np.sum(fixed_norms < 1e-8)\n",
        "    if remaining_zeros > 0:\n",
        "        print(f\"   âš ï¸ è­¦å‘Š: ä»æœ‰ {remaining_zeros} ä¸ªé›¶å››å…ƒæ•°æœªä¿®å¤\")\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# æ‰§è¡Œä¿®å¤\n",
        "sequences = fix_zero_quaternions(sequences)\n",
        "\n",
        "labels = agg_train_df['label'].values\n",
        "groups = agg_train_df['subject'].values\n",
        "print(f\"\\næˆåŠŸåŠ è½½å¹¶å¤„ç† {len(sequences)} æ¡åºåˆ—æ•°æ®ã€‚\")\n",
        "\n",
        "# ===================================================================\n",
        "# 3. æ•°æ®æ ‡å‡†åŒ–å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆ - ä¸å¯¹å››å…ƒæ•°è¿›è¡Œå½’ä¸€åŒ–ï¼‰\n",
        "# ===================================================================\n",
        "def standardize_sequences_with_quaternion(sequences_train, sequences_val, fit_on_train=True):\n",
        "    train_quat = [seq[:, :4] for seq in sequences_train]\n",
        "    train_other = [seq[:, 4:] for seq in sequences_train]\n",
        "    val_quat = [seq[:, :4] for seq in sequences_val]\n",
        "    val_other = [seq[:, 4:] for seq in sequences_val]\n",
        "\n",
        "    # æ ‡å‡†åŒ–éå››å…ƒæ•°ç‰¹å¾\n",
        "    train_other_flat = np.vstack(train_other)\n",
        "    val_other_flat = np.vstack(val_other)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    if fit_on_train:\n",
        "        train_other_scaled = scaler.fit_transform(train_other_flat)\n",
        "        val_other_scaled = scaler.transform(val_other_flat)\n",
        "    else:\n",
        "        all_other_flat = np.vstack([train_other_flat, val_other_flat])\n",
        "        scaler.fit(all_other_flat)\n",
        "        train_other_scaled = scaler.transform(train_other_flat)\n",
        "        val_other_scaled = scaler.transform(val_other_flat)\n",
        "\n",
        "    sequences_train_scaled, sequences_val_scaled = [], []\n",
        "\n",
        "    # è®­ç»ƒé›† - ç›´æ¥ä½¿ç”¨åŸå§‹å››å…ƒæ•°ï¼ˆä¸å½’ä¸€åŒ–ï¼‰\n",
        "    train_start = 0\n",
        "    for i, seq in enumerate(sequences_train):\n",
        "        seq_len = len(seq)\n",
        "        quat = train_quat[i]  # ç›´æ¥ä½¿ç”¨ï¼Œä¸å½’ä¸€åŒ–\n",
        "        other_scaled = train_other_scaled[train_start:train_start+seq_len]\n",
        "        seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "        sequences_train_scaled.append(seq_scaled)\n",
        "        train_start += seq_len\n",
        "\n",
        "    # éªŒè¯é›† - åŒæ ·å¤„ç†\n",
        "    val_start = 0\n",
        "    for i, seq in enumerate(sequences_val):\n",
        "        seq_len = len(seq)\n",
        "        quat = val_quat[i]  # ç›´æ¥ä½¿ç”¨ï¼Œä¸å½’ä¸€åŒ–\n",
        "        other_scaled = val_other_scaled[val_start:val_start+seq_len]\n",
        "        seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "        sequences_val_scaled.append(seq_scaled)\n",
        "        val_start += seq_len\n",
        "\n",
        "    return sequences_train_scaled, sequences_val_scaled, scaler\n",
        "\n",
        "# ===================================================================\n",
        "# 4. å››å…ƒæ•°å·¥å…·å‡½æ•°\n",
        "# ===================================================================\n",
        "def normalize_quaternion(quat):\n",
        "    norm = tf.norm(quat, axis=-1, keepdims=True)\n",
        "    norm = tf.maximum(norm, 1e-8)\n",
        "    return quat / norm\n",
        "\n",
        "def quaternion_slerp(q1, q2, t):\n",
        "    dot = tf.reduce_sum(q1 * q2, axis=-1, keepdims=True)\n",
        "    q2 = tf.where(dot < 0, -q2, q2)\n",
        "    dot = tf.abs(dot)\n",
        "    dot = tf.clip_by_value(dot, -1.0, 1.0)\n",
        "    theta = tf.acos(dot)\n",
        "    sin_theta = tf.sin(theta)\n",
        "    w1 = tf.where(sin_theta > 1e-4, tf.sin((1.0 - t) * theta) / sin_theta, 1.0 - t)\n",
        "    w2 = tf.where(sin_theta > 1e-4, tf.sin(t * theta) / sin_theta, t)\n",
        "    result = w1 * q1 + w2 * q2\n",
        "    return normalize_quaternion(result)\n",
        "\n",
        "# ===================================================================\n",
        "# 5. å››å…ƒæ•°å®‰å…¨çš„æ•°æ®å¢å¼ºï¼ˆä¿ç•™å¢å¼ºåçš„å½’ä¸€åŒ–ï¼‰\n",
        "# ===================================================================\n",
        "def safe_tf_time_stretch(sequence, stretch_range=(0.8, 1.2)):\n",
        "    seq_len_float = tf.cast(tf.shape(sequence)[0], tf.float32)\n",
        "    stretch_factor = tf.random.uniform(shape=(), minval=stretch_range[0], maxval=stretch_range[1])\n",
        "    new_len = tf.cast(seq_len_float / stretch_factor, tf.int32)\n",
        "    quat_features = sequence[:, :4]\n",
        "    other_features = sequence[:, 4:]\n",
        "    quat_reshaped = tf.reshape(quat_features, [1, tf.shape(quat_features)[0], 1, 4])\n",
        "    quat_stretched = tf.image.resize(quat_reshaped, [new_len, 1], method=tf.image.ResizeMethod.BILINEAR)\n",
        "    quat_stretched = tf.reshape(quat_stretched, [new_len, 4])\n",
        "    quat_stretched = normalize_quaternion(quat_stretched)  # ä¿ç•™ï¼šæ‹‰ä¼¸åéœ€è¦å½’ä¸€åŒ–\n",
        "    if tf.shape(other_features)[1] > 0:\n",
        "        other_reshaped = tf.reshape(other_features, [1, tf.shape(other_features)[0], 1, tf.shape(other_features)[1]])\n",
        "        other_stretched = tf.image.resize(other_reshaped, [new_len, 1], method=tf.image.ResizeMethod.BILINEAR)\n",
        "        other_stretched = tf.reshape(other_stretched, [new_len, tf.shape(other_features)[1]])\n",
        "        stretched_sequence = tf.concat([quat_stretched, other_stretched], axis=1)\n",
        "    else:\n",
        "        stretched_sequence = quat_stretched\n",
        "    return stretched_sequence\n",
        "\n",
        "def safe_tf_augment(sequence, label, aug_prob=0.5):\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        sequence = safe_tf_time_stretch(sequence)\n",
        "        # é‡ç®—magnitude\n",
        "        lin_acc_xyz = sequence[:, 4:7]\n",
        "        sequence = tf.concat([\n",
        "            sequence[:, :4],  # å››å…ƒæ•°\n",
        "            lin_acc_xyz,\n",
        "            tf.norm(lin_acc_xyz, axis=1, keepdims=True),  # é‡ç®—lin_mag\n",
        "            sequence[:, 8:]  # ä¿ç•™å…¶ä»–ç‰¹å¾ä¸å˜\n",
        "        ], axis=1)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        seq_len = tf.shape(sequence)[0]\n",
        "        max_shift = tf.cast(tf.cast(seq_len, tf.float32) * 0.1, tf.int32)\n",
        "        shift = tf.random.uniform(shape=(), minval=-max_shift, maxval=max_shift, dtype=tf.int32)\n",
        "        sequence = tf.roll(sequence, shift=shift, axis=0)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        quat_features = sequence[:, :4]\n",
        "        other_features = sequence[:, 4:]\n",
        "        quat_noise = tf.random.normal(shape=tf.shape(quat_features), stddev=0.015)\n",
        "        quat_features = normalize_quaternion(quat_features + quat_noise)  # ä¿ç•™ï¼šåŠ å™ªåéœ€è¦å½’ä¸€åŒ–\n",
        "        if tf.shape(other_features)[1] > 0:\n",
        "            other_noise = tf.random.normal(shape=tf.shape(other_features), stddev=0.03)\n",
        "            other_features = other_features + other_noise\n",
        "            sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "        else:\n",
        "            sequence = quat_features\n",
        "    #if tf.random.uniform(()) < aug_prob:\n",
        "        #quat_features = sequence[:, :4]\n",
        "        #other_features = sequence[:, 4:]\n",
        "        #if tf.shape(other_features)[1] > 0:\n",
        "            #scale_factor = tf.random.uniform(shape=(), minval=0.9, maxval=1.1)\n",
        "            #other_features = other_features * scale_factor\n",
        "            #sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        seq_len = tf.shape(sequence)[0]\n",
        "        mask_ratio = 0.15\n",
        "        mask_length = tf.cast(tf.cast(seq_len, tf.float32) * mask_ratio, tf.int32)\n",
        "        if mask_length > 0:\n",
        "            start_idx = tf.random.uniform(shape=(), maxval=tf.maximum(1, seq_len - mask_length), dtype=tf.int32)\n",
        "            quat_features = sequence[:, :4]\n",
        "            other_features = sequence[:, 4:]\n",
        "            if tf.shape(other_features)[1] > 0:\n",
        "                mask = tf.concat([\n",
        "                    tf.ones([start_idx, tf.shape(other_features)[1]]),\n",
        "                    tf.zeros([mask_length, tf.shape(other_features)[1]]),\n",
        "                    tf.ones([seq_len - start_idx - mask_length, tf.shape(other_features)[1]])\n",
        "                ], axis=0)\n",
        "                other_features = other_features * mask\n",
        "                sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "    return sequence, label\n",
        "\n",
        "# ===================================================================\n",
        "# 6. TensorFlow æ•°æ®ç®¡é“ï¼ˆå››å…ƒæ•°å®‰å…¨ç‰ˆæœ¬ï¼‰\n",
        "# ===================================================================\n",
        "def create_tf_dataset(X, y, batch_size, is_training=True, use_augmentation=True, use_mixup=False, mixup_alpha=0.2):\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: ((seq, label) for seq, label in zip(X, y)),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, len(FEATURE_NAMES)), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(buffer_size=len(X)).repeat()\n",
        "        if use_augmentation:\n",
        "            dataset = dataset.map(safe_tf_augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.map(lambda seq, label: (seq[:MAX_SEQ_LENGTH], label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.padded_batch(batch_size, padded_shapes=([MAX_SEQ_LENGTH, len(FEATURE_NAMES)], []), padding_values=(0.0, 0), drop_remainder=True)\n",
        "        if use_mixup:\n",
        "            mixup_partner_ds = dataset.shuffle(buffer_size=len(X)//batch_size if batch_size > 0 else 1)\n",
        "            dataset = tf.data.Dataset.zip((dataset, mixup_partner_ds))\n",
        "            def safe_mixup_map(data1, data2):\n",
        "                (seq1, label1), (seq2, label2) = data1, data2\n",
        "                dist = tfp.distributions.Beta(mixup_alpha, mixup_alpha)\n",
        "                lambda_ = dist.sample()\n",
        "                quat1, quat2 = seq1[:, :, :4], seq2[:, :, :4]\n",
        "                other1, other2 = seq1[:, :, 4:], seq2[:, :, 4:]\n",
        "                mixed_quat = quaternion_slerp(quat1, quat2, 1.0 - lambda_)  # SLERPå†…éƒ¨ä¼šå½’ä¸€åŒ–\n",
        "                mixed_other = lambda_ * other1 + (1 - lambda_) * other2\n",
        "                mixed_seq = tf.concat([mixed_quat, mixed_other], axis=-1)\n",
        "                label1_oh = tf.one_hot(tf.cast(label1, tf.int32), N_CLASSES)\n",
        "                label2_oh = tf.one_hot(tf.cast(label2, tf.int32), N_CLASSES)\n",
        "                mixed_label = lambda_ * label1_oh + (1 - lambda_) * label2_oh\n",
        "                return mixed_seq, mixed_label\n",
        "            dataset = dataset.map(safe_mixup_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        else:\n",
        "            dataset = dataset.map(lambda seq, label: (seq, tf.one_hot(tf.cast(label, tf.int32), N_CLASSES)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        dataset = dataset.map(lambda seq, label: (seq[:MAX_SEQ_LENGTH], label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.padded_batch(batch_size, padded_shapes=([MAX_SEQ_LENGTH, len(FEATURE_NAMES)], []), padding_values=(0.0, 0), drop_remainder=False)\n",
        "        dataset = dataset.map(lambda seq, label: (seq, tf.one_hot(tf.cast(label, tf.int32), N_CLASSES)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ===================================================================\n",
        "# 7. Keras æ¨¡å‹å®šä¹‰\n",
        "# ===================================================================\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class SumPooling1D(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SumPooling1D, self).__init__(**kwargs)\n",
        "    def call(self, inputs):\n",
        "        return tf.reduce_sum(inputs, axis=1)\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[2])\n",
        "    def get_config(self):\n",
        "        base_config = super(SumPooling1D, self).get_config()\n",
        "        return base_config\n",
        "\n",
        "def se_block(input_tensor, reduction=8):\n",
        "    channels = input_tensor.shape[-1]\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(input_tensor)\n",
        "    x = tf.keras.layers.Dense(channels // reduction, activation='relu', use_bias=False)(x)\n",
        "    x = tf.keras.layers.Dense(channels, activation='sigmoid', use_bias=False)(x)\n",
        "    x = tf.keras.layers.Reshape((1, channels))(x)\n",
        "    return tf.keras.layers.Multiply()([input_tensor, x])\n",
        "\n",
        "def residual_se_cnn_block(input_tensor, out_channels, kernel_size, pool_size=2, dropout=0.3, dilation_rate=1):\n",
        "    in_channels = input_tensor.shape[-1]\n",
        "    if in_channels != out_channels:\n",
        "        shortcut = tf.keras.layers.Conv1D(out_channels, 1, use_bias=False)(input_tensor)\n",
        "        shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = input_tensor\n",
        "    x = tf.keras.layers.Conv1D(out_channels, kernel_size, padding='same', use_bias=False)(input_tensor)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv1D(out_channels, kernel_size, padding='same', use_bias=False, dilation_rate=dilation_rate)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = se_block(x)\n",
        "    x = tf.keras.layers.Add()([shortcut, x])\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    return x\n",
        "\n",
        "def attention_layer(input_tensor):\n",
        "    x = tf.keras.layers.LayerNormalization(axis=[1, 2])(input_tensor)\n",
        "    scores = tf.keras.layers.Dense(1, activation='tanh', name='attention_scores')(x)\n",
        "    weights = tf.keras.layers.Softmax(axis=1, name='attention_weights')(scores)\n",
        "    context = tf.keras.layers.Multiply()([input_tensor, weights])  # æ³¨æ„è¿™é‡Œç”¨åŸå§‹è¾“å…¥\n",
        "    context = SumPooling1D()(context)\n",
        "    return context\n",
        "\n",
        "def build_model(imu_dim=len(FEATURE_NAMES), n_classes=N_CLASSES, max_seq_len=MAX_SEQ_LENGTH):\n",
        "    input_layer = tf.keras.layers.Input(shape=(max_seq_len, imu_dim), name='input_sequence')\n",
        "    rot_features = tf.keras.layers.Lambda(lambda x: x[:, :, 0:4], name='rot_features')(input_layer)\n",
        "    acc_features = tf.keras.layers.Lambda(lambda x: x[:, :, 4:8], name='acc_features')(input_layer)\n",
        "    vel_features = tf.keras.layers.Lambda(lambda x: x[:, :, 8:12], name='vel_features')(input_layer)\n",
        "    other_features = tf.keras.layers.Lambda(lambda x: x[:, :, 12:], name='other_features')(input_layer)\n",
        "    rot_branch = residual_se_cnn_block(rot_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    rot_branch = residual_se_cnn_block(rot_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    acc_branch = residual_se_cnn_block(acc_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    acc_branch = residual_se_cnn_block(acc_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    vel_branch = residual_se_cnn_block(vel_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    vel_branch = residual_se_cnn_block(vel_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    if other_features.shape[-1] > 0:\n",
        "        other_branch = residual_se_cnn_block(other_features, out_channels=32, kernel_size=3, pool_size=2, dropout=0.2)\n",
        "        other_branch = residual_se_cnn_block(other_branch, out_channels=64, kernel_size=3, pool_size=2, dropout=0.35)\n",
        "        merged = tf.keras.layers.Concatenate(axis=-1)([rot_branch, acc_branch, vel_branch, other_branch])\n",
        "    else:\n",
        "        merged = tf.keras.layers.Concatenate(axis=-1)([rot_branch, acc_branch, vel_branch])\n",
        "    x = residual_se_cnn_block(merged, out_channels=256, kernel_size=3, pool_size=1, dropout=0.4)\n",
        "    x = residual_se_cnn_block(x, out_channels=512, kernel_size=5, pool_size=1, dropout=0.45)\n",
        "    x = attention_layer(x)\n",
        "    x = tf.keras.layers.Dense(128, use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dropout(0.45)(x)\n",
        "    x = tf.keras.layers.Dense(64, use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dropout(0.25)(x)\n",
        "    output_layer = tf.keras.layers.Dense(n_classes, activation='linear', name='output_logits')(x)\n",
        "    return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# ===================================================================\n",
        "# 8. è®­ç»ƒã€å›è°ƒå‡½æ•°ä¸è¯„ä¼°\n",
        "# ===================================================================\n",
        "sys.path.append('/kaggle/usr/lib/cmi_2025_metric_copy_for_import')\n",
        "try:\n",
        "    import cmi_2025_metric_copy_for_import as metric\n",
        "    print(\"æˆåŠŸå¯¼å…¥æœ¬åœ°è¯„ä¼°æŒ‡æ ‡æ–‡ä»¶ã€‚\")\n",
        "    def get_competition_score(true, pred):\n",
        "        true_labels = [IDX2LABEL[x] for x in true]\n",
        "        pred_labels = [IDX2LABEL[x] for x in pred]\n",
        "        true_df = pd.DataFrame({'id': range(len(true_labels)), 'gesture': true_labels})\n",
        "        pred_df = pd.DataFrame({'id': range(len(pred_labels)), 'gesture': pred_labels})\n",
        "        return metric.score(true_df, pred_df, 'id')\n",
        "except ImportError:\n",
        "    print(\"æ— æ³•å¯¼å…¥æœ¬åœ°è¯„ä¼°æŒ‡æ ‡æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ accuracy ä½œä¸ºæ›¿ä»£ã€‚\")\n",
        "    def get_competition_score(true, pred):\n",
        "        return accuracy_score(true, pred)\n",
        "\n",
        "class CompetitionScoreCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        super().__init__()\n",
        "        self.val_data = validation_data\n",
        "        self.val_labels = np.concatenate([y for x, y in validation_data], axis=0)\n",
        "        self.val_labels = np.argmax(self.val_labels, axis=1)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_preds = self.model.predict(self.val_data, verbose=0)\n",
        "        val_preds = np.argmax(val_preds, axis=1)\n",
        "        score = get_competition_score(self.val_labels, val_preds)\n",
        "        print(f\" - val_score: {score:.4f}\", end=\"\")\n",
        "        logs['val_score'] = score\n",
        "\n",
        "def plot_training_history(history, fold):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax1.plot(history.history['loss'], label='Train Loss')\n",
        "    ax1.plot(history.history['val_loss'], label='Val Loss')\n",
        "    ax1.set_title(f'Fold {fold} - Loss'); ax1.set_xlabel('Epoch'); ax1.legend()\n",
        "    ax2.plot(history.history['categorical_accuracy'], label='Train Accuracy')\n",
        "    ax2.plot(history.history['val_categorical_accuracy'], label='Val Accuracy')\n",
        "    ax2.set_title(f'Fold {fold} - Accuracy'); ax2.set_xlabel('Epoch'); ax2.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{OUTPUT_DIR}/fold_{fold}_training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, base_lr, warmup_steps, total_steps, min_lr=1e-5):\n",
        "        super().__init__()\n",
        "        self.base_lr, self.warmup_steps, self.total_steps, self.min_lr = base_lr, warmup_steps, total_steps, min_lr\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warm = self.base_lr * (step / tf.cast(self.warmup_steps, tf.float32))\n",
        "        progress = (step - self.warmup_steps) / tf.maximum(1.0, self.total_steps - self.warmup_steps)\n",
        "        cosine = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + tf.cos(np.pi * tf.clip_by_value(progress, 0.0, 1.0)))\n",
        "        return tf.where(step < self.warmup_steps, warm, cosine)\n",
        "    def get_config(self):\n",
        "        return {\"base_lr\": self.base_lr, \"warmup_steps\": self.warmup_steps, \"total_steps\": self.total_steps, \"min_lr\": self.min_lr}\n",
        "\n",
        "# ===================================================================\n",
        "# 9. æ•°æ®éªŒè¯å‡½æ•°\n",
        "# ===================================================================\n",
        "def check_quaternion_norm(sequences, tolerance=0.01):\n",
        "    if isinstance(sequences, list):\n",
        "        all_norms = np.concatenate([np.linalg.norm(seq[:, :4], axis=1) for seq in sequences])\n",
        "    else:\n",
        "        quat = sequences[:, :, :4] if len(sequences.shape) == 3 else sequences[:, :4]\n",
        "        all_norms = np.linalg.norm(quat, axis=-1).flatten()\n",
        "    stats = {'mean': np.mean(all_norms), 'std': np.std(all_norms), 'min': np.min(all_norms), 'max': np.max(all_norms), 'is_valid': np.all(np.abs(all_norms - 1.0) < tolerance)}\n",
        "    return stats['is_valid'], stats\n",
        "\n",
        "# ===================================================================\n",
        "# 10. K-Fold äº¤å‰éªŒè¯è®­ç»ƒæµç¨‹\n",
        "# ===================================================================\n",
        "def run_kfold_training(sequences, labels, groups, n_folds=5, **kwargs):\n",
        "    print(\"ğŸš€ å¼€å§‹ K-Fold è®­ç»ƒï¼ˆç®€åŒ–ç‰ˆ - ä»…ä¿®å¤é›¶å››å…ƒæ•°ï¼‰...\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nğŸ“Š éªŒè¯æ•°æ®çš„å››å…ƒæ•°èŒƒæ•°ï¼ˆå·²ä¿®å¤é›¶å››å…ƒæ•°ï¼‰...\")\n",
        "    is_valid, stats = check_quaternion_norm(sequences)\n",
        "    print(f\"   å¹³å‡èŒƒæ•°: {stats['mean']:.4f} (Â±{stats['std']:.4f})\")\n",
        "    print(f\"   èŒƒæ•°èŒƒå›´: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
        "\n",
        "    kfold_results = []\n",
        "    sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    sequences_np = np.array(sequences, dtype=object)\n",
        "\n",
        "    for fold_num, (train_idx, val_idx) in enumerate(sgkf.split(sequences_np, labels, groups), 1):\n",
        "        print(f\"\\n{'='*60}\\nğŸ”„ è®­ç»ƒ Fold {fold_num}/{n_folds}\\n{'='*60}\")\n",
        "        X_train_raw, X_val_raw = sequences_np[train_idx], sequences_np[val_idx]\n",
        "        y_train, y_val = labels[train_idx], labels[val_idx]\n",
        "\n",
        "        print(f\"ğŸ“Š Fold {fold_num}: æ ‡å‡†åŒ–æ•°æ®ï¼ˆå››å…ƒæ•°ä¿æŒåŸå§‹å€¼ï¼‰...\")\n",
        "        X_train_scaled, X_val_scaled, scaler = standardize_sequences_with_quaternion(X_train_raw.tolist(), X_val_raw.tolist(), fit_on_train=True)\n",
        "        is_valid_train, stats_train = check_quaternion_norm(X_train_scaled)\n",
        "        print(f\"   è®­ç»ƒé›†å››å…ƒæ•°èŒƒæ•°: {stats_train['mean']:.4f} (Â±{stats_train['std']:.4f})\")\n",
        "        scaler_path = f'{OUTPUT_DIR}/fold_{fold_num}_scaler.joblib'\n",
        "        joblib.dump(scaler, scaler_path)\n",
        "        print(f\"ğŸ’¾ ä¿å­˜ Fold {fold_num} çš„ StandardScaler åˆ° {scaler_path}\")\n",
        "\n",
        "        train_ds = create_tf_dataset(X_train_scaled, y_train, kwargs['batch_size'], is_training=True, use_augmentation=True, use_mixup=kwargs['use_mixup'], mixup_alpha=kwargs.get('mixup_alpha', 0.2))\n",
        "        val_ds = create_tf_dataset(X_val_scaled, y_val, kwargs['batch_size'], is_training=False)\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "        model = build_model()\n",
        "        steps_per_epoch = len(X_train_scaled) // kwargs['batch_size']\n",
        "        if steps_per_epoch == 0:\n",
        "            print(f\"é”™è¯¯: Fold {fold_num} çš„è®­ç»ƒæ ·æœ¬æ•°å°äºbatch_size\")\n",
        "            continue\n",
        "\n",
        "        total_steps = steps_per_epoch * kwargs['num_epochs']\n",
        "        lr_sched = WarmupCosine(base_lr=kwargs['learning_rate'], warmup_steps=int(0.05 * total_steps), total_steps=total_steps, min_lr=1e-5)\n",
        "        optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_sched, weight_decay=5e-3)\n",
        "        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=kwargs['label_smoothing'])\n",
        "        model.compile(optimizer=optimizer, loss=loss_fn, metrics=['categorical_accuracy'])\n",
        "\n",
        "        model_save_path = f'{OUTPUT_DIR}/fold_{fold_num}_model.keras'\n",
        "        callbacks = [\n",
        "            CompetitionScoreCallback(validation_data=val_ds),\n",
        "            tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path, save_best_only=True, monitor='val_score', mode='max', verbose=1),\n",
        "            tf.keras.callbacks.EarlyStopping(monitor='val_score', patience=kwargs['patience'], mode='max', verbose=1, restore_best_weights=True),\n",
        "        ]\n",
        "\n",
        "        print(f\"ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ Fold {fold_num}...\")\n",
        "        history = model.fit(train_ds, epochs=kwargs['num_epochs'], validation_data=val_ds, callbacks=callbacks, steps_per_epoch=steps_per_epoch, verbose=2)\n",
        "        plot_training_history(history, fold_num)\n",
        "\n",
        "        print(f\"\\nğŸ¯ Fold {fold_num} æœ€ç»ˆè¯„ä¼°:\")\n",
        "        y_val_unbatched = np.concatenate([y for x, y in val_ds], axis=0)\n",
        "        y_val_labels = np.argmax(y_val_unbatched, axis=1)\n",
        "        val_preds_final = model.predict(val_ds)\n",
        "        val_preds_final_labels = np.argmax(val_preds_final, axis=1)\n",
        "        y_val_labels = y_val_labels[:len(val_preds_final_labels)]\n",
        "\n",
        "        final_score = get_competition_score(y_val_labels, val_preds_final_labels)\n",
        "        final_acc = np.mean(val_preds_final_labels == y_val_labels)\n",
        "        print(f\"       éªŒè¯é›† Accuracy: {final_acc:.4f}\")\n",
        "        print(f\"       éªŒè¯é›† Score: {final_score:.4f}\")\n",
        "\n",
        "        print(f\"\\nğŸ“Š Fold {fold_num} - åˆ†ç±»æŠ¥å‘Š:\")\n",
        "        print(classification_report(\n",
        "            y_val_labels,\n",
        "            val_preds_final_labels,\n",
        "            target_names=list(IDX2LABEL.values()),\n",
        "            digits=4\n",
        "        ))\n",
        "\n",
        "        kfold_results.append({'fold': fold_num, 'val_accuracy': final_acc, 'val_score': final_score})\n",
        "\n",
        "    return kfold_results\n",
        "\n",
        "# ===================================================================\n",
        "# 11. æµ‹è¯•é›†é¢„æµ‹å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆ - ä¸å½’ä¸€åŒ–å››å…ƒæ•°ï¼‰\n",
        "# ===================================================================\n",
        "def predict_test_data(test_sequences_raw, output_dir=OUTPUT_DIR, n_folds=5):\n",
        "    print(\"ğŸ”® å¼€å§‹æµ‹è¯•é›†é¢„æµ‹...\")\n",
        "    all_predictions = []\n",
        "    for fold_num in range(1, n_folds + 1):\n",
        "        print(f\"\\nå¤„ç† Fold {fold_num}...\")\n",
        "        scaler_path = f'{output_dir}/fold_{fold_num}_scaler.joblib'\n",
        "        scaler = joblib.load(scaler_path)\n",
        "        print(f\"   åŠ è½½scaler: {scaler_path}\")\n",
        "        test_quat = [seq[:, :4] for seq in test_sequences_raw]\n",
        "        test_other = [seq[:, 4:] for seq in test_sequences_raw]\n",
        "        test_other_flat = np.vstack(test_other)\n",
        "        test_other_scaled = scaler.transform(test_other_flat)\n",
        "        test_sequences_scaled = []\n",
        "        start = 0\n",
        "        for i, seq in enumerate(test_sequences_raw):\n",
        "            seq_len = len(seq)\n",
        "            quat = test_quat[i]  # ç›´æ¥ä½¿ç”¨ï¼Œä¸å½’ä¸€åŒ–\n",
        "            other_scaled = test_other_scaled[start:start+seq_len]\n",
        "            seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "            test_sequences_scaled.append(seq_scaled)\n",
        "            start += seq_len\n",
        "        model_path = f'{output_dir}/fold_{fold_num}_model.keras'\n",
        "        custom_objects = {'WarmupCosine': WarmupCosine, 'SumPooling1D': SumPooling1D}\n",
        "        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects, compile=False)\n",
        "        print(f\"   åŠ è½½æ¨¡å‹: {model_path}\")\n",
        "        dummy_labels = np.zeros(len(test_sequences_scaled), dtype=np.int32)\n",
        "        test_ds = create_tf_dataset(test_sequences_scaled, dummy_labels, batch_size=64, is_training=False)\n",
        "        fold_predictions = model.predict(test_ds, verbose=0)\n",
        "        all_predictions.append(fold_predictions)\n",
        "        print(f\"   å®ŒæˆFold {fold_num}çš„é¢„æµ‹\")\n",
        "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
        "    final_predictions = np.argmax(ensemble_predictions, axis=1)\n",
        "    print(f\"\\nâœ… æµ‹è¯•é›†é¢„æµ‹å®Œæˆï¼\")\n",
        "    return final_predictions, ensemble_predictions\n",
        "\n",
        "# ===================================================================\n",
        "# 12. ä¸»æ‰§è¡Œé€»è¾‘\n",
        "# ===================================================================\n",
        "if __name__ == '__main__':\n",
        "    training_params = {\n",
        "        'num_epochs': 3 if DEBUG else 100,\n",
        "        'learning_rate': 0.001,\n",
        "        'patience': 20,\n",
        "        'batch_size': 64,\n",
        "        'label_smoothing': 0.1,\n",
        "        'use_mixup': True,\n",
        "        'mixup_alpha': 0.4\n",
        "    }\n",
        "\n",
        "    if TRAIN:\n",
        "        results = run_kfold_training(sequences, labels, groups, n_folds=5, **training_params)\n",
        "        print(\"\\n\\n\" + \"=\"*60 + \"\\nğŸ‰ K-Fold è®­ç»ƒæ€»ç»“\\n\" + \"=\"*60)\n",
        "        if results:\n",
        "            val_scores = [r['val_score'] for r in results]\n",
        "            val_accs = [r['val_accuracy'] for r in results]\n",
        "            print(f\"å¹³å‡éªŒè¯é›† Score: {np.mean(val_scores):.4f} Â± {np.std(val_scores):.4f}\")\n",
        "            print(f\"å¹³å‡éªŒè¯é›† Accuracy: {np.mean(val_accs):.4f} Â± {np.std(val_accs):.4f}\")\n",
        "            feature_selection_info = {'all_features': ALL_FEATURE_NAMES, 'selected_features': FEATURE_NAMES, 'feature_indices': FEATURE_INDICES, 'num_features': len(FEATURE_NAMES), 'quaternion_safe': True}\n",
        "            results_with_features = {'kfold_results': results, 'feature_selection': feature_selection_info, 'preprocessing_note': 'ç®€åŒ–ç‰ˆï¼šä»…ä¿®å¤é›¶å››å…ƒæ•°ï¼Œä¸è¿›è¡Œé˜ˆå€¼å½’ä¸€åŒ–', 'lr_schedule': 'WarmupCosine'}\n",
        "            with open(f'{OUTPUT_DIR}/kfold_results.json', 'w') as f:\n",
        "                json.dump(results_with_features, f, indent=2)\n",
        "\n",
        "    print(\"\\nğŸ“ é‡è¦è¯´æ˜:\")\n",
        "    print(\"1. è‡ªåŠ¨æ£€æµ‹å¹¶ä¿®å¤é›¶å››å…ƒæ•°ï¼ˆå¡«å……ä¸ºå•ä½å››å…ƒæ•°ï¼‰\")\n",
        "    print(\"2. å››å…ƒæ•°åœ¨æ ‡å‡†åŒ–æ—¶ä¿æŒåŸå§‹å€¼ï¼Œä¸è¿›è¡Œå½’ä¸€åŒ–\")\n",
        "    print(\"3. æ•°æ®å¢å¼ºåå¯¹å››å…ƒæ•°è¿›è¡Œå¿…è¦çš„å½’ä¸€åŒ–\")\n",
        "    print(\"4. MixUpä½¿ç”¨SLERPè¿›è¡Œå››å…ƒæ•°æ’å€¼\")\n",
        "    print(\"5. æ ‡å‡†åŒ–ä»…åº”ç”¨äºéå››å…ƒæ•°ç‰¹å¾\")\n",
        "    print(\"6. æ¯æŠ˜è®­ç»ƒåä¼šæ‰“å°è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š\")\n",
        "\n",
        "# ===================================================================\n",
        "# 13. ç”Ÿæˆæäº¤æ–‡ä»¶\n",
        "# ===================================================================\n",
        "def generate_submission(submission_output_path='submission.csv'):\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\nğŸ“„ ç”Ÿæˆæäº¤æ–‡ä»¶\" + \"\\n\" + \"=\"*60)\n",
        "    print(\"åŠ è½½é¢„å¤„ç†çš„æµ‹è¯•æ•°æ®...\")\n",
        "    test_data = joblib.load(f'{PROCESSED_DATA_DIR}/processed_test_data_raw.joblib')\n",
        "    test_sequences_full = test_data['sequence'].tolist()\n",
        "    test_sequences = [seq[:, FEATURE_INDICES] for seq in test_sequences_full]\n",
        "\n",
        "    # ä¿®å¤æµ‹è¯•é›†çš„é›¶å››å…ƒæ•°\n",
        "    print(\"\\nå¤„ç†æµ‹è¯•é›†æ•°æ®...\")\n",
        "    test_sequences = fix_zero_quaternions(test_sequences)\n",
        "\n",
        "    test_sequence_ids = test_data['sequence_id'].values\n",
        "    predictions, _ = predict_test_data(test_sequences, OUTPUT_DIR, n_folds=5)\n",
        "    predicted_labels = [IDX2LABEL[pred] for pred in predictions]\n",
        "    submission_df = pd.DataFrame({'sequence_id': test_sequence_ids, 'gesture': predicted_labels})\n",
        "    submission_df.to_csv(submission_output_path, index=False)\n",
        "    print(f\"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜åˆ°: {submission_output_path}\")\n",
        "    print(\"\\nğŸ“Š é¢„æµ‹åˆ†å¸ƒ:\")\n",
        "    print(submission_df['gesture'].value_counts())\n",
        "    return submission_df"
      ],
      "metadata": {
        "id": "VzXPSyiEc3q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ä»”ç»†æ£€æŸ¥ä¿®æ”¹æ•°æ®å¢å¼ºå’Œç‰¹å¾å·¥ç¨‹å†²çªï¼Œä¿®å¤æ•°æ®å¢å¼ºåç ´åçš„æ•°æ®ç‰©ç†ç‰¹æ€§"
      ],
      "metadata": {
        "id": "tsfwPscz7XCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "#\n",
        "# å®Œæ•´çš„ TensorFlow/Keras è®­ç»ƒä»£ç  - CMI è¡Œä¸ºæ£€æµ‹\n",
        "# se1dcnn+attentionæ¨¡å‹æ¶æ„ - å››å…ƒæ•°å®‰å…¨ç‰ˆæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼šä»…ä¿®å¤é›¶å››å…ƒæ•°ï¼‰\n",
        "# 10-FOLDäº¤å‰éªŒè¯ç‰ˆæœ¬\n",
        "# cv = 0.8104,lb = 0.813\n",
        "# è¿‡æ‹Ÿåˆå·²è§£å†³\n",
        "#\n",
        "# ===================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Your input ran out of data.*\")\n",
        "\n",
        "# æŠ‘åˆ¶ TensorFlow çš„ä¸€äº›æ—¥å¿—è¾“å‡º\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# ===================================================================\n",
        "# 1. é…ç½®ä¸å…¨å±€è®¾ç½®\n",
        "# ===================================================================\n",
        "DEBUG = False\n",
        "TRAIN = True\n",
        "MAX_SEQ_LENGTH = 128\n",
        "PROCESSED_DATA_DIR = '/kaggle/input/imuonly-process/kaggle/working/processed_data_selected_features_v1' # è¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…è·¯å¾„\n",
        "OUTPUT_DIR = './saved_models_keras_fixed_test'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# ä»é¢„å¤„ç†é˜¶æ®µç”Ÿæˆçš„æ–‡ä»¶ä¸­åŠ è½½æ ‡ç­¾æ˜ å°„\n",
        "with open(f'{PROCESSED_DATA_DIR}/label_map.json', 'r') as f:\n",
        "    LABEL2IDX = json.load(f)\n",
        "IDX2LABEL = {v: k for k, v in LABEL2IDX.items()}\n",
        "N_CLASSES = len(LABEL2IDX)\n",
        "\n",
        "# åŠ è½½ç‰¹å¾é…ç½®\n",
        "with open(f'{PROCESSED_DATA_DIR}/feature_names.json', 'r') as f:\n",
        "    feature_info = json.load(f)\n",
        "    ALL_FEATURE_NAMES = feature_info['all_features']\n",
        "    TIME_FEATURE_NAMES = feature_info['time_features']\n",
        "    PSD_FEATURE_NAMES = feature_info['psd_features']\n",
        "    STAT_FEATURE_NAMES = feature_info['stat_features']\n",
        "\n",
        "# ===================================================================\n",
        "# ç‰¹å¾é€‰æ‹©é…ç½® - æ³¨æ„å‰4ä¸ªå¿…é¡»æ˜¯å››å…ƒæ•°\n",
        "# ===================================================================\n",
        "SELECTED_FEATURES = [\n",
        "    'rot_w', 'rot_x', 'rot_y', 'rot_z',           # å››å…ƒæ•°\n",
        "    'linear_acc_x', 'linear_acc_y', 'linear_acc_z', # çº¿æ€§åŠ é€Ÿåº¦\n",
        "    'linear_acc_mag',                               # çº¿æ€§åŠ é€Ÿåº¦æ¨¡é•¿\n",
        "    'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # è§’é€Ÿåº¦\n",
        "    'angular_distance',                             # è§’è·ç¦»\n",
        "    'acc_mag'                                       # åŠ é€Ÿåº¦æ¨¡é•¿\n",
        "]\n",
        "\n",
        "if SELECTED_FEATURES is not None:\n",
        "    FEATURE_NAMES = SELECTED_FEATURES\n",
        "else:\n",
        "    FEATURE_NAMES = ALL_FEATURE_NAMES\n",
        "\n",
        "FEATURE_INDICES = [ALL_FEATURE_NAMES.index(f) for f in FEATURE_NAMES]\n",
        "\n",
        "print(f\"ä½¿ç”¨çš„ç‰¹å¾æ•°é‡: {len(FEATURE_NAMES)}\")\n",
        "print(f\"ä½¿ç”¨çš„ç‰¹å¾: {FEATURE_NAMES}\")\n",
        "print(f\"å‰4ä¸ªç‰¹å¾æ˜¯å››å…ƒæ•°: {FEATURE_NAMES[:4]}\")\n",
        "\n",
        "tf.keras.utils.set_random_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ===================================================================\n",
        "# 2. åŠ è½½é¢„å¤„ç†å¥½çš„æ•°æ®ï¼ˆåŸå§‹æ•°æ®ï¼Œæœªæ ‡å‡†åŒ–ï¼‰\n",
        "# ===================================================================\n",
        "print(\"\\nåŠ è½½é¢„å¤„ç†å¥½çš„æ•°æ®ï¼ˆåŸå§‹ç‰¹å¾ï¼‰...\")\n",
        "agg_train_df = joblib.load(f'{PROCESSED_DATA_DIR}/processed_train_data_raw.joblib')\n",
        "\n",
        "if DEBUG:\n",
        "    agg_train_df = agg_train_df.head(2000)\n",
        "\n",
        "sequences_full = agg_train_df['sequence'].tolist()\n",
        "sequences = [seq[:, FEATURE_INDICES] for seq in sequences_full]\n",
        "\n",
        "# ===================================================================\n",
        "# ä¿®å¤é›¶å››å…ƒæ•°ï¼ˆç¼ºå¤±å€¼å¡«å……å¯¼è‡´çš„é—®é¢˜ï¼‰- ä¿ç•™\n",
        "# ===================================================================\n",
        "def fix_zero_quaternions(sequences):\n",
        "    \"\"\"\n",
        "    ä¿®å¤é›¶å››å…ƒæ•°ï¼Œå°†å…¶æ›¿æ¢ä¸ºå•ä½å››å…ƒæ•° [1,0,0,0] (w,x,y,zæ ¼å¼)\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤é›¶å››å…ƒæ•°...\")\n",
        "    total_frames = 0\n",
        "    zero_frames = 0\n",
        "    problematic_sequences = []\n",
        "\n",
        "    for seq_idx, seq in enumerate(sequences):\n",
        "        # è®¡ç®—æ¯å¸§å››å…ƒæ•°çš„èŒƒæ•°\n",
        "        quat_norms = np.linalg.norm(seq[:, :4], axis=1)\n",
        "        total_frames += len(quat_norms)\n",
        "\n",
        "        # æ‰¾å‡ºé›¶å››å…ƒæ•°ï¼ˆèŒƒæ•°æ¥è¿‘0ï¼‰\n",
        "        zero_mask = quat_norms < 1e-8\n",
        "        num_zeros = np.sum(zero_mask)\n",
        "\n",
        "        if num_zeros > 0:\n",
        "            # ä¿®å¤ï¼šå°†é›¶å››å…ƒæ•°æ›¿æ¢ä¸ºå•ä½å››å…ƒæ•° [1,0,0,0]\n",
        "            seq[zero_mask, :4] = [1.0, 0.0, 0.0, 0.0]\n",
        "            zero_frames += num_zeros\n",
        "            problematic_sequences.append(seq_idx)\n",
        "\n",
        "    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯\n",
        "    if zero_frames > 0:\n",
        "        print(f\"   âœ… ä¿®å¤äº† {zero_frames}/{total_frames} ä¸ªé›¶å››å…ƒæ•°å¸§ \"\n",
        "              f\"({100*zero_frames/total_frames:.2f}%)\")\n",
        "        print(f\"   ğŸ“Š æ¶‰åŠ {len(problematic_sequences)} ä¸ªåºåˆ—\")\n",
        "    else:\n",
        "        print(f\"   âœ… æœªå‘ç°é›¶å››å…ƒæ•°ï¼Œæ•°æ®è´¨é‡è‰¯å¥½\")\n",
        "\n",
        "    # éªŒè¯ä¿®å¤åçš„ç»“æœ\n",
        "    print(\"\\n   éªŒè¯ä¿®å¤åçš„å››å…ƒæ•°èŒƒæ•°:\")\n",
        "    all_quats = np.vstack([seq[:, :4] for seq in sequences])\n",
        "    fixed_norms = np.linalg.norm(all_quats, axis=1)\n",
        "    print(f\"     å‡å€¼: {np.mean(fixed_norms):.6f}\")\n",
        "    print(f\"     æ ‡å‡†å·®: {np.std(fixed_norms):.6f}\")\n",
        "    print(f\"     èŒƒå›´: [{np.min(fixed_norms):.6f}, {np.max(fixed_norms):.6f}]\")\n",
        "\n",
        "    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é—®é¢˜\n",
        "    remaining_zeros = np.sum(fixed_norms < 1e-8)\n",
        "    if remaining_zeros > 0:\n",
        "        print(f\"   âš ï¸ è­¦å‘Š: ä»æœ‰ {remaining_zeros} ä¸ªé›¶å››å…ƒæ•°æœªä¿®å¤\")\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# æ‰§è¡Œä¿®å¤\n",
        "sequences = fix_zero_quaternions(sequences)\n",
        "\n",
        "labels = agg_train_df['label'].values\n",
        "groups = agg_train_df['subject'].values\n",
        "print(f\"\\næˆåŠŸåŠ è½½å¹¶å¤„ç† {len(sequences)} æ¡åºåˆ—æ•°æ®ã€‚\")\n",
        "\n",
        "# ===================================================================\n",
        "# 3. æ•°æ®æ ‡å‡†åŒ–å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆ - ä¸å¯¹å››å…ƒæ•°è¿›è¡Œå½’ä¸€åŒ–ï¼‰\n",
        "# ===================================================================\n",
        "def standardize_sequences_with_quaternion(sequences_train, sequences_val, fit_on_train=True):\n",
        "    train_quat = [seq[:, :4] for seq in sequences_train]\n",
        "    train_other = [seq[:, 4:] for seq in sequences_train]\n",
        "    val_quat = [seq[:, :4] for seq in sequences_val]\n",
        "    val_other = [seq[:, 4:] for seq in sequences_val]\n",
        "\n",
        "    # æ ‡å‡†åŒ–éå››å…ƒæ•°ç‰¹å¾\n",
        "    train_other_flat = np.vstack(train_other)\n",
        "    val_other_flat = np.vstack(val_other)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    if fit_on_train:\n",
        "        train_other_scaled = scaler.fit_transform(train_other_flat)\n",
        "        val_other_scaled = scaler.transform(val_other_flat)\n",
        "    else:\n",
        "        all_other_flat = np.vstack([train_other_flat, val_other_flat])\n",
        "        scaler.fit(all_other_flat)\n",
        "        train_other_scaled = scaler.transform(train_other_flat)\n",
        "        val_other_scaled = scaler.transform(val_other_flat)\n",
        "\n",
        "    sequences_train_scaled, sequences_val_scaled = [], []\n",
        "\n",
        "    # è®­ç»ƒé›† - ç›´æ¥ä½¿ç”¨åŸå§‹å››å…ƒæ•°ï¼ˆä¸å½’ä¸€åŒ–ï¼‰\n",
        "    train_start = 0\n",
        "    for i, seq in enumerate(sequences_train):\n",
        "        seq_len = len(seq)\n",
        "        quat = train_quat[i]  # ç›´æ¥ä½¿ç”¨ï¼Œä¸å½’ä¸€åŒ–\n",
        "        other_scaled = train_other_scaled[train_start:train_start+seq_len]\n",
        "        seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "        sequences_train_scaled.append(seq_scaled)\n",
        "        train_start += seq_len\n",
        "\n",
        "    # éªŒè¯é›† - åŒæ ·å¤„ç†\n",
        "    val_start = 0\n",
        "    for i, seq in enumerate(sequences_val):\n",
        "        seq_len = len(seq)\n",
        "        quat = val_quat[i]  # ç›´æ¥ä½¿ç”¨ï¼Œä¸å½’ä¸€åŒ–\n",
        "        other_scaled = val_other_scaled[val_start:val_start+seq_len]\n",
        "        seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "        sequences_val_scaled.append(seq_scaled)\n",
        "        val_start += seq_len\n",
        "\n",
        "    return sequences_train_scaled, sequences_val_scaled, scaler\n",
        "\n",
        "# ===================================================================\n",
        "# 4. å››å…ƒæ•°å·¥å…·å‡½æ•°\n",
        "# ===================================================================\n",
        "def normalize_quaternion(quat):\n",
        "    norm = tf.norm(quat, axis=-1, keepdims=True)\n",
        "    norm = tf.maximum(norm, 1e-8)\n",
        "    return quat / norm\n",
        "\n",
        "def quaternion_slerp(q1, q2, t):\n",
        "    dot = tf.reduce_sum(q1 * q2, axis=-1, keepdims=True)\n",
        "    q2 = tf.where(dot < 0, -q2, q2)\n",
        "    dot = tf.abs(dot)\n",
        "    dot = tf.clip_by_value(dot, -1.0, 1.0)\n",
        "    theta = tf.acos(dot)\n",
        "    sin_theta = tf.sin(theta)\n",
        "    w1 = tf.where(sin_theta > 1e-4, tf.sin((1.0 - t) * theta) / sin_theta, 1.0 - t)\n",
        "    w2 = tf.where(sin_theta > 1e-4, tf.sin(t * theta) / sin_theta, t)\n",
        "    result = w1 * q1 + w2 * q2\n",
        "    return normalize_quaternion(result)\n",
        "\n",
        "# ===================================================================\n",
        "# 5. å››å…ƒæ•°å®‰å…¨çš„æ•°æ®å¢å¼ºï¼ˆä¿ç•™å¢å¼ºåçš„å½’ä¸€åŒ–ï¼‰\n",
        "# ===================================================================\n",
        "def safe_tf_time_stretch(sequence, stretch_range=(0.8, 1.2)):\n",
        "    seq_len_float = tf.cast(tf.shape(sequence)[0], tf.float32)\n",
        "    stretch_factor = tf.random.uniform(shape=(), minval=stretch_range[0], maxval=stretch_range[1])\n",
        "    new_len = tf.cast(seq_len_float / stretch_factor, tf.int32)\n",
        "    quat_features = sequence[:, :4]\n",
        "    other_features = sequence[:, 4:]\n",
        "    quat_reshaped = tf.reshape(quat_features, [1, tf.shape(quat_features)[0], 1, 4])\n",
        "    quat_stretched = tf.image.resize(quat_reshaped, [new_len, 1], method=tf.image.ResizeMethod.BILINEAR)\n",
        "    quat_stretched = tf.reshape(quat_stretched, [new_len, 4])\n",
        "    quat_stretched = normalize_quaternion(quat_stretched)  # ä¿ç•™ï¼šæ‹‰ä¼¸åéœ€è¦å½’ä¸€åŒ–\n",
        "    if tf.shape(other_features)[1] > 0:\n",
        "        other_reshaped = tf.reshape(other_features, [1, tf.shape(other_features)[0], 1, tf.shape(other_features)[1]])\n",
        "        other_stretched = tf.image.resize(other_reshaped, [new_len, 1], method=tf.image.ResizeMethod.BILINEAR)\n",
        "        other_stretched = tf.reshape(other_stretched, [new_len, tf.shape(other_features)[1]])\n",
        "        stretched_sequence = tf.concat([quat_stretched, other_stretched], axis=1)\n",
        "    else:\n",
        "        stretched_sequence = quat_stretched\n",
        "    return stretched_sequence\n",
        "\n",
        "def safe_tf_augment(sequence, label, aug_prob=0.5):\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        sequence = safe_tf_time_stretch(sequence)\n",
        "        # é‡ç®—magnitude\n",
        "        lin_acc_xyz = sequence[:, 4:7]\n",
        "        sequence = tf.concat([\n",
        "            sequence[:, :4],  # å››å…ƒæ•°\n",
        "            lin_acc_xyz,\n",
        "            tf.norm(lin_acc_xyz, axis=1, keepdims=True),  # é‡ç®—lin_mag\n",
        "            sequence[:, 8:]  # ä¿ç•™å…¶ä»–ç‰¹å¾ä¸å˜\n",
        "        ], axis=1)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        seq_len = tf.shape(sequence)[0]\n",
        "        max_shift = tf.cast(tf.cast(seq_len, tf.float32) * 0.1, tf.int32)\n",
        "        shift = tf.random.uniform(shape=(), minval=-max_shift, maxval=max_shift, dtype=tf.int32)\n",
        "        sequence = tf.roll(sequence, shift=shift, axis=0)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        quat_features = sequence[:, :4]\n",
        "        other_features = sequence[:, 4:]\n",
        "        quat_noise = tf.random.normal(shape=tf.shape(quat_features), stddev=0.015)\n",
        "        quat_features = normalize_quaternion(quat_features + quat_noise)  # ä¿ç•™ï¼šåŠ å™ªåéœ€è¦å½’ä¸€åŒ–\n",
        "        if tf.shape(other_features)[1] > 0:\n",
        "            other_noise = tf.random.normal(shape=tf.shape(other_features), stddev=0.03)\n",
        "            other_features = other_features + other_noise\n",
        "            sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "        else:\n",
        "            sequence = quat_features\n",
        "    #if tf.random.uniform(()) < aug_prob:\n",
        "        #quat_features = sequence[:, :4]\n",
        "        #other_features = sequence[:, 4:]\n",
        "        #if tf.shape(other_features)[1] > 0:\n",
        "            #scale_factor = tf.random.uniform(shape=(), minval=0.9, maxval=1.1)\n",
        "            #other_features = other_features * scale_factor\n",
        "            #sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        seq_len = tf.shape(sequence)[0]\n",
        "        mask_ratio = 0.15\n",
        "        mask_length = tf.cast(tf.cast(seq_len, tf.float32) * mask_ratio, tf.int32)\n",
        "        if mask_length > 0:\n",
        "            start_idx = tf.random.uniform(shape=(), maxval=tf.maximum(1, seq_len - mask_length), dtype=tf.int32)\n",
        "            quat_features = sequence[:, :4]\n",
        "            other_features = sequence[:, 4:]\n",
        "            if tf.shape(other_features)[1] > 0:\n",
        "                mask = tf.concat([\n",
        "                    tf.ones([start_idx, tf.shape(other_features)[1]]),\n",
        "                    tf.zeros([mask_length, tf.shape(other_features)[1]]),\n",
        "                    tf.ones([seq_len - start_idx - mask_length, tf.shape(other_features)[1]])\n",
        "                ], axis=0)\n",
        "                other_features = other_features * mask\n",
        "                sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "    return sequence, label\n",
        "\n",
        "# ===================================================================\n",
        "# 6. TensorFlow æ•°æ®ç®¡é“ï¼ˆå››å…ƒæ•°å®‰å…¨ç‰ˆæœ¬ï¼‰\n",
        "# ===================================================================\n",
        "def create_tf_dataset(X, y, batch_size, is_training=True, use_augmentation=True, use_mixup=False, mixup_alpha=0.2):\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: ((seq, label) for seq, label in zip(X, y)),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, len(FEATURE_NAMES)), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(buffer_size=len(X)).repeat()\n",
        "        if use_augmentation:\n",
        "            dataset = dataset.map(safe_tf_augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.map(lambda seq, label: (seq[:MAX_SEQ_LENGTH], label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.padded_batch(batch_size, padded_shapes=([MAX_SEQ_LENGTH, len(FEATURE_NAMES)], []), padding_values=(0.0, 0), drop_remainder=True)\n",
        "        if use_mixup:\n",
        "            mixup_partner_ds = dataset.shuffle(buffer_size=len(X)//batch_size if batch_size > 0 else 1)\n",
        "            dataset = tf.data.Dataset.zip((dataset, mixup_partner_ds))\n",
        "            def safe_mixup_map(data1, data2):\n",
        "                (seq1, label1), (seq2, label2) = data1, data2\n",
        "                dist = tfp.distributions.Beta(mixup_alpha, mixup_alpha)\n",
        "                lambda_ = dist.sample()\n",
        "                quat1, quat2 = seq1[:, :, :4], seq2[:, :, :4]\n",
        "                other1, other2 = seq1[:, :, 4:], seq2[:, :, 4:]\n",
        "                mixed_quat = quaternion_slerp(quat1, quat2, 1.0 - lambda_)  # SLERPå†…éƒ¨ä¼šå½’ä¸€åŒ–\n",
        "                mixed_other = lambda_ * other1 + (1 - lambda_) * other2\n",
        "                mixed_seq = tf.concat([mixed_quat, mixed_other], axis=-1)\n",
        "                label1_oh = tf.one_hot(tf.cast(label1, tf.int32), N_CLASSES)\n",
        "                label2_oh = tf.one_hot(tf.cast(label2, tf.int32), N_CLASSES)\n",
        "                mixed_label = lambda_ * label1_oh + (1 - lambda_) * label2_oh\n",
        "                return mixed_seq, mixed_label\n",
        "            dataset = dataset.map(safe_mixup_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        else:\n",
        "            dataset = dataset.map(lambda seq, label: (seq, tf.one_hot(tf.cast(label, tf.int32), N_CLASSES)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        dataset = dataset.map(lambda seq, label: (seq[:MAX_SEQ_LENGTH], label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.padded_batch(batch_size, padded_shapes=([MAX_SEQ_LENGTH, len(FEATURE_NAMES)], []), padding_values=(0.0, 0), drop_remainder=False)\n",
        "        dataset = dataset.map(lambda seq, label: (seq, tf.one_hot(tf.cast(label, tf.int32), N_CLASSES)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ===================================================================\n",
        "# 7. Keras æ¨¡å‹å®šä¹‰\n",
        "# ===================================================================\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class SumPooling1D(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SumPooling1D, self).__init__(**kwargs)\n",
        "    def call(self, inputs):\n",
        "        return tf.reduce_sum(inputs, axis=1)\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[2])\n",
        "    def get_config(self):\n",
        "        base_config = super(SumPooling1D, self).get_config()\n",
        "        return base_config\n",
        "\n",
        "def se_block(input_tensor, reduction=8):\n",
        "    channels = input_tensor.shape[-1]\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(input_tensor)\n",
        "    x = tf.keras.layers.Dense(channels // reduction, activation='relu', use_bias=False)(x)\n",
        "    x = tf.keras.layers.Dense(channels, activation='sigmoid', use_bias=False)(x)\n",
        "    x = tf.keras.layers.Reshape((1, channels))(x)\n",
        "    return tf.keras.layers.Multiply()([input_tensor, x])\n",
        "\n",
        "def residual_se_cnn_block(input_tensor, out_channels, kernel_size, pool_size=2, dropout=0.3, dilation_rate=1):\n",
        "    in_channels = input_tensor.shape[-1]\n",
        "    if in_channels != out_channels:\n",
        "        shortcut = tf.keras.layers.Conv1D(out_channels, 1, use_bias=False)(input_tensor)\n",
        "        shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = input_tensor\n",
        "    x = tf.keras.layers.Conv1D(out_channels, kernel_size, padding='same', use_bias=False)(input_tensor)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv1D(out_channels, kernel_size, padding='same', use_bias=False, dilation_rate=dilation_rate)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = se_block(x)\n",
        "    x = tf.keras.layers.Add()([shortcut, x])\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    return x\n",
        "\n",
        "def attention_layer(input_tensor):\n",
        "    x = tf.keras.layers.LayerNormalization(axis=[1, 2])(input_tensor)\n",
        "    scores = tf.keras.layers.Dense(1, activation='tanh', name='attention_scores')(x)\n",
        "    weights = tf.keras.layers.Softmax(axis=1, name='attention_weights')(scores)\n",
        "    context = tf.keras.layers.Multiply()([input_tensor, weights])  # æ³¨æ„è¿™é‡Œç”¨åŸå§‹è¾“å…¥\n",
        "    context = SumPooling1D()(context)\n",
        "    return context\n",
        "\n",
        "def build_model(imu_dim=len(FEATURE_NAMES), n_classes=N_CLASSES, max_seq_len=MAX_SEQ_LENGTH):\n",
        "    input_layer = tf.keras.layers.Input(shape=(max_seq_len, imu_dim), name='input_sequence')\n",
        "    rot_features = tf.keras.layers.Lambda(lambda x: x[:, :, 0:4], name='rot_features')(input_layer)\n",
        "    acc_features = tf.keras.layers.Lambda(lambda x: x[:, :, 4:8], name='acc_features')(input_layer)\n",
        "    vel_features = tf.keras.layers.Lambda(lambda x: x[:, :, 8:12], name='vel_features')(input_layer)\n",
        "    other_features = tf.keras.layers.Lambda(lambda x: x[:, :, 12:], name='other_features')(input_layer)\n",
        "    rot_branch = residual_se_cnn_block(rot_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    rot_branch = residual_se_cnn_block(rot_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    acc_branch = residual_se_cnn_block(acc_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    acc_branch = residual_se_cnn_block(acc_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    vel_branch = residual_se_cnn_block(vel_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    vel_branch = residual_se_cnn_block(vel_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    if other_features.shape[-1] > 0:\n",
        "        other_branch = residual_se_cnn_block(other_features, out_channels=32, kernel_size=3, pool_size=2, dropout=0.2)\n",
        "        other_branch = residual_se_cnn_block(other_branch, out_channels=64, kernel_size=3, pool_size=2, dropout=0.35)\n",
        "        merged = tf.keras.layers.Concatenate(axis=-1)([rot_branch, acc_branch, vel_branch, other_branch])\n",
        "    else:\n",
        "        merged = tf.keras.layers.Concatenate(axis=-1)([rot_branch, acc_branch, vel_branch])\n",
        "    x = residual_se_cnn_block(merged, out_channels=256, kernel_size=3, pool_size=1, dropout=0.4)\n",
        "    x = residual_se_cnn_block(x, out_channels=512, kernel_size=5, pool_size=1, dropout=0.45)\n",
        "    x = attention_layer(x)\n",
        "    x = tf.keras.layers.Dense(128, use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dropout(0.45)(x)\n",
        "    x = tf.keras.layers.Dense(64, use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dropout(0.25)(x)\n",
        "    output_layer = tf.keras.layers.Dense(n_classes, activation='linear', name='output_logits')(x)\n",
        "    return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# ===================================================================\n",
        "# 8. è®­ç»ƒã€å›è°ƒå‡½æ•°ä¸è¯„ä¼°\n",
        "# ===================================================================\n",
        "sys.path.append('/kaggle/usr/lib/cmi_2025_metric_copy_for_import')\n",
        "try:\n",
        "    import cmi_2025_metric_copy_for_import as metric\n",
        "    print(\"æˆåŠŸå¯¼å…¥æœ¬åœ°è¯„ä¼°æŒ‡æ ‡æ–‡ä»¶ã€‚\")\n",
        "    def get_competition_score(true, pred):\n",
        "        true_labels = [IDX2LABEL[x] for x in true]\n",
        "        pred_labels = [IDX2LABEL[x] for x in pred]\n",
        "        true_df = pd.DataFrame({'id': range(len(true_labels)), 'gesture': true_labels})\n",
        "        pred_df = pd.DataFrame({'id': range(len(pred_labels)), 'gesture': pred_labels})\n",
        "        return metric.score(true_df, pred_df, 'id')\n",
        "except ImportError:\n",
        "    print(\"æ— æ³•å¯¼å…¥æœ¬åœ°è¯„ä¼°æŒ‡æ ‡æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ accuracy ä½œä¸ºæ›¿ä»£ã€‚\")\n",
        "    def get_competition_score(true, pred):\n",
        "        return accuracy_score(true, pred)\n",
        "\n",
        "class CompetitionScoreCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        super().__init__()\n",
        "        self.val_data = validation_data\n",
        "        self.val_labels = np.concatenate([y for x, y in validation_data], axis=0)\n",
        "        self.val_labels = np.argmax(self.val_labels, axis=1)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_preds = self.model.predict(self.val_data, verbose=0)\n",
        "        val_preds = np.argmax(val_preds, axis=1)\n",
        "        score = get_competition_score(self.val_labels, val_preds)\n",
        "        print(f\" - val_score: {score:.4f}\", end=\"\")\n",
        "        logs['val_score'] = score\n",
        "\n",
        "def plot_training_history(history, fold):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax1.plot(history.history['loss'], label='Train Loss')\n",
        "    ax1.plot(history.history['val_loss'], label='Val Loss')\n",
        "    ax1.set_title(f'Fold {fold} - Loss'); ax1.set_xlabel('Epoch'); ax1.legend()\n",
        "    ax2.plot(history.history['categorical_accuracy'], label='Train Accuracy')\n",
        "    ax2.plot(history.history['val_categorical_accuracy'], label='Val Accuracy')\n",
        "    ax2.set_title(f'Fold {fold} - Accuracy'); ax2.set_xlabel('Epoch'); ax2.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{OUTPUT_DIR}/fold_{fold}_training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, base_lr, warmup_steps, total_steps, min_lr=1e-5):\n",
        "        super().__init__()\n",
        "        self.base_lr, self.warmup_steps, self.total_steps, self.min_lr = base_lr, warmup_steps, total_steps, min_lr\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warm = self.base_lr * (step / tf.cast(self.warmup_steps, tf.float32))\n",
        "        progress = (step - self.warmup_steps) / tf.maximum(1.0, self.total_steps - self.warmup_steps)\n",
        "        cosine = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + tf.cos(np.pi * tf.clip_by_value(progress, 0.0, 1.0)))\n",
        "        return tf.where(step < self.warmup_steps, warm, cosine)\n",
        "    def get_config(self):\n",
        "        return {\"base_lr\": self.base_lr, \"warmup_steps\": self.warmup_steps, \"total_steps\": self.total_steps, \"min_lr\": self.min_lr}\n",
        "\n",
        "# ===================================================================\n",
        "# 9. æ•°æ®éªŒè¯å‡½æ•°\n",
        "# ===================================================================\n",
        "def check_quaternion_norm(sequences, tolerance=0.01):\n",
        "    if isinstance(sequences, list):\n",
        "        all_norms = np.concatenate([np.linalg.norm(seq[:, :4], axis=1) for seq in sequences])\n",
        "    else:\n",
        "        quat = sequences[:, :, :4] if len(sequences.shape) == 3 else sequences[:, :4]\n",
        "        all_norms = np.linalg.norm(quat, axis=-1).flatten()\n",
        "    stats = {'mean': np.mean(all_norms), 'std': np.std(all_norms), 'min': np.min(all_norms), 'max': np.max(all_norms), 'is_valid': np.all(np.abs(all_norms - 1.0) < tolerance)}\n",
        "    return stats['is_valid'], stats\n",
        "\n",
        "# ===================================================================\n",
        "# 10. K-Fold äº¤å‰éªŒè¯è®­ç»ƒæµç¨‹\n",
        "# ===================================================================\n",
        "def run_kfold_training(sequences, labels, groups, n_folds=5, **kwargs):\n",
        "    print(f\"ğŸš€ å¼€å§‹ {n_folds}-Fold è®­ç»ƒï¼ˆç®€åŒ–ç‰ˆ - ä»…ä¿®å¤é›¶å››å…ƒæ•°ï¼‰...\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nğŸ“Š éªŒè¯æ•°æ®çš„å››å…ƒæ•°èŒƒæ•°ï¼ˆå·²ä¿®å¤é›¶å››å…ƒæ•°ï¼‰...\")\n",
        "    is_valid, stats = check_quaternion_norm(sequences)\n",
        "    print(f\"   å¹³å‡èŒƒæ•°: {stats['mean']:.4f} (Â±{stats['std']:.4f})\")\n",
        "    print(f\"   èŒƒæ•°èŒƒå›´: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
        "\n",
        "    kfold_results = []\n",
        "    sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    sequences_np = np.array(sequences, dtype=object)\n",
        "\n",
        "    for fold_num, (train_idx, val_idx) in enumerate(sgkf.split(sequences_np, labels, groups), 1):\n",
        "        print(f\"\\n{'='*60}\\nğŸ”„ è®­ç»ƒ Fold {fold_num}/{n_folds}\\n{'='*60}\")\n",
        "        X_train_raw, X_val_raw = sequences_np[train_idx], sequences_np[val_idx]\n",
        "        y_train, y_val = labels[train_idx], labels[val_idx]\n",
        "\n",
        "        print(f\"ğŸ“Š Fold {fold_num}: æ ‡å‡†åŒ–æ•°æ®ï¼ˆå››å…ƒæ•°ä¿æŒåŸå§‹å€¼ï¼‰...\")\n",
        "        X_train_scaled, X_val_scaled, scaler = standardize_sequences_with_quaternion(X_train_raw.tolist(), X_val_raw.tolist(), fit_on_train=True)\n",
        "        is_valid_train, stats_train = check_quaternion_norm(X_train_scaled)\n",
        "        print(f\"   è®­ç»ƒé›†å››å…ƒæ•°èŒƒæ•°: {stats_train['mean']:.4f} (Â±{stats_train['std']:.4f})\")\n",
        "        scaler_path = f'{OUTPUT_DIR}/fold_{fold_num}_scaler.joblib'\n",
        "        joblib.dump(scaler, scaler_path)\n",
        "        print(f\"ğŸ’¾ ä¿å­˜ Fold {fold_num} çš„ StandardScaler åˆ° {scaler_path}\")\n",
        "\n",
        "        train_ds = create_tf_dataset(X_train_scaled, y_train, kwargs['batch_size'], is_training=True, use_augmentation=True, use_mixup=kwargs['use_mixup'], mixup_alpha=kwargs.get('mixup_alpha', 0.2))\n",
        "        val_ds = create_tf_dataset(X_val_scaled, y_val, kwargs['batch_size'], is_training=False)\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "        model = build_model()\n",
        "        steps_per_epoch = len(X_train_scaled) // kwargs['batch_size']\n",
        "        if steps_per_epoch == 0:\n",
        "            print(f\"é”™è¯¯: Fold {fold_num} çš„è®­ç»ƒæ ·æœ¬æ•°å°äºbatch_size\")\n",
        "            continue\n",
        "\n",
        "        total_steps = steps_per_epoch * kwargs['num_epochs']\n",
        "        lr_sched = WarmupCosine(base_lr=kwargs['learning_rate'], warmup_steps=int(0.05 * total_steps), total_steps=total_steps, min_lr=1e-5)\n",
        "        optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_sched, weight_decay=5e-3)\n",
        "        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=kwargs['label_smoothing'])\n",
        "        model.compile(optimizer=optimizer, loss=loss_fn, metrics=['categorical_accuracy'])\n",
        "\n",
        "        model_save_path = f'{OUTPUT_DIR}/fold_{fold_num}_model.keras'\n",
        "        callbacks = [\n",
        "            CompetitionScoreCallback(validation_data=val_ds),\n",
        "            tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path, save_best_only=True, monitor='val_score', mode='max', verbose=1),\n",
        "            tf.keras.callbacks.EarlyStopping(monitor='val_score', patience=kwargs['patience'], mode='max', verbose=1, restore_best_weights=True),\n",
        "        ]\n",
        "\n",
        "        print(f\"ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ Fold {fold_num}...\")\n",
        "        history = model.fit(train_ds, epochs=kwargs['num_epochs'], validation_data=val_ds, callbacks=callbacks, steps_per_epoch=steps_per_epoch, verbose=2)\n",
        "        plot_training_history(history, fold_num)\n",
        "\n",
        "        print(f\"\\nğŸ¯ Fold {fold_num} æœ€ç»ˆè¯„ä¼°:\")\n",
        "        y_val_unbatched = np.concatenate([y for x, y in val_ds], axis=0)\n",
        "        y_val_labels = np.argmax(y_val_unbatched, axis=1)\n",
        "        val_preds_final = model.predict(val_ds)\n",
        "        val_preds_final_labels = np.argmax(val_preds_final, axis=1)\n",
        "        y_val_labels = y_val_labels[:len(val_preds_final_labels)]\n",
        "\n",
        "        final_score = get_competition_score(y_val_labels, val_preds_final_labels)\n",
        "        final_acc = np.mean(val_preds_final_labels == y_val_labels)\n",
        "        print(f\"       éªŒè¯é›† Accuracy: {final_acc:.4f}\")\n",
        "        print(f\"       éªŒè¯é›† Score: {final_score:.4f}\")\n",
        "\n",
        "        print(f\"\\nğŸ“Š Fold {fold_num} - åˆ†ç±»æŠ¥å‘Š:\")\n",
        "        print(classification_report(\n",
        "            y_val_labels,\n",
        "            val_preds_final_labels,\n",
        "            target_names=list(IDX2LABEL.values()),\n",
        "            digits=4\n",
        "        ))\n",
        "\n",
        "        kfold_results.append({'fold': fold_num, 'val_accuracy': final_acc, 'val_score': final_score})\n",
        "\n",
        "    return kfold_results\n",
        "\n",
        "# ===================================================================\n",
        "# 11. æµ‹è¯•é›†é¢„æµ‹å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆ - ä¸å½’ä¸€åŒ–å››å…ƒæ•°ï¼‰\n",
        "# ===================================================================\n",
        "def predict_test_data(test_sequences_raw, output_dir=OUTPUT_DIR, n_folds=10):  # æ”¹ä¸º10\n",
        "    print(f\"ğŸ”® å¼€å§‹æµ‹è¯•é›†é¢„æµ‹ ({n_folds}-fold ensemble)...\")\n",
        "    all_predictions = []\n",
        "    for fold_num in range(1, n_folds + 1):\n",
        "        print(f\"\\nå¤„ç† Fold {fold_num}...\")\n",
        "        scaler_path = f'{output_dir}/fold_{fold_num}_scaler.joblib'\n",
        "        scaler = joblib.load(scaler_path)\n",
        "        print(f\"   åŠ è½½scaler: {scaler_path}\")\n",
        "        test_quat = [seq[:, :4] for seq in test_sequences_raw]\n",
        "        test_other = [seq[:, 4:] for seq in test_sequences_raw]\n",
        "        test_other_flat = np.vstack(test_other)\n",
        "        test_other_scaled = scaler.transform(test_other_flat)\n",
        "        test_sequences_scaled = []\n",
        "        start = 0\n",
        "        for i, seq in enumerate(test_sequences_raw):\n",
        "            seq_len = len(seq)\n",
        "            quat = test_quat[i]  # ç›´æ¥ä½¿ç”¨ï¼Œä¸å½’ä¸€åŒ–\n",
        "            other_scaled = test_other_scaled[start:start+seq_len]\n",
        "            seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "            test_sequences_scaled.append(seq_scaled)\n",
        "            start += seq_len\n",
        "        model_path = f'{output_dir}/fold_{fold_num}_model.keras'\n",
        "        custom_objects = {'WarmupCosine': WarmupCosine, 'SumPooling1D': SumPooling1D}\n",
        "        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects, compile=False)\n",
        "        print(f\"   åŠ è½½æ¨¡å‹: {model_path}\")\n",
        "        dummy_labels = np.zeros(len(test_sequences_scaled), dtype=np.int32)\n",
        "        test_ds = create_tf_dataset(test_sequences_scaled, dummy_labels, batch_size=64, is_training=False)\n",
        "        fold_predictions = model.predict(test_ds, verbose=0)\n",
        "        all_predictions.append(fold_predictions)\n",
        "        print(f\"   å®ŒæˆFold {fold_num}çš„é¢„æµ‹\")\n",
        "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
        "    final_predictions = np.argmax(ensemble_predictions, axis=1)\n",
        "    print(f\"\\nâœ… æµ‹è¯•é›†é¢„æµ‹å®Œæˆï¼\")\n",
        "    return final_predictions, ensemble_predictions\n",
        "\n",
        "# ===================================================================\n",
        "# 12. ä¸»æ‰§è¡Œé€»è¾‘ - 10-FOLDç‰ˆæœ¬\n",
        "# ===================================================================\n",
        "if __name__ == '__main__':\n",
        "    training_params = {\n",
        "        'num_epochs': 3 if DEBUG else 100,\n",
        "        'learning_rate': 0.001,\n",
        "        'patience': 24,\n",
        "        'batch_size': 64,\n",
        "        'label_smoothing': 0.1,\n",
        "        'use_mixup': True,\n",
        "        'mixup_alpha': 0.4\n",
        "    }\n",
        "\n",
        "    if TRAIN:\n",
        "        # ä½¿ç”¨10-foldäº¤å‰éªŒè¯\n",
        "        results = run_kfold_training(sequences, labels, groups, n_folds=10, **training_params)\n",
        "        print(\"\\n\\n\" + \"=\"*60 + \"\\nğŸ‰ 10-Fold è®­ç»ƒæ€»ç»“\\n\" + \"=\"*60)\n",
        "        if results:\n",
        "            val_scores = [r['val_score'] for r in results]\n",
        "            val_accs = [r['val_accuracy'] for r in results]\n",
        "            print(f\"å¹³å‡éªŒè¯é›† Score: {np.mean(val_scores):.4f} Â± {np.std(val_scores):.4f}\")\n",
        "            print(f\"å¹³å‡éªŒè¯é›† Accuracy: {np.mean(val_accs):.4f} Â± {np.std(val_accs):.4f}\")\n",
        "            print(f\"æœ€ä½³æŠ˜ Score: {np.max(val_scores):.4f} (Fold {np.argmax(val_scores)+1})\")\n",
        "            print(f\"æœ€å·®æŠ˜ Score: {np.min(val_scores):.4f} (Fold {np.argmin(val_scores)+1})\")\n",
        "\n",
        "            feature_selection_info = {'all_features': ALL_FEATURE_NAMES, 'selected_features': FEATURE_NAMES, 'feature_indices': FEATURE_INDICES, 'num_features': len(FEATURE_NAMES), 'quaternion_safe': True, 'n_folds': 10}\n",
        "            results_with_features = {'kfold_results': results, 'feature_selection': feature_selection_info, 'preprocessing_note': '10-fold CVç‰ˆæœ¬ï¼šä»…ä¿®å¤é›¶å››å…ƒæ•°ï¼Œä¸è¿›è¡Œé˜ˆå€¼å½’ä¸€åŒ–', 'lr_schedule': 'WarmupCosine'}\n",
        "            with open(f'{OUTPUT_DIR}/kfold_results_10fold.json', 'w') as f:\n",
        "                json.dump(results_with_features, f, indent=2)\n",
        "\n",
        "    print(\"\\nğŸ“ é‡è¦è¯´æ˜:\")\n",
        "    print(\"1. ä½¿ç”¨10-foldäº¤å‰éªŒè¯æé«˜æ¨¡å‹ç¨³å®šæ€§\")\n",
        "    print(\"2. è‡ªåŠ¨æ£€æµ‹å¹¶ä¿®å¤é›¶å››å…ƒæ•°ï¼ˆå¡«å……ä¸ºå•ä½å››å…ƒæ•°ï¼‰\")\n",
        "    print(\"3. å››å…ƒæ•°åœ¨æ ‡å‡†åŒ–æ—¶ä¿æŒåŸå§‹å€¼ï¼Œä¸è¿›è¡Œå½’ä¸€åŒ–\")\n",
        "    print(\"4. æ•°æ®å¢å¼ºåå¯¹å››å…ƒæ•°è¿›è¡Œå¿…è¦çš„å½’ä¸€åŒ–\")\n",
        "    print(\"5. MixUpä½¿ç”¨SLERPè¿›è¡Œå››å…ƒæ•°æ’å€¼\")\n",
        "    print(\"6. æ ‡å‡†åŒ–ä»…åº”ç”¨äºéå››å…ƒæ•°ç‰¹å¾\")\n",
        "    print(\"7. æ¯æŠ˜è®­ç»ƒåä¼šæ‰“å°è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š\")\n",
        "\n",
        "# ===================================================================\n",
        "# 13. ç”Ÿæˆæäº¤æ–‡ä»¶ - 10-FOLDç‰ˆæœ¬\n",
        "# ===================================================================\n",
        "def generate_submission(submission_output_path='submission.csv'):\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\nğŸ“„ ç”Ÿæˆæäº¤æ–‡ä»¶ï¼ˆ10-fold ensembleï¼‰\" + \"\\n\" + \"=\"*60)\n",
        "    print(\"åŠ è½½é¢„å¤„ç†çš„æµ‹è¯•æ•°æ®...\")\n",
        "    test_data = joblib.load(f'{PROCESSED_DATA_DIR}/processed_test_data_raw.joblib')\n",
        "    test_sequences_full = test_data['sequence'].tolist()\n",
        "    test_sequences = [seq[:, FEATURE_INDICES] for seq in test_sequences_full]\n",
        "\n",
        "    # ä¿®å¤æµ‹è¯•é›†çš„é›¶å››å…ƒæ•°\n",
        "    print(\"\\nå¤„ç†æµ‹è¯•é›†æ•°æ®...\")\n",
        "    test_sequences = fix_zero_quaternions(test_sequences)\n",
        "\n",
        "    test_sequence_ids = test_data['sequence_id'].values\n",
        "    # ä½¿ç”¨10-fold ensembleé¢„æµ‹\n",
        "    predictions, _ = predict_test_data(test_sequences, OUTPUT_DIR, n_folds=10)\n",
        "    predicted_labels = [IDX2LABEL[pred] for pred in predictions]\n",
        "    submission_df = pd.DataFrame({'sequence_id': test_sequence_ids, 'gesture': predicted_labels})\n",
        "    submission_df.to_csv(submission_output_path, index=False)\n",
        "    print(f\"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜åˆ°: {submission_output_path}\")\n",
        "    print(\"\\nğŸ“Š é¢„æµ‹åˆ†å¸ƒ:\")\n",
        "    print(submission_df['gesture'].value_counts())\n",
        "    return submission_df"
      ],
      "metadata": {
        "id": "sk8K9vYYUuy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}