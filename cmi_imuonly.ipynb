{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "imuonly-dataprocoss"
      ],
      "metadata": {
        "id": "AcTGU0DYX7eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "#  数据处理脚本\n",
        "#  CMI Behavior Detection - 精简特征版\n",
        "#\n",
        "#  排除已知问题序列: SEQ_011975 (存在测量问题)\n",
        "#\n",
        "#  训练特征: rot, linear_acc, linear_acc_mag, angular_vel, angular_distance, acc_mag\n",
        "# ===================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suppress noisy warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# ===================================================================\n",
        "# 1. Configuration & Global Settings\n",
        "# ===================================================================\n",
        "\n",
        "DATA_ROOT = '/kaggle/input/cmi-detect-behavior-with-sensor-data'\n",
        "OUTPUT_DIR = './processed_data_selected_features_v1'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# 训练使用的特征列表\n",
        "SELECTED_FEATURES = [\n",
        "    'rot_w', 'rot_x', 'rot_y', 'rot_z',           # 四元数\n",
        "    'linear_acc_x', 'linear_acc_y', 'linear_acc_z', # 线性加速度\n",
        "    'linear_acc_mag',                               # 线性加速度模长\n",
        "    'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # 角速度\n",
        "    'angular_distance',                             # 角距离\n",
        "    'acc_mag'                                       # 加速度模长\n",
        "]\n",
        "\n",
        "# Labels\n",
        "LABEL_NAMES = [\n",
        "    'Forehead - pull hairline', 'Neck - pinch skin', 'Forehead - scratch',\n",
        "    'Eyelash - pull hair', 'Text on phone', 'Eyebrow - pull hair',\n",
        "    'Neck - scratch', 'Above ear - pull hair', 'Cheek - pinch skin',\n",
        "    'Wave hello', 'Write name in air', 'Pull air toward your face',\n",
        "    'Feel around in tray and pull out an object', 'Write name on leg',\n",
        "    'Pinch knee/leg skin', 'Scratch knee/leg skin', 'Drink from bottle/cup',\n",
        "    'Glasses on/off'\n",
        "]\n",
        "LABEL2IDX = {x: i for i, x in enumerate(LABEL_NAMES)}\n",
        "\n",
        "# ===================================================================\n",
        "# 2. 核心特征工程函数\n",
        "# ===================================================================\n",
        "\n",
        "def remove_gravity_from_acc(acc_data, rot_data):\n",
        "    \"\"\"去除重力影响，计算线性加速度\"\"\"\n",
        "    acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
        "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    linear_accel = np.zeros_like(acc_values)\n",
        "    gravity_world = np.array([0, 0, 9.81])\n",
        "\n",
        "    for i in range(len(acc_values)):\n",
        "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
        "            linear_accel[i, :] = acc_values[i, :]\n",
        "            continue\n",
        "        try:\n",
        "            rotation = R.from_quat(quat_values[i])\n",
        "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
        "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
        "        except ValueError:\n",
        "            linear_accel[i, :] = acc_values[i, :]\n",
        "    return linear_accel\n",
        "\n",
        "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200):\n",
        "    \"\"\"从四元数计算角速度\"\"\"\n",
        "    quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
        "    angular_vel = np.zeros((len(quat_values), 3))\n",
        "\n",
        "    for i in range(len(quat_values) - 1):\n",
        "        q_t, q_t_plus_dt = quat_values[i], quat_values[i+1]\n",
        "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
        "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
        "            continue\n",
        "        try:\n",
        "            rot_t = R.from_quat(q_t)\n",
        "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
        "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
        "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
        "        except ValueError:\n",
        "            pass\n",
        "    return angular_vel\n",
        "\n",
        "def compute_angular_distance_xyzw(rot_df):\n",
        "    \"\"\"计算相邻帧的角距离\"\"\"\n",
        "    q = rot_df[['rot_x','rot_y','rot_z','rot_w']].values.astype(np.float32)\n",
        "    n = len(q)\n",
        "    ang = np.zeros(n, dtype=np.float32)\n",
        "    if n <= 1:\n",
        "        return ang\n",
        "    # 归一化\n",
        "    norm = np.linalg.norm(q, axis=1, keepdims=True)\n",
        "    mask = norm[:,0] > 1e-8\n",
        "    q[mask] = q[mask] / norm[mask]\n",
        "    # 计算角距离\n",
        "    dot = np.sum(q[:-1] * q[1:], axis=1)\n",
        "    dot = np.clip(np.abs(dot), -1.0, 1.0)\n",
        "    ang[1:] = 2.0 * np.arccos(dot).astype(np.float32)\n",
        "    return ang\n",
        "\n",
        "def feature_engineering(df):\n",
        "    \"\"\"精简版特征工程\"\"\"\n",
        "    print(\"开始特征工程...\")\n",
        "\n",
        "    # 1. 加速度模长\n",
        "    df['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\n",
        "\n",
        "    # 2. 线性加速度（去除重力）\n",
        "    print(\"计算线性加速度...\")\n",
        "    tqdm.pandas(desc=\"去除重力\")\n",
        "    linear_accel_df = df.groupby('sequence_id', group_keys=False).progress_apply(\n",
        "        lambda g: pd.DataFrame(\n",
        "            remove_gravity_from_acc(\n",
        "                g[['acc_x', 'acc_y', 'acc_z']],\n",
        "                g[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
        "            ),\n",
        "            columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'],\n",
        "            index=g.index\n",
        "        )\n",
        "    )\n",
        "    df = df.join(linear_accel_df)\n",
        "\n",
        "    # 线性加速度模长\n",
        "    df['linear_acc_mag'] = np.sqrt(\n",
        "        df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2\n",
        "    )\n",
        "\n",
        "    # 3. 角速度\n",
        "    print(\"计算角速度...\")\n",
        "    tqdm.pandas(desc=\"计算角速度\")\n",
        "    angular_velocity_df = df.groupby('sequence_id', group_keys=False).progress_apply(\n",
        "        lambda g: pd.DataFrame(\n",
        "            calculate_angular_velocity_from_quat(g[['rot_x', 'rot_y', 'rot_z', 'rot_w']]),\n",
        "            columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'],\n",
        "            index=g.index\n",
        "        )\n",
        "    )\n",
        "    df = df.join(angular_velocity_df)\n",
        "\n",
        "    # 4. 角距离\n",
        "    print(\"计算角距离...\")\n",
        "    tqdm.pandas(desc=\"计算角距离\")\n",
        "    angdist_df = df.groupby('sequence_id', group_keys=False).progress_apply(\n",
        "        lambda g: pd.Series(compute_angular_distance_xyzw(g[['rot_x','rot_y','rot_z','rot_w']].reset_index(drop=True)),\n",
        "                            index=g.index, name='angular_distance')\n",
        "    ).to_frame()\n",
        "    df = df.join(angdist_df)\n",
        "\n",
        "    # 填充缺失值（按序列分组）\n",
        "    print(\"填充缺失值...\")\n",
        "    df[SELECTED_FEATURES] = (\n",
        "        df.groupby('sequence_id')[SELECTED_FEATURES]\n",
        "        .apply(lambda g: g.ffill().bfill())\n",
        "        .reset_index(level=0, drop=True)\n",
        "        .fillna(0.0)\n",
        "        .astype('float32')\n",
        "    )\n",
        "\n",
        "    print(f\"特征工程完成。特征数: {len(SELECTED_FEATURES)}\")\n",
        "    return df\n",
        "\n",
        "# ===================================================================\n",
        "# 3. 主执行流程\n",
        "# ===================================================================\n",
        "\n",
        "PROBLEMATIC_SEQUENCES = ['SEQ_011975']\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"加载原始数据...\")\n",
        "    train_df = pd.read_csv(f'{DATA_ROOT}/train.csv')\n",
        "    train_demo_df = pd.read_csv(f'{DATA_ROOT}/train_demographics.csv')\n",
        "    train_df = pd.merge(train_df, train_demo_df, how='left', on='subject')\n",
        "\n",
        "    # 排除问题序列\n",
        "    print(f\"排除问题序列: {PROBLEMATIC_SEQUENCES}\")\n",
        "    original_count = len(train_df)\n",
        "    train_df = train_df[~train_df['sequence_id'].isin(PROBLEMATIC_SEQUENCES)].copy()\n",
        "    excluded_count = original_count - len(train_df)\n",
        "    print(f\"已排除 {excluded_count} 条数据记录\")\n",
        "\n",
        "    # 运行特征工程\n",
        "    train_df = feature_engineering(train_df)\n",
        "\n",
        "    print(\"聚合数据为序列...\")\n",
        "    agg_train_df = train_df.groupby(['sequence_id', 'subject', 'gesture']).apply(\n",
        "        lambda df: df[SELECTED_FEATURES].values,\n",
        "        include_groups=False,\n",
        "    ).reset_index()\n",
        "    agg_train_df.columns = ['sequence_id', 'subject', 'gesture', 'sequence']\n",
        "    agg_train_df['label'] = agg_train_df.gesture.map(LABEL2IDX)\n",
        "\n",
        "    # 保存处理后的数据\n",
        "    output_path = f'{OUTPUT_DIR}/processed_train_data_raw.joblib'\n",
        "    print(f\"保存聚合数据到 {output_path}\")\n",
        "    joblib.dump(agg_train_df, output_path)\n",
        "\n",
        "    # 保存特征配置\n",
        "    print(f\"保存特征配置...\")\n",
        "    feature_info = {\n",
        "        'all_features': SELECTED_FEATURES,\n",
        "        'time_features': SELECTED_FEATURES,\n",
        "        'psd_features': [],\n",
        "        'stat_features': [],\n",
        "        'feature_count': len(SELECTED_FEATURES)\n",
        "    }\n",
        "    with open(f'{OUTPUT_DIR}/feature_names.json', 'w') as f:\n",
        "        json.dump(feature_info, f, indent=2)\n",
        "\n",
        "    with open(f'{OUTPUT_DIR}/label_map.json', 'w') as f:\n",
        "        json.dump(LABEL2IDX, f, indent=2)\n",
        "\n",
        "    print(\"\\n✅ 数据预处理完成!\")\n",
        "    print(f\"特征数: {len(SELECTED_FEATURES)}\")\n",
        "    print(\"\\n特征列表:\")\n",
        "    for i, feat in enumerate(SELECTED_FEATURES, 1):\n",
        "        print(f\"  {i:2d}. {feat}\")\n",
        "\n",
        "# ===================================================================\n",
        "# 4. 测试集预处理函数\n",
        "# ===================================================================\n",
        "\n",
        "def preprocess_test_data(test_csv_path, demographics_csv_path=None, output_dir=OUTPUT_DIR):\n",
        "    \"\"\"处理测试数据\"\"\"\n",
        "    print(\"处理测试数据...\")\n",
        "\n",
        "    test_df = pd.read_csv(test_csv_path)\n",
        "    if demographics_csv_path:\n",
        "        test_demo_df = pd.read_csv(demographics_csv_path)\n",
        "        test_df = pd.merge(test_df, test_demo_df, how='left', on='subject')\n",
        "\n",
        "    # 排除问题序列\n",
        "    if 'sequence_id' in test_df.columns:\n",
        "        test_df = test_df[~test_df['sequence_id'].isin(PROBLEMATIC_SEQUENCES)].copy()\n",
        "\n",
        "    # 特征工程\n",
        "    test_df = feature_engineering(test_df)\n",
        "\n",
        "    # 聚合数据\n",
        "    agg_test_df = test_df.groupby(['sequence_id', 'subject']).apply(\n",
        "        lambda df: df[SELECTED_FEATURES].values,\n",
        "        include_groups=False,\n",
        "    ).reset_index()\n",
        "    agg_test_df.columns = ['sequence_id', 'subject', 'sequence']\n",
        "\n",
        "    # 保存\n",
        "    test_output_path = f'{output_dir}/processed_test_data_raw.joblib'\n",
        "    print(f\"保存测试数据到 {test_output_path}\")\n",
        "    joblib.dump(agg_test_df, test_output_path)\n",
        "\n",
        "    print(\"✅ 测试数据预处理完成!\")\n",
        "    return agg_test_df"
      ],
      "metadata": {
        "id": "YqSOlAvpc2Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training-model"
      ],
      "metadata": {
        "id": "Dhmd2MmIYBmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "#\n",
        "# 完整的 TensorFlow/Keras 训练代码 - CMI 行为检测\n",
        "# se1dcnn+attention模型架构 - 四元数安全版本（简化版：仅修复零四元数）\n",
        "# cv = 0.8094, lb = 0.812\n",
        "# 过拟合已解决\n",
        "#\n",
        "# ===================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Your input ran out of data.*\")\n",
        "\n",
        "# 抑制 TensorFlow 的一些日志输出\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# ===================================================================\n",
        "# 1. 配置与全局设置\n",
        "# ===================================================================\n",
        "DEBUG = False\n",
        "TRAIN = True\n",
        "MAX_SEQ_LENGTH = 128\n",
        "PROCESSED_DATA_DIR = '/kaggle/input/imuonly-process/kaggle/working/processed_data_selected_features_v1' # 请替换为您的实际路径\n",
        "OUTPUT_DIR = './saved_models_keras_fixed_test'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# 从预处理阶段生成的文件中加载标签映射\n",
        "with open(f'{PROCESSED_DATA_DIR}/label_map.json', 'r') as f:\n",
        "    LABEL2IDX = json.load(f)\n",
        "IDX2LABEL = {v: k for k, v in LABEL2IDX.items()}\n",
        "N_CLASSES = len(LABEL2IDX)\n",
        "\n",
        "# 加载特征配置\n",
        "with open(f'{PROCESSED_DATA_DIR}/feature_names.json', 'r') as f:\n",
        "    feature_info = json.load(f)\n",
        "    ALL_FEATURE_NAMES = feature_info['all_features']\n",
        "    TIME_FEATURE_NAMES = feature_info['time_features']\n",
        "    PSD_FEATURE_NAMES = feature_info['psd_features']\n",
        "    STAT_FEATURE_NAMES = feature_info['stat_features']\n",
        "\n",
        "# ===================================================================\n",
        "# 特征选择配置 - 注意前4个必须是四元数\n",
        "# ===================================================================\n",
        "SELECTED_FEATURES = [\n",
        "    'rot_w', 'rot_x', 'rot_y', 'rot_z',           # 四元数\n",
        "    'linear_acc_x', 'linear_acc_y', 'linear_acc_z', # 线性加速度\n",
        "    'linear_acc_mag',                               # 线性加速度模长\n",
        "    'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # 角速度\n",
        "    'angular_distance',                             # 角距离\n",
        "    'acc_mag'                                       # 加速度模长\n",
        "]\n",
        "\n",
        "if SELECTED_FEATURES is not None:\n",
        "    FEATURE_NAMES = SELECTED_FEATURES\n",
        "else:\n",
        "    FEATURE_NAMES = ALL_FEATURE_NAMES\n",
        "\n",
        "FEATURE_INDICES = [ALL_FEATURE_NAMES.index(f) for f in FEATURE_NAMES]\n",
        "\n",
        "print(f\"使用的特征数量: {len(FEATURE_NAMES)}\")\n",
        "print(f\"使用的特征: {FEATURE_NAMES}\")\n",
        "print(f\"前4个特征是四元数: {FEATURE_NAMES[:4]}\")\n",
        "\n",
        "tf.keras.utils.set_random_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ===================================================================\n",
        "# 2. 加载预处理好的数据（原始数据，未标准化）\n",
        "# ===================================================================\n",
        "print(\"\\n加载预处理好的数据（原始特征）...\")\n",
        "agg_train_df = joblib.load(f'{PROCESSED_DATA_DIR}/processed_train_data_raw.joblib')\n",
        "\n",
        "if DEBUG:\n",
        "    agg_train_df = agg_train_df.head(2000)\n",
        "\n",
        "sequences_full = agg_train_df['sequence'].tolist()\n",
        "sequences = [seq[:, FEATURE_INDICES] for seq in sequences_full]\n",
        "\n",
        "# ===================================================================\n",
        "# 修复零四元数（缺失值填充导致的问题）- 保留\n",
        "# ===================================================================\n",
        "def fix_zero_quaternions(sequences):\n",
        "    \"\"\"\n",
        "    修复零四元数，将其替换为单位四元数 [1,0,0,0] (w,x,y,z格式)\n",
        "    \"\"\"\n",
        "    print(\"\\n🔧 检查并修复零四元数...\")\n",
        "    total_frames = 0\n",
        "    zero_frames = 0\n",
        "    problematic_sequences = []\n",
        "\n",
        "    for seq_idx, seq in enumerate(sequences):\n",
        "        # 计算每帧四元数的范数\n",
        "        quat_norms = np.linalg.norm(seq[:, :4], axis=1)\n",
        "        total_frames += len(quat_norms)\n",
        "\n",
        "        # 找出零四元数（范数接近0）\n",
        "        zero_mask = quat_norms < 1e-8\n",
        "        num_zeros = np.sum(zero_mask)\n",
        "\n",
        "        if num_zeros > 0:\n",
        "            # 修复：将零四元数替换为单位四元数 [1,0,0,0]\n",
        "            seq[zero_mask, :4] = [1.0, 0.0, 0.0, 0.0]\n",
        "            zero_frames += num_zeros\n",
        "            problematic_sequences.append(seq_idx)\n",
        "\n",
        "    # 输出统计信息\n",
        "    if zero_frames > 0:\n",
        "        print(f\"   ✅ 修复了 {zero_frames}/{total_frames} 个零四元数帧 \"\n",
        "              f\"({100*zero_frames/total_frames:.2f}%)\")\n",
        "        print(f\"   📊 涉及 {len(problematic_sequences)} 个序列\")\n",
        "    else:\n",
        "        print(f\"   ✅ 未发现零四元数，数据质量良好\")\n",
        "\n",
        "    # 验证修复后的结果\n",
        "    print(\"\\n   验证修复后的四元数范数:\")\n",
        "    all_quats = np.vstack([seq[:, :4] for seq in sequences])\n",
        "    fixed_norms = np.linalg.norm(all_quats, axis=1)\n",
        "    print(f\"     均值: {np.mean(fixed_norms):.6f}\")\n",
        "    print(f\"     标准差: {np.std(fixed_norms):.6f}\")\n",
        "    print(f\"     范围: [{np.min(fixed_norms):.6f}, {np.max(fixed_norms):.6f}]\")\n",
        "\n",
        "    # 检查是否还有问题\n",
        "    remaining_zeros = np.sum(fixed_norms < 1e-8)\n",
        "    if remaining_zeros > 0:\n",
        "        print(f\"   ⚠️ 警告: 仍有 {remaining_zeros} 个零四元数未修复\")\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# 执行修复\n",
        "sequences = fix_zero_quaternions(sequences)\n",
        "\n",
        "labels = agg_train_df['label'].values\n",
        "groups = agg_train_df['subject'].values\n",
        "print(f\"\\n成功加载并处理 {len(sequences)} 条序列数据。\")\n",
        "\n",
        "# ===================================================================\n",
        "# 3. 数据标准化函数（简化版 - 不对四元数进行归一化）\n",
        "# ===================================================================\n",
        "def standardize_sequences_with_quaternion(sequences_train, sequences_val, fit_on_train=True):\n",
        "    train_quat = [seq[:, :4] for seq in sequences_train]\n",
        "    train_other = [seq[:, 4:] for seq in sequences_train]\n",
        "    val_quat = [seq[:, :4] for seq in sequences_val]\n",
        "    val_other = [seq[:, 4:] for seq in sequences_val]\n",
        "\n",
        "    # 标准化非四元数特征\n",
        "    train_other_flat = np.vstack(train_other)\n",
        "    val_other_flat = np.vstack(val_other)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    if fit_on_train:\n",
        "        train_other_scaled = scaler.fit_transform(train_other_flat)\n",
        "        val_other_scaled = scaler.transform(val_other_flat)\n",
        "    else:\n",
        "        all_other_flat = np.vstack([train_other_flat, val_other_flat])\n",
        "        scaler.fit(all_other_flat)\n",
        "        train_other_scaled = scaler.transform(train_other_flat)\n",
        "        val_other_scaled = scaler.transform(val_other_flat)\n",
        "\n",
        "    sequences_train_scaled, sequences_val_scaled = [], []\n",
        "\n",
        "    # 训练集 - 直接使用原始四元数（不归一化）\n",
        "    train_start = 0\n",
        "    for i, seq in enumerate(sequences_train):\n",
        "        seq_len = len(seq)\n",
        "        quat = train_quat[i]  # 直接使用，不归一化\n",
        "        other_scaled = train_other_scaled[train_start:train_start+seq_len]\n",
        "        seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "        sequences_train_scaled.append(seq_scaled)\n",
        "        train_start += seq_len\n",
        "\n",
        "    # 验证集 - 同样处理\n",
        "    val_start = 0\n",
        "    for i, seq in enumerate(sequences_val):\n",
        "        seq_len = len(seq)\n",
        "        quat = val_quat[i]  # 直接使用，不归一化\n",
        "        other_scaled = val_other_scaled[val_start:val_start+seq_len]\n",
        "        seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "        sequences_val_scaled.append(seq_scaled)\n",
        "        val_start += seq_len\n",
        "\n",
        "    return sequences_train_scaled, sequences_val_scaled, scaler\n",
        "\n",
        "# ===================================================================\n",
        "# 4. 四元数工具函数\n",
        "# ===================================================================\n",
        "def normalize_quaternion(quat):\n",
        "    norm = tf.norm(quat, axis=-1, keepdims=True)\n",
        "    norm = tf.maximum(norm, 1e-8)\n",
        "    return quat / norm\n",
        "\n",
        "def quaternion_slerp(q1, q2, t):\n",
        "    dot = tf.reduce_sum(q1 * q2, axis=-1, keepdims=True)\n",
        "    q2 = tf.where(dot < 0, -q2, q2)\n",
        "    dot = tf.abs(dot)\n",
        "    dot = tf.clip_by_value(dot, -1.0, 1.0)\n",
        "    theta = tf.acos(dot)\n",
        "    sin_theta = tf.sin(theta)\n",
        "    w1 = tf.where(sin_theta > 1e-4, tf.sin((1.0 - t) * theta) / sin_theta, 1.0 - t)\n",
        "    w2 = tf.where(sin_theta > 1e-4, tf.sin(t * theta) / sin_theta, t)\n",
        "    result = w1 * q1 + w2 * q2\n",
        "    return normalize_quaternion(result)\n",
        "\n",
        "# ===================================================================\n",
        "# 5. 四元数安全的数据增强（保留增强后的归一化）\n",
        "# ===================================================================\n",
        "def safe_tf_time_stretch(sequence, stretch_range=(0.8, 1.2)):\n",
        "    seq_len_float = tf.cast(tf.shape(sequence)[0], tf.float32)\n",
        "    stretch_factor = tf.random.uniform(shape=(), minval=stretch_range[0], maxval=stretch_range[1])\n",
        "    new_len = tf.cast(seq_len_float / stretch_factor, tf.int32)\n",
        "    quat_features = sequence[:, :4]\n",
        "    other_features = sequence[:, 4:]\n",
        "    quat_reshaped = tf.reshape(quat_features, [1, tf.shape(quat_features)[0], 1, 4])\n",
        "    quat_stretched = tf.image.resize(quat_reshaped, [new_len, 1], method=tf.image.ResizeMethod.BILINEAR)\n",
        "    quat_stretched = tf.reshape(quat_stretched, [new_len, 4])\n",
        "    quat_stretched = normalize_quaternion(quat_stretched)  # 保留：拉伸后需要归一化\n",
        "    if tf.shape(other_features)[1] > 0:\n",
        "        other_reshaped = tf.reshape(other_features, [1, tf.shape(other_features)[0], 1, tf.shape(other_features)[1]])\n",
        "        other_stretched = tf.image.resize(other_reshaped, [new_len, 1], method=tf.image.ResizeMethod.BILINEAR)\n",
        "        other_stretched = tf.reshape(other_stretched, [new_len, tf.shape(other_features)[1]])\n",
        "        stretched_sequence = tf.concat([quat_stretched, other_stretched], axis=1)\n",
        "    else:\n",
        "        stretched_sequence = quat_stretched\n",
        "    return stretched_sequence\n",
        "\n",
        "def safe_tf_augment(sequence, label, aug_prob=0.5):\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        sequence = safe_tf_time_stretch(sequence)\n",
        "        # 重算magnitude\n",
        "        lin_acc_xyz = sequence[:, 4:7]\n",
        "        sequence = tf.concat([\n",
        "            sequence[:, :4],  # 四元数\n",
        "            lin_acc_xyz,\n",
        "            tf.norm(lin_acc_xyz, axis=1, keepdims=True),  # 重算lin_mag\n",
        "            sequence[:, 8:]  # 保留其他特征不变\n",
        "        ], axis=1)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        seq_len = tf.shape(sequence)[0]\n",
        "        max_shift = tf.cast(tf.cast(seq_len, tf.float32) * 0.1, tf.int32)\n",
        "        shift = tf.random.uniform(shape=(), minval=-max_shift, maxval=max_shift, dtype=tf.int32)\n",
        "        sequence = tf.roll(sequence, shift=shift, axis=0)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        quat_features = sequence[:, :4]\n",
        "        other_features = sequence[:, 4:]\n",
        "        quat_noise = tf.random.normal(shape=tf.shape(quat_features), stddev=0.015)\n",
        "        quat_features = normalize_quaternion(quat_features + quat_noise)  # 保留：加噪后需要归一化\n",
        "        if tf.shape(other_features)[1] > 0:\n",
        "            other_noise = tf.random.normal(shape=tf.shape(other_features), stddev=0.03)\n",
        "            other_features = other_features + other_noise\n",
        "            sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "        else:\n",
        "            sequence = quat_features\n",
        "    #if tf.random.uniform(()) < aug_prob:\n",
        "        #quat_features = sequence[:, :4]\n",
        "        #other_features = sequence[:, 4:]\n",
        "        #if tf.shape(other_features)[1] > 0:\n",
        "            #scale_factor = tf.random.uniform(shape=(), minval=0.9, maxval=1.1)\n",
        "            #other_features = other_features * scale_factor\n",
        "            #sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        seq_len = tf.shape(sequence)[0]\n",
        "        mask_ratio = 0.15\n",
        "        mask_length = tf.cast(tf.cast(seq_len, tf.float32) * mask_ratio, tf.int32)\n",
        "        if mask_length > 0:\n",
        "            start_idx = tf.random.uniform(shape=(), maxval=tf.maximum(1, seq_len - mask_length), dtype=tf.int32)\n",
        "            quat_features = sequence[:, :4]\n",
        "            other_features = sequence[:, 4:]\n",
        "            if tf.shape(other_features)[1] > 0:\n",
        "                mask = tf.concat([\n",
        "                    tf.ones([start_idx, tf.shape(other_features)[1]]),\n",
        "                    tf.zeros([mask_length, tf.shape(other_features)[1]]),\n",
        "                    tf.ones([seq_len - start_idx - mask_length, tf.shape(other_features)[1]])\n",
        "                ], axis=0)\n",
        "                other_features = other_features * mask\n",
        "                sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "    return sequence, label\n",
        "\n",
        "# ===================================================================\n",
        "# 6. TensorFlow 数据管道（四元数安全版本）\n",
        "# ===================================================================\n",
        "def create_tf_dataset(X, y, batch_size, is_training=True, use_augmentation=True, use_mixup=False, mixup_alpha=0.2):\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: ((seq, label) for seq, label in zip(X, y)),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, len(FEATURE_NAMES)), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(buffer_size=len(X)).repeat()\n",
        "        if use_augmentation:\n",
        "            dataset = dataset.map(safe_tf_augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.map(lambda seq, label: (seq[:MAX_SEQ_LENGTH], label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.padded_batch(batch_size, padded_shapes=([MAX_SEQ_LENGTH, len(FEATURE_NAMES)], []), padding_values=(0.0, 0), drop_remainder=True)\n",
        "        if use_mixup:\n",
        "            mixup_partner_ds = dataset.shuffle(buffer_size=len(X)//batch_size if batch_size > 0 else 1)\n",
        "            dataset = tf.data.Dataset.zip((dataset, mixup_partner_ds))\n",
        "            def safe_mixup_map(data1, data2):\n",
        "                (seq1, label1), (seq2, label2) = data1, data2\n",
        "                dist = tfp.distributions.Beta(mixup_alpha, mixup_alpha)\n",
        "                lambda_ = dist.sample()\n",
        "                quat1, quat2 = seq1[:, :, :4], seq2[:, :, :4]\n",
        "                other1, other2 = seq1[:, :, 4:], seq2[:, :, 4:]\n",
        "                mixed_quat = quaternion_slerp(quat1, quat2, 1.0 - lambda_)  # SLERP内部会归一化\n",
        "                mixed_other = lambda_ * other1 + (1 - lambda_) * other2\n",
        "                mixed_seq = tf.concat([mixed_quat, mixed_other], axis=-1)\n",
        "                label1_oh = tf.one_hot(tf.cast(label1, tf.int32), N_CLASSES)\n",
        "                label2_oh = tf.one_hot(tf.cast(label2, tf.int32), N_CLASSES)\n",
        "                mixed_label = lambda_ * label1_oh + (1 - lambda_) * label2_oh\n",
        "                return mixed_seq, mixed_label\n",
        "            dataset = dataset.map(safe_mixup_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        else:\n",
        "            dataset = dataset.map(lambda seq, label: (seq, tf.one_hot(tf.cast(label, tf.int32), N_CLASSES)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        dataset = dataset.map(lambda seq, label: (seq[:MAX_SEQ_LENGTH], label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.padded_batch(batch_size, padded_shapes=([MAX_SEQ_LENGTH, len(FEATURE_NAMES)], []), padding_values=(0.0, 0), drop_remainder=False)\n",
        "        dataset = dataset.map(lambda seq, label: (seq, tf.one_hot(tf.cast(label, tf.int32), N_CLASSES)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ===================================================================\n",
        "# 7. Keras 模型定义\n",
        "# ===================================================================\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class SumPooling1D(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SumPooling1D, self).__init__(**kwargs)\n",
        "    def call(self, inputs):\n",
        "        return tf.reduce_sum(inputs, axis=1)\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[2])\n",
        "    def get_config(self):\n",
        "        base_config = super(SumPooling1D, self).get_config()\n",
        "        return base_config\n",
        "\n",
        "def se_block(input_tensor, reduction=8):\n",
        "    channels = input_tensor.shape[-1]\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(input_tensor)\n",
        "    x = tf.keras.layers.Dense(channels // reduction, activation='relu', use_bias=False)(x)\n",
        "    x = tf.keras.layers.Dense(channels, activation='sigmoid', use_bias=False)(x)\n",
        "    x = tf.keras.layers.Reshape((1, channels))(x)\n",
        "    return tf.keras.layers.Multiply()([input_tensor, x])\n",
        "\n",
        "def residual_se_cnn_block(input_tensor, out_channels, kernel_size, pool_size=2, dropout=0.3, dilation_rate=1):\n",
        "    in_channels = input_tensor.shape[-1]\n",
        "    if in_channels != out_channels:\n",
        "        shortcut = tf.keras.layers.Conv1D(out_channels, 1, use_bias=False)(input_tensor)\n",
        "        shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = input_tensor\n",
        "    x = tf.keras.layers.Conv1D(out_channels, kernel_size, padding='same', use_bias=False)(input_tensor)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv1D(out_channels, kernel_size, padding='same', use_bias=False, dilation_rate=dilation_rate)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = se_block(x)\n",
        "    x = tf.keras.layers.Add()([shortcut, x])\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    return x\n",
        "\n",
        "def attention_layer(input_tensor):\n",
        "    x = tf.keras.layers.LayerNormalization(axis=[1, 2])(input_tensor)\n",
        "    scores = tf.keras.layers.Dense(1, activation='tanh', name='attention_scores')(x)\n",
        "    weights = tf.keras.layers.Softmax(axis=1, name='attention_weights')(scores)\n",
        "    context = tf.keras.layers.Multiply()([input_tensor, weights])  # 注意这里用原始输入\n",
        "    context = SumPooling1D()(context)\n",
        "    return context\n",
        "\n",
        "def build_model(imu_dim=len(FEATURE_NAMES), n_classes=N_CLASSES, max_seq_len=MAX_SEQ_LENGTH):\n",
        "    input_layer = tf.keras.layers.Input(shape=(max_seq_len, imu_dim), name='input_sequence')\n",
        "    rot_features = tf.keras.layers.Lambda(lambda x: x[:, :, 0:4], name='rot_features')(input_layer)\n",
        "    acc_features = tf.keras.layers.Lambda(lambda x: x[:, :, 4:8], name='acc_features')(input_layer)\n",
        "    vel_features = tf.keras.layers.Lambda(lambda x: x[:, :, 8:12], name='vel_features')(input_layer)\n",
        "    other_features = tf.keras.layers.Lambda(lambda x: x[:, :, 12:], name='other_features')(input_layer)\n",
        "    rot_branch = residual_se_cnn_block(rot_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    rot_branch = residual_se_cnn_block(rot_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    acc_branch = residual_se_cnn_block(acc_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    acc_branch = residual_se_cnn_block(acc_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    vel_branch = residual_se_cnn_block(vel_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    vel_branch = residual_se_cnn_block(vel_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    if other_features.shape[-1] > 0:\n",
        "        other_branch = residual_se_cnn_block(other_features, out_channels=32, kernel_size=3, pool_size=2, dropout=0.2)\n",
        "        other_branch = residual_se_cnn_block(other_branch, out_channels=64, kernel_size=3, pool_size=2, dropout=0.35)\n",
        "        merged = tf.keras.layers.Concatenate(axis=-1)([rot_branch, acc_branch, vel_branch, other_branch])\n",
        "    else:\n",
        "        merged = tf.keras.layers.Concatenate(axis=-1)([rot_branch, acc_branch, vel_branch])\n",
        "    x = residual_se_cnn_block(merged, out_channels=256, kernel_size=3, pool_size=1, dropout=0.4)\n",
        "    x = residual_se_cnn_block(x, out_channels=512, kernel_size=5, pool_size=1, dropout=0.45)\n",
        "    x = attention_layer(x)\n",
        "    x = tf.keras.layers.Dense(128, use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dropout(0.45)(x)\n",
        "    x = tf.keras.layers.Dense(64, use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dropout(0.25)(x)\n",
        "    output_layer = tf.keras.layers.Dense(n_classes, activation='linear', name='output_logits')(x)\n",
        "    return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# ===================================================================\n",
        "# 8. 训练、回调函数与评估\n",
        "# ===================================================================\n",
        "sys.path.append('/kaggle/usr/lib/cmi_2025_metric_copy_for_import')\n",
        "try:\n",
        "    import cmi_2025_metric_copy_for_import as metric\n",
        "    print(\"成功导入本地评估指标文件。\")\n",
        "    def get_competition_score(true, pred):\n",
        "        true_labels = [IDX2LABEL[x] for x in true]\n",
        "        pred_labels = [IDX2LABEL[x] for x in pred]\n",
        "        true_df = pd.DataFrame({'id': range(len(true_labels)), 'gesture': true_labels})\n",
        "        pred_df = pd.DataFrame({'id': range(len(pred_labels)), 'gesture': pred_labels})\n",
        "        return metric.score(true_df, pred_df, 'id')\n",
        "except ImportError:\n",
        "    print(\"无法导入本地评估指标文件，将使用 accuracy 作为替代。\")\n",
        "    def get_competition_score(true, pred):\n",
        "        return accuracy_score(true, pred)\n",
        "\n",
        "class CompetitionScoreCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        super().__init__()\n",
        "        self.val_data = validation_data\n",
        "        self.val_labels = np.concatenate([y for x, y in validation_data], axis=0)\n",
        "        self.val_labels = np.argmax(self.val_labels, axis=1)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_preds = self.model.predict(self.val_data, verbose=0)\n",
        "        val_preds = np.argmax(val_preds, axis=1)\n",
        "        score = get_competition_score(self.val_labels, val_preds)\n",
        "        print(f\" - val_score: {score:.4f}\", end=\"\")\n",
        "        logs['val_score'] = score\n",
        "\n",
        "def plot_training_history(history, fold):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax1.plot(history.history['loss'], label='Train Loss')\n",
        "    ax1.plot(history.history['val_loss'], label='Val Loss')\n",
        "    ax1.set_title(f'Fold {fold} - Loss'); ax1.set_xlabel('Epoch'); ax1.legend()\n",
        "    ax2.plot(history.history['categorical_accuracy'], label='Train Accuracy')\n",
        "    ax2.plot(history.history['val_categorical_accuracy'], label='Val Accuracy')\n",
        "    ax2.set_title(f'Fold {fold} - Accuracy'); ax2.set_xlabel('Epoch'); ax2.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{OUTPUT_DIR}/fold_{fold}_training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, base_lr, warmup_steps, total_steps, min_lr=1e-5):\n",
        "        super().__init__()\n",
        "        self.base_lr, self.warmup_steps, self.total_steps, self.min_lr = base_lr, warmup_steps, total_steps, min_lr\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warm = self.base_lr * (step / tf.cast(self.warmup_steps, tf.float32))\n",
        "        progress = (step - self.warmup_steps) / tf.maximum(1.0, self.total_steps - self.warmup_steps)\n",
        "        cosine = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + tf.cos(np.pi * tf.clip_by_value(progress, 0.0, 1.0)))\n",
        "        return tf.where(step < self.warmup_steps, warm, cosine)\n",
        "    def get_config(self):\n",
        "        return {\"base_lr\": self.base_lr, \"warmup_steps\": self.warmup_steps, \"total_steps\": self.total_steps, \"min_lr\": self.min_lr}\n",
        "\n",
        "# ===================================================================\n",
        "# 9. 数据验证函数\n",
        "# ===================================================================\n",
        "def check_quaternion_norm(sequences, tolerance=0.01):\n",
        "    if isinstance(sequences, list):\n",
        "        all_norms = np.concatenate([np.linalg.norm(seq[:, :4], axis=1) for seq in sequences])\n",
        "    else:\n",
        "        quat = sequences[:, :, :4] if len(sequences.shape) == 3 else sequences[:, :4]\n",
        "        all_norms = np.linalg.norm(quat, axis=-1).flatten()\n",
        "    stats = {'mean': np.mean(all_norms), 'std': np.std(all_norms), 'min': np.min(all_norms), 'max': np.max(all_norms), 'is_valid': np.all(np.abs(all_norms - 1.0) < tolerance)}\n",
        "    return stats['is_valid'], stats\n",
        "\n",
        "# ===================================================================\n",
        "# 10. K-Fold 交叉验证训练流程\n",
        "# ===================================================================\n",
        "def run_kfold_training(sequences, labels, groups, n_folds=5, **kwargs):\n",
        "    print(\"🚀 开始 K-Fold 训练（简化版 - 仅修复零四元数）...\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\n📊 验证数据的四元数范数（已修复零四元数）...\")\n",
        "    is_valid, stats = check_quaternion_norm(sequences)\n",
        "    print(f\"   平均范数: {stats['mean']:.4f} (±{stats['std']:.4f})\")\n",
        "    print(f\"   范数范围: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
        "\n",
        "    kfold_results = []\n",
        "    sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    sequences_np = np.array(sequences, dtype=object)\n",
        "\n",
        "    for fold_num, (train_idx, val_idx) in enumerate(sgkf.split(sequences_np, labels, groups), 1):\n",
        "        print(f\"\\n{'='*60}\\n🔄 训练 Fold {fold_num}/{n_folds}\\n{'='*60}\")\n",
        "        X_train_raw, X_val_raw = sequences_np[train_idx], sequences_np[val_idx]\n",
        "        y_train, y_val = labels[train_idx], labels[val_idx]\n",
        "\n",
        "        print(f\"📊 Fold {fold_num}: 标准化数据（四元数保持原始值）...\")\n",
        "        X_train_scaled, X_val_scaled, scaler = standardize_sequences_with_quaternion(X_train_raw.tolist(), X_val_raw.tolist(), fit_on_train=True)\n",
        "        is_valid_train, stats_train = check_quaternion_norm(X_train_scaled)\n",
        "        print(f\"   训练集四元数范数: {stats_train['mean']:.4f} (±{stats_train['std']:.4f})\")\n",
        "        scaler_path = f'{OUTPUT_DIR}/fold_{fold_num}_scaler.joblib'\n",
        "        joblib.dump(scaler, scaler_path)\n",
        "        print(f\"💾 保存 Fold {fold_num} 的 StandardScaler 到 {scaler_path}\")\n",
        "\n",
        "        train_ds = create_tf_dataset(X_train_scaled, y_train, kwargs['batch_size'], is_training=True, use_augmentation=True, use_mixup=kwargs['use_mixup'], mixup_alpha=kwargs.get('mixup_alpha', 0.2))\n",
        "        val_ds = create_tf_dataset(X_val_scaled, y_val, kwargs['batch_size'], is_training=False)\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "        model = build_model()\n",
        "        steps_per_epoch = len(X_train_scaled) // kwargs['batch_size']\n",
        "        if steps_per_epoch == 0:\n",
        "            print(f\"错误: Fold {fold_num} 的训练样本数小于batch_size\")\n",
        "            continue\n",
        "\n",
        "        total_steps = steps_per_epoch * kwargs['num_epochs']\n",
        "        lr_sched = WarmupCosine(base_lr=kwargs['learning_rate'], warmup_steps=int(0.05 * total_steps), total_steps=total_steps, min_lr=1e-5)\n",
        "        optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_sched, weight_decay=5e-3)\n",
        "        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=kwargs['label_smoothing'])\n",
        "        model.compile(optimizer=optimizer, loss=loss_fn, metrics=['categorical_accuracy'])\n",
        "\n",
        "        model_save_path = f'{OUTPUT_DIR}/fold_{fold_num}_model.keras'\n",
        "        callbacks = [\n",
        "            CompetitionScoreCallback(validation_data=val_ds),\n",
        "            tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path, save_best_only=True, monitor='val_score', mode='max', verbose=1),\n",
        "            tf.keras.callbacks.EarlyStopping(monitor='val_score', patience=kwargs['patience'], mode='max', verbose=1, restore_best_weights=True),\n",
        "        ]\n",
        "\n",
        "        print(f\"🏋️ 开始训练 Fold {fold_num}...\")\n",
        "        history = model.fit(train_ds, epochs=kwargs['num_epochs'], validation_data=val_ds, callbacks=callbacks, steps_per_epoch=steps_per_epoch, verbose=2)\n",
        "        plot_training_history(history, fold_num)\n",
        "\n",
        "        print(f\"\\n🎯 Fold {fold_num} 最终评估:\")\n",
        "        y_val_unbatched = np.concatenate([y for x, y in val_ds], axis=0)\n",
        "        y_val_labels = np.argmax(y_val_unbatched, axis=1)\n",
        "        val_preds_final = model.predict(val_ds)\n",
        "        val_preds_final_labels = np.argmax(val_preds_final, axis=1)\n",
        "        y_val_labels = y_val_labels[:len(val_preds_final_labels)]\n",
        "\n",
        "        final_score = get_competition_score(y_val_labels, val_preds_final_labels)\n",
        "        final_acc = np.mean(val_preds_final_labels == y_val_labels)\n",
        "        print(f\"       验证集 Accuracy: {final_acc:.4f}\")\n",
        "        print(f\"       验证集 Score: {final_score:.4f}\")\n",
        "\n",
        "        print(f\"\\n📊 Fold {fold_num} - 分类报告:\")\n",
        "        print(classification_report(\n",
        "            y_val_labels,\n",
        "            val_preds_final_labels,\n",
        "            target_names=list(IDX2LABEL.values()),\n",
        "            digits=4\n",
        "        ))\n",
        "\n",
        "        kfold_results.append({'fold': fold_num, 'val_accuracy': final_acc, 'val_score': final_score})\n",
        "\n",
        "    return kfold_results\n",
        "\n",
        "# ===================================================================\n",
        "# 11. 测试集预测函数（简化版 - 不归一化四元数）\n",
        "# ===================================================================\n",
        "def predict_test_data(test_sequences_raw, output_dir=OUTPUT_DIR, n_folds=5):\n",
        "    print(\"🔮 开始测试集预测...\")\n",
        "    all_predictions = []\n",
        "    for fold_num in range(1, n_folds + 1):\n",
        "        print(f\"\\n处理 Fold {fold_num}...\")\n",
        "        scaler_path = f'{output_dir}/fold_{fold_num}_scaler.joblib'\n",
        "        scaler = joblib.load(scaler_path)\n",
        "        print(f\"   加载scaler: {scaler_path}\")\n",
        "        test_quat = [seq[:, :4] for seq in test_sequences_raw]\n",
        "        test_other = [seq[:, 4:] for seq in test_sequences_raw]\n",
        "        test_other_flat = np.vstack(test_other)\n",
        "        test_other_scaled = scaler.transform(test_other_flat)\n",
        "        test_sequences_scaled = []\n",
        "        start = 0\n",
        "        for i, seq in enumerate(test_sequences_raw):\n",
        "            seq_len = len(seq)\n",
        "            quat = test_quat[i]  # 直接使用，不归一化\n",
        "            other_scaled = test_other_scaled[start:start+seq_len]\n",
        "            seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "            test_sequences_scaled.append(seq_scaled)\n",
        "            start += seq_len\n",
        "        model_path = f'{output_dir}/fold_{fold_num}_model.keras'\n",
        "        custom_objects = {'WarmupCosine': WarmupCosine, 'SumPooling1D': SumPooling1D}\n",
        "        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects, compile=False)\n",
        "        print(f\"   加载模型: {model_path}\")\n",
        "        dummy_labels = np.zeros(len(test_sequences_scaled), dtype=np.int32)\n",
        "        test_ds = create_tf_dataset(test_sequences_scaled, dummy_labels, batch_size=64, is_training=False)\n",
        "        fold_predictions = model.predict(test_ds, verbose=0)\n",
        "        all_predictions.append(fold_predictions)\n",
        "        print(f\"   完成Fold {fold_num}的预测\")\n",
        "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
        "    final_predictions = np.argmax(ensemble_predictions, axis=1)\n",
        "    print(f\"\\n✅ 测试集预测完成！\")\n",
        "    return final_predictions, ensemble_predictions\n",
        "\n",
        "# ===================================================================\n",
        "# 12. 主执行逻辑\n",
        "# ===================================================================\n",
        "if __name__ == '__main__':\n",
        "    training_params = {\n",
        "        'num_epochs': 3 if DEBUG else 100,\n",
        "        'learning_rate': 0.001,\n",
        "        'patience': 20,\n",
        "        'batch_size': 64,\n",
        "        'label_smoothing': 0.1,\n",
        "        'use_mixup': True,\n",
        "        'mixup_alpha': 0.4\n",
        "    }\n",
        "\n",
        "    if TRAIN:\n",
        "        results = run_kfold_training(sequences, labels, groups, n_folds=5, **training_params)\n",
        "        print(\"\\n\\n\" + \"=\"*60 + \"\\n🎉 K-Fold 训练总结\\n\" + \"=\"*60)\n",
        "        if results:\n",
        "            val_scores = [r['val_score'] for r in results]\n",
        "            val_accs = [r['val_accuracy'] for r in results]\n",
        "            print(f\"平均验证集 Score: {np.mean(val_scores):.4f} ± {np.std(val_scores):.4f}\")\n",
        "            print(f\"平均验证集 Accuracy: {np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}\")\n",
        "            feature_selection_info = {'all_features': ALL_FEATURE_NAMES, 'selected_features': FEATURE_NAMES, 'feature_indices': FEATURE_INDICES, 'num_features': len(FEATURE_NAMES), 'quaternion_safe': True}\n",
        "            results_with_features = {'kfold_results': results, 'feature_selection': feature_selection_info, 'preprocessing_note': '简化版：仅修复零四元数，不进行阈值归一化', 'lr_schedule': 'WarmupCosine'}\n",
        "            with open(f'{OUTPUT_DIR}/kfold_results.json', 'w') as f:\n",
        "                json.dump(results_with_features, f, indent=2)\n",
        "\n",
        "    print(\"\\n📝 重要说明:\")\n",
        "    print(\"1. 自动检测并修复零四元数（填充为单位四元数）\")\n",
        "    print(\"2. 四元数在标准化时保持原始值，不进行归一化\")\n",
        "    print(\"3. 数据增强后对四元数进行必要的归一化\")\n",
        "    print(\"4. MixUp使用SLERP进行四元数插值\")\n",
        "    print(\"5. 标准化仅应用于非四元数特征\")\n",
        "    print(\"6. 每折训练后会打印详细的分类报告\")\n",
        "\n",
        "# ===================================================================\n",
        "# 13. 生成提交文件\n",
        "# ===================================================================\n",
        "def generate_submission(submission_output_path='submission.csv'):\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n📄 生成提交文件\" + \"\\n\" + \"=\"*60)\n",
        "    print(\"加载预处理的测试数据...\")\n",
        "    test_data = joblib.load(f'{PROCESSED_DATA_DIR}/processed_test_data_raw.joblib')\n",
        "    test_sequences_full = test_data['sequence'].tolist()\n",
        "    test_sequences = [seq[:, FEATURE_INDICES] for seq in test_sequences_full]\n",
        "\n",
        "    # 修复测试集的零四元数\n",
        "    print(\"\\n处理测试集数据...\")\n",
        "    test_sequences = fix_zero_quaternions(test_sequences)\n",
        "\n",
        "    test_sequence_ids = test_data['sequence_id'].values\n",
        "    predictions, _ = predict_test_data(test_sequences, OUTPUT_DIR, n_folds=5)\n",
        "    predicted_labels = [IDX2LABEL[pred] for pred in predictions]\n",
        "    submission_df = pd.DataFrame({'sequence_id': test_sequence_ids, 'gesture': predicted_labels})\n",
        "    submission_df.to_csv(submission_output_path, index=False)\n",
        "    print(f\"✅ 提交文件已保存到: {submission_output_path}\")\n",
        "    print(\"\\n📊 预测分布:\")\n",
        "    print(submission_df['gesture'].value_counts())\n",
        "    return submission_df"
      ],
      "metadata": {
        "id": "VzXPSyiEc3q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#仔细检查修改数据增强和特征工程冲突，修复数据增强后破坏的数据物理特性"
      ],
      "metadata": {
        "id": "tsfwPscz7XCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "#\n",
        "# 完整的 TensorFlow/Keras 训练代码 - CMI 行为检测\n",
        "# se1dcnn+attention模型架构 - 四元数安全版本（简化版：仅修复零四元数）\n",
        "# 10-FOLD交叉验证版本\n",
        "# cv = 0.8104,lb = 0.813\n",
        "# 过拟合已解决\n",
        "#\n",
        "# ===================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Your input ran out of data.*\")\n",
        "\n",
        "# 抑制 TensorFlow 的一些日志输出\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# ===================================================================\n",
        "# 1. 配置与全局设置\n",
        "# ===================================================================\n",
        "DEBUG = False\n",
        "TRAIN = True\n",
        "MAX_SEQ_LENGTH = 128\n",
        "PROCESSED_DATA_DIR = '/kaggle/input/imuonly-process/kaggle/working/processed_data_selected_features_v1' # 请替换为您的实际路径\n",
        "OUTPUT_DIR = './saved_models_keras_fixed_test'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# 从预处理阶段生成的文件中加载标签映射\n",
        "with open(f'{PROCESSED_DATA_DIR}/label_map.json', 'r') as f:\n",
        "    LABEL2IDX = json.load(f)\n",
        "IDX2LABEL = {v: k for k, v in LABEL2IDX.items()}\n",
        "N_CLASSES = len(LABEL2IDX)\n",
        "\n",
        "# 加载特征配置\n",
        "with open(f'{PROCESSED_DATA_DIR}/feature_names.json', 'r') as f:\n",
        "    feature_info = json.load(f)\n",
        "    ALL_FEATURE_NAMES = feature_info['all_features']\n",
        "    TIME_FEATURE_NAMES = feature_info['time_features']\n",
        "    PSD_FEATURE_NAMES = feature_info['psd_features']\n",
        "    STAT_FEATURE_NAMES = feature_info['stat_features']\n",
        "\n",
        "# ===================================================================\n",
        "# 特征选择配置 - 注意前4个必须是四元数\n",
        "# ===================================================================\n",
        "SELECTED_FEATURES = [\n",
        "    'rot_w', 'rot_x', 'rot_y', 'rot_z',           # 四元数\n",
        "    'linear_acc_x', 'linear_acc_y', 'linear_acc_z', # 线性加速度\n",
        "    'linear_acc_mag',                               # 线性加速度模长\n",
        "    'angular_vel_x', 'angular_vel_y', 'angular_vel_z', # 角速度\n",
        "    'angular_distance',                             # 角距离\n",
        "    'acc_mag'                                       # 加速度模长\n",
        "]\n",
        "\n",
        "if SELECTED_FEATURES is not None:\n",
        "    FEATURE_NAMES = SELECTED_FEATURES\n",
        "else:\n",
        "    FEATURE_NAMES = ALL_FEATURE_NAMES\n",
        "\n",
        "FEATURE_INDICES = [ALL_FEATURE_NAMES.index(f) for f in FEATURE_NAMES]\n",
        "\n",
        "print(f\"使用的特征数量: {len(FEATURE_NAMES)}\")\n",
        "print(f\"使用的特征: {FEATURE_NAMES}\")\n",
        "print(f\"前4个特征是四元数: {FEATURE_NAMES[:4]}\")\n",
        "\n",
        "tf.keras.utils.set_random_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ===================================================================\n",
        "# 2. 加载预处理好的数据（原始数据，未标准化）\n",
        "# ===================================================================\n",
        "print(\"\\n加载预处理好的数据（原始特征）...\")\n",
        "agg_train_df = joblib.load(f'{PROCESSED_DATA_DIR}/processed_train_data_raw.joblib')\n",
        "\n",
        "if DEBUG:\n",
        "    agg_train_df = agg_train_df.head(2000)\n",
        "\n",
        "sequences_full = agg_train_df['sequence'].tolist()\n",
        "sequences = [seq[:, FEATURE_INDICES] for seq in sequences_full]\n",
        "\n",
        "# ===================================================================\n",
        "# 修复零四元数（缺失值填充导致的问题）- 保留\n",
        "# ===================================================================\n",
        "def fix_zero_quaternions(sequences):\n",
        "    \"\"\"\n",
        "    修复零四元数，将其替换为单位四元数 [1,0,0,0] (w,x,y,z格式)\n",
        "    \"\"\"\n",
        "    print(\"\\n🔧 检查并修复零四元数...\")\n",
        "    total_frames = 0\n",
        "    zero_frames = 0\n",
        "    problematic_sequences = []\n",
        "\n",
        "    for seq_idx, seq in enumerate(sequences):\n",
        "        # 计算每帧四元数的范数\n",
        "        quat_norms = np.linalg.norm(seq[:, :4], axis=1)\n",
        "        total_frames += len(quat_norms)\n",
        "\n",
        "        # 找出零四元数（范数接近0）\n",
        "        zero_mask = quat_norms < 1e-8\n",
        "        num_zeros = np.sum(zero_mask)\n",
        "\n",
        "        if num_zeros > 0:\n",
        "            # 修复：将零四元数替换为单位四元数 [1,0,0,0]\n",
        "            seq[zero_mask, :4] = [1.0, 0.0, 0.0, 0.0]\n",
        "            zero_frames += num_zeros\n",
        "            problematic_sequences.append(seq_idx)\n",
        "\n",
        "    # 输出统计信息\n",
        "    if zero_frames > 0:\n",
        "        print(f\"   ✅ 修复了 {zero_frames}/{total_frames} 个零四元数帧 \"\n",
        "              f\"({100*zero_frames/total_frames:.2f}%)\")\n",
        "        print(f\"   📊 涉及 {len(problematic_sequences)} 个序列\")\n",
        "    else:\n",
        "        print(f\"   ✅ 未发现零四元数，数据质量良好\")\n",
        "\n",
        "    # 验证修复后的结果\n",
        "    print(\"\\n   验证修复后的四元数范数:\")\n",
        "    all_quats = np.vstack([seq[:, :4] for seq in sequences])\n",
        "    fixed_norms = np.linalg.norm(all_quats, axis=1)\n",
        "    print(f\"     均值: {np.mean(fixed_norms):.6f}\")\n",
        "    print(f\"     标准差: {np.std(fixed_norms):.6f}\")\n",
        "    print(f\"     范围: [{np.min(fixed_norms):.6f}, {np.max(fixed_norms):.6f}]\")\n",
        "\n",
        "    # 检查是否还有问题\n",
        "    remaining_zeros = np.sum(fixed_norms < 1e-8)\n",
        "    if remaining_zeros > 0:\n",
        "        print(f\"   ⚠️ 警告: 仍有 {remaining_zeros} 个零四元数未修复\")\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# 执行修复\n",
        "sequences = fix_zero_quaternions(sequences)\n",
        "\n",
        "labels = agg_train_df['label'].values\n",
        "groups = agg_train_df['subject'].values\n",
        "print(f\"\\n成功加载并处理 {len(sequences)} 条序列数据。\")\n",
        "\n",
        "# ===================================================================\n",
        "# 3. 数据标准化函数（简化版 - 不对四元数进行归一化）\n",
        "# ===================================================================\n",
        "def standardize_sequences_with_quaternion(sequences_train, sequences_val, fit_on_train=True):\n",
        "    train_quat = [seq[:, :4] for seq in sequences_train]\n",
        "    train_other = [seq[:, 4:] for seq in sequences_train]\n",
        "    val_quat = [seq[:, :4] for seq in sequences_val]\n",
        "    val_other = [seq[:, 4:] for seq in sequences_val]\n",
        "\n",
        "    # 标准化非四元数特征\n",
        "    train_other_flat = np.vstack(train_other)\n",
        "    val_other_flat = np.vstack(val_other)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    if fit_on_train:\n",
        "        train_other_scaled = scaler.fit_transform(train_other_flat)\n",
        "        val_other_scaled = scaler.transform(val_other_flat)\n",
        "    else:\n",
        "        all_other_flat = np.vstack([train_other_flat, val_other_flat])\n",
        "        scaler.fit(all_other_flat)\n",
        "        train_other_scaled = scaler.transform(train_other_flat)\n",
        "        val_other_scaled = scaler.transform(val_other_flat)\n",
        "\n",
        "    sequences_train_scaled, sequences_val_scaled = [], []\n",
        "\n",
        "    # 训练集 - 直接使用原始四元数（不归一化）\n",
        "    train_start = 0\n",
        "    for i, seq in enumerate(sequences_train):\n",
        "        seq_len = len(seq)\n",
        "        quat = train_quat[i]  # 直接使用，不归一化\n",
        "        other_scaled = train_other_scaled[train_start:train_start+seq_len]\n",
        "        seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "        sequences_train_scaled.append(seq_scaled)\n",
        "        train_start += seq_len\n",
        "\n",
        "    # 验证集 - 同样处理\n",
        "    val_start = 0\n",
        "    for i, seq in enumerate(sequences_val):\n",
        "        seq_len = len(seq)\n",
        "        quat = val_quat[i]  # 直接使用，不归一化\n",
        "        other_scaled = val_other_scaled[val_start:val_start+seq_len]\n",
        "        seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "        sequences_val_scaled.append(seq_scaled)\n",
        "        val_start += seq_len\n",
        "\n",
        "    return sequences_train_scaled, sequences_val_scaled, scaler\n",
        "\n",
        "# ===================================================================\n",
        "# 4. 四元数工具函数\n",
        "# ===================================================================\n",
        "def normalize_quaternion(quat):\n",
        "    norm = tf.norm(quat, axis=-1, keepdims=True)\n",
        "    norm = tf.maximum(norm, 1e-8)\n",
        "    return quat / norm\n",
        "\n",
        "def quaternion_slerp(q1, q2, t):\n",
        "    dot = tf.reduce_sum(q1 * q2, axis=-1, keepdims=True)\n",
        "    q2 = tf.where(dot < 0, -q2, q2)\n",
        "    dot = tf.abs(dot)\n",
        "    dot = tf.clip_by_value(dot, -1.0, 1.0)\n",
        "    theta = tf.acos(dot)\n",
        "    sin_theta = tf.sin(theta)\n",
        "    w1 = tf.where(sin_theta > 1e-4, tf.sin((1.0 - t) * theta) / sin_theta, 1.0 - t)\n",
        "    w2 = tf.where(sin_theta > 1e-4, tf.sin(t * theta) / sin_theta, t)\n",
        "    result = w1 * q1 + w2 * q2\n",
        "    return normalize_quaternion(result)\n",
        "\n",
        "# ===================================================================\n",
        "# 5. 四元数安全的数据增强（保留增强后的归一化）\n",
        "# ===================================================================\n",
        "def safe_tf_time_stretch(sequence, stretch_range=(0.8, 1.2)):\n",
        "    seq_len_float = tf.cast(tf.shape(sequence)[0], tf.float32)\n",
        "    stretch_factor = tf.random.uniform(shape=(), minval=stretch_range[0], maxval=stretch_range[1])\n",
        "    new_len = tf.cast(seq_len_float / stretch_factor, tf.int32)\n",
        "    quat_features = sequence[:, :4]\n",
        "    other_features = sequence[:, 4:]\n",
        "    quat_reshaped = tf.reshape(quat_features, [1, tf.shape(quat_features)[0], 1, 4])\n",
        "    quat_stretched = tf.image.resize(quat_reshaped, [new_len, 1], method=tf.image.ResizeMethod.BILINEAR)\n",
        "    quat_stretched = tf.reshape(quat_stretched, [new_len, 4])\n",
        "    quat_stretched = normalize_quaternion(quat_stretched)  # 保留：拉伸后需要归一化\n",
        "    if tf.shape(other_features)[1] > 0:\n",
        "        other_reshaped = tf.reshape(other_features, [1, tf.shape(other_features)[0], 1, tf.shape(other_features)[1]])\n",
        "        other_stretched = tf.image.resize(other_reshaped, [new_len, 1], method=tf.image.ResizeMethod.BILINEAR)\n",
        "        other_stretched = tf.reshape(other_stretched, [new_len, tf.shape(other_features)[1]])\n",
        "        stretched_sequence = tf.concat([quat_stretched, other_stretched], axis=1)\n",
        "    else:\n",
        "        stretched_sequence = quat_stretched\n",
        "    return stretched_sequence\n",
        "\n",
        "def safe_tf_augment(sequence, label, aug_prob=0.5):\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        sequence = safe_tf_time_stretch(sequence)\n",
        "        # 重算magnitude\n",
        "        lin_acc_xyz = sequence[:, 4:7]\n",
        "        sequence = tf.concat([\n",
        "            sequence[:, :4],  # 四元数\n",
        "            lin_acc_xyz,\n",
        "            tf.norm(lin_acc_xyz, axis=1, keepdims=True),  # 重算lin_mag\n",
        "            sequence[:, 8:]  # 保留其他特征不变\n",
        "        ], axis=1)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        seq_len = tf.shape(sequence)[0]\n",
        "        max_shift = tf.cast(tf.cast(seq_len, tf.float32) * 0.1, tf.int32)\n",
        "        shift = tf.random.uniform(shape=(), minval=-max_shift, maxval=max_shift, dtype=tf.int32)\n",
        "        sequence = tf.roll(sequence, shift=shift, axis=0)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        quat_features = sequence[:, :4]\n",
        "        other_features = sequence[:, 4:]\n",
        "        quat_noise = tf.random.normal(shape=tf.shape(quat_features), stddev=0.015)\n",
        "        quat_features = normalize_quaternion(quat_features + quat_noise)  # 保留：加噪后需要归一化\n",
        "        if tf.shape(other_features)[1] > 0:\n",
        "            other_noise = tf.random.normal(shape=tf.shape(other_features), stddev=0.03)\n",
        "            other_features = other_features + other_noise\n",
        "            sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "        else:\n",
        "            sequence = quat_features\n",
        "    #if tf.random.uniform(()) < aug_prob:\n",
        "        #quat_features = sequence[:, :4]\n",
        "        #other_features = sequence[:, 4:]\n",
        "        #if tf.shape(other_features)[1] > 0:\n",
        "            #scale_factor = tf.random.uniform(shape=(), minval=0.9, maxval=1.1)\n",
        "            #other_features = other_features * scale_factor\n",
        "            #sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "    if tf.random.uniform(()) < aug_prob:\n",
        "        seq_len = tf.shape(sequence)[0]\n",
        "        mask_ratio = 0.15\n",
        "        mask_length = tf.cast(tf.cast(seq_len, tf.float32) * mask_ratio, tf.int32)\n",
        "        if mask_length > 0:\n",
        "            start_idx = tf.random.uniform(shape=(), maxval=tf.maximum(1, seq_len - mask_length), dtype=tf.int32)\n",
        "            quat_features = sequence[:, :4]\n",
        "            other_features = sequence[:, 4:]\n",
        "            if tf.shape(other_features)[1] > 0:\n",
        "                mask = tf.concat([\n",
        "                    tf.ones([start_idx, tf.shape(other_features)[1]]),\n",
        "                    tf.zeros([mask_length, tf.shape(other_features)[1]]),\n",
        "                    tf.ones([seq_len - start_idx - mask_length, tf.shape(other_features)[1]])\n",
        "                ], axis=0)\n",
        "                other_features = other_features * mask\n",
        "                sequence = tf.concat([quat_features, other_features], axis=1)\n",
        "    return sequence, label\n",
        "\n",
        "# ===================================================================\n",
        "# 6. TensorFlow 数据管道（四元数安全版本）\n",
        "# ===================================================================\n",
        "def create_tf_dataset(X, y, batch_size, is_training=True, use_augmentation=True, use_mixup=False, mixup_alpha=0.2):\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: ((seq, label) for seq, label in zip(X, y)),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, len(FEATURE_NAMES)), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(buffer_size=len(X)).repeat()\n",
        "        if use_augmentation:\n",
        "            dataset = dataset.map(safe_tf_augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.map(lambda seq, label: (seq[:MAX_SEQ_LENGTH], label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.padded_batch(batch_size, padded_shapes=([MAX_SEQ_LENGTH, len(FEATURE_NAMES)], []), padding_values=(0.0, 0), drop_remainder=True)\n",
        "        if use_mixup:\n",
        "            mixup_partner_ds = dataset.shuffle(buffer_size=len(X)//batch_size if batch_size > 0 else 1)\n",
        "            dataset = tf.data.Dataset.zip((dataset, mixup_partner_ds))\n",
        "            def safe_mixup_map(data1, data2):\n",
        "                (seq1, label1), (seq2, label2) = data1, data2\n",
        "                dist = tfp.distributions.Beta(mixup_alpha, mixup_alpha)\n",
        "                lambda_ = dist.sample()\n",
        "                quat1, quat2 = seq1[:, :, :4], seq2[:, :, :4]\n",
        "                other1, other2 = seq1[:, :, 4:], seq2[:, :, 4:]\n",
        "                mixed_quat = quaternion_slerp(quat1, quat2, 1.0 - lambda_)  # SLERP内部会归一化\n",
        "                mixed_other = lambda_ * other1 + (1 - lambda_) * other2\n",
        "                mixed_seq = tf.concat([mixed_quat, mixed_other], axis=-1)\n",
        "                label1_oh = tf.one_hot(tf.cast(label1, tf.int32), N_CLASSES)\n",
        "                label2_oh = tf.one_hot(tf.cast(label2, tf.int32), N_CLASSES)\n",
        "                mixed_label = lambda_ * label1_oh + (1 - lambda_) * label2_oh\n",
        "                return mixed_seq, mixed_label\n",
        "            dataset = dataset.map(safe_mixup_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        else:\n",
        "            dataset = dataset.map(lambda seq, label: (seq, tf.one_hot(tf.cast(label, tf.int32), N_CLASSES)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        dataset = dataset.map(lambda seq, label: (seq[:MAX_SEQ_LENGTH], label), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.padded_batch(batch_size, padded_shapes=([MAX_SEQ_LENGTH, len(FEATURE_NAMES)], []), padding_values=(0.0, 0), drop_remainder=False)\n",
        "        dataset = dataset.map(lambda seq, label: (seq, tf.one_hot(tf.cast(label, tf.int32), N_CLASSES)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ===================================================================\n",
        "# 7. Keras 模型定义\n",
        "# ===================================================================\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class SumPooling1D(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SumPooling1D, self).__init__(**kwargs)\n",
        "    def call(self, inputs):\n",
        "        return tf.reduce_sum(inputs, axis=1)\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[2])\n",
        "    def get_config(self):\n",
        "        base_config = super(SumPooling1D, self).get_config()\n",
        "        return base_config\n",
        "\n",
        "def se_block(input_tensor, reduction=8):\n",
        "    channels = input_tensor.shape[-1]\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(input_tensor)\n",
        "    x = tf.keras.layers.Dense(channels // reduction, activation='relu', use_bias=False)(x)\n",
        "    x = tf.keras.layers.Dense(channels, activation='sigmoid', use_bias=False)(x)\n",
        "    x = tf.keras.layers.Reshape((1, channels))(x)\n",
        "    return tf.keras.layers.Multiply()([input_tensor, x])\n",
        "\n",
        "def residual_se_cnn_block(input_tensor, out_channels, kernel_size, pool_size=2, dropout=0.3, dilation_rate=1):\n",
        "    in_channels = input_tensor.shape[-1]\n",
        "    if in_channels != out_channels:\n",
        "        shortcut = tf.keras.layers.Conv1D(out_channels, 1, use_bias=False)(input_tensor)\n",
        "        shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = input_tensor\n",
        "    x = tf.keras.layers.Conv1D(out_channels, kernel_size, padding='same', use_bias=False)(input_tensor)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv1D(out_channels, kernel_size, padding='same', use_bias=False, dilation_rate=dilation_rate)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = se_block(x)\n",
        "    x = tf.keras.layers.Add()([shortcut, x])\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout)(x)\n",
        "    return x\n",
        "\n",
        "def attention_layer(input_tensor):\n",
        "    x = tf.keras.layers.LayerNormalization(axis=[1, 2])(input_tensor)\n",
        "    scores = tf.keras.layers.Dense(1, activation='tanh', name='attention_scores')(x)\n",
        "    weights = tf.keras.layers.Softmax(axis=1, name='attention_weights')(scores)\n",
        "    context = tf.keras.layers.Multiply()([input_tensor, weights])  # 注意这里用原始输入\n",
        "    context = SumPooling1D()(context)\n",
        "    return context\n",
        "\n",
        "def build_model(imu_dim=len(FEATURE_NAMES), n_classes=N_CLASSES, max_seq_len=MAX_SEQ_LENGTH):\n",
        "    input_layer = tf.keras.layers.Input(shape=(max_seq_len, imu_dim), name='input_sequence')\n",
        "    rot_features = tf.keras.layers.Lambda(lambda x: x[:, :, 0:4], name='rot_features')(input_layer)\n",
        "    acc_features = tf.keras.layers.Lambda(lambda x: x[:, :, 4:8], name='acc_features')(input_layer)\n",
        "    vel_features = tf.keras.layers.Lambda(lambda x: x[:, :, 8:12], name='vel_features')(input_layer)\n",
        "    other_features = tf.keras.layers.Lambda(lambda x: x[:, :, 12:], name='other_features')(input_layer)\n",
        "    rot_branch = residual_se_cnn_block(rot_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    rot_branch = residual_se_cnn_block(rot_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    acc_branch = residual_se_cnn_block(acc_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    acc_branch = residual_se_cnn_block(acc_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    vel_branch = residual_se_cnn_block(vel_features, out_channels=64, kernel_size=3, pool_size=2, dropout=0.25)\n",
        "    vel_branch = residual_se_cnn_block(vel_branch, out_channels=128, kernel_size=3, pool_size=2, dropout=0.3)\n",
        "    if other_features.shape[-1] > 0:\n",
        "        other_branch = residual_se_cnn_block(other_features, out_channels=32, kernel_size=3, pool_size=2, dropout=0.2)\n",
        "        other_branch = residual_se_cnn_block(other_branch, out_channels=64, kernel_size=3, pool_size=2, dropout=0.35)\n",
        "        merged = tf.keras.layers.Concatenate(axis=-1)([rot_branch, acc_branch, vel_branch, other_branch])\n",
        "    else:\n",
        "        merged = tf.keras.layers.Concatenate(axis=-1)([rot_branch, acc_branch, vel_branch])\n",
        "    x = residual_se_cnn_block(merged, out_channels=256, kernel_size=3, pool_size=1, dropout=0.4)\n",
        "    x = residual_se_cnn_block(x, out_channels=512, kernel_size=5, pool_size=1, dropout=0.45)\n",
        "    x = attention_layer(x)\n",
        "    x = tf.keras.layers.Dense(128, use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dropout(0.45)(x)\n",
        "    x = tf.keras.layers.Dense(64, use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dropout(0.25)(x)\n",
        "    output_layer = tf.keras.layers.Dense(n_classes, activation='linear', name='output_logits')(x)\n",
        "    return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# ===================================================================\n",
        "# 8. 训练、回调函数与评估\n",
        "# ===================================================================\n",
        "sys.path.append('/kaggle/usr/lib/cmi_2025_metric_copy_for_import')\n",
        "try:\n",
        "    import cmi_2025_metric_copy_for_import as metric\n",
        "    print(\"成功导入本地评估指标文件。\")\n",
        "    def get_competition_score(true, pred):\n",
        "        true_labels = [IDX2LABEL[x] for x in true]\n",
        "        pred_labels = [IDX2LABEL[x] for x in pred]\n",
        "        true_df = pd.DataFrame({'id': range(len(true_labels)), 'gesture': true_labels})\n",
        "        pred_df = pd.DataFrame({'id': range(len(pred_labels)), 'gesture': pred_labels})\n",
        "        return metric.score(true_df, pred_df, 'id')\n",
        "except ImportError:\n",
        "    print(\"无法导入本地评估指标文件，将使用 accuracy 作为替代。\")\n",
        "    def get_competition_score(true, pred):\n",
        "        return accuracy_score(true, pred)\n",
        "\n",
        "class CompetitionScoreCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        super().__init__()\n",
        "        self.val_data = validation_data\n",
        "        self.val_labels = np.concatenate([y for x, y in validation_data], axis=0)\n",
        "        self.val_labels = np.argmax(self.val_labels, axis=1)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_preds = self.model.predict(self.val_data, verbose=0)\n",
        "        val_preds = np.argmax(val_preds, axis=1)\n",
        "        score = get_competition_score(self.val_labels, val_preds)\n",
        "        print(f\" - val_score: {score:.4f}\", end=\"\")\n",
        "        logs['val_score'] = score\n",
        "\n",
        "def plot_training_history(history, fold):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax1.plot(history.history['loss'], label='Train Loss')\n",
        "    ax1.plot(history.history['val_loss'], label='Val Loss')\n",
        "    ax1.set_title(f'Fold {fold} - Loss'); ax1.set_xlabel('Epoch'); ax1.legend()\n",
        "    ax2.plot(history.history['categorical_accuracy'], label='Train Accuracy')\n",
        "    ax2.plot(history.history['val_categorical_accuracy'], label='Val Accuracy')\n",
        "    ax2.set_title(f'Fold {fold} - Accuracy'); ax2.set_xlabel('Epoch'); ax2.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{OUTPUT_DIR}/fold_{fold}_training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, base_lr, warmup_steps, total_steps, min_lr=1e-5):\n",
        "        super().__init__()\n",
        "        self.base_lr, self.warmup_steps, self.total_steps, self.min_lr = base_lr, warmup_steps, total_steps, min_lr\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warm = self.base_lr * (step / tf.cast(self.warmup_steps, tf.float32))\n",
        "        progress = (step - self.warmup_steps) / tf.maximum(1.0, self.total_steps - self.warmup_steps)\n",
        "        cosine = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + tf.cos(np.pi * tf.clip_by_value(progress, 0.0, 1.0)))\n",
        "        return tf.where(step < self.warmup_steps, warm, cosine)\n",
        "    def get_config(self):\n",
        "        return {\"base_lr\": self.base_lr, \"warmup_steps\": self.warmup_steps, \"total_steps\": self.total_steps, \"min_lr\": self.min_lr}\n",
        "\n",
        "# ===================================================================\n",
        "# 9. 数据验证函数\n",
        "# ===================================================================\n",
        "def check_quaternion_norm(sequences, tolerance=0.01):\n",
        "    if isinstance(sequences, list):\n",
        "        all_norms = np.concatenate([np.linalg.norm(seq[:, :4], axis=1) for seq in sequences])\n",
        "    else:\n",
        "        quat = sequences[:, :, :4] if len(sequences.shape) == 3 else sequences[:, :4]\n",
        "        all_norms = np.linalg.norm(quat, axis=-1).flatten()\n",
        "    stats = {'mean': np.mean(all_norms), 'std': np.std(all_norms), 'min': np.min(all_norms), 'max': np.max(all_norms), 'is_valid': np.all(np.abs(all_norms - 1.0) < tolerance)}\n",
        "    return stats['is_valid'], stats\n",
        "\n",
        "# ===================================================================\n",
        "# 10. K-Fold 交叉验证训练流程\n",
        "# ===================================================================\n",
        "def run_kfold_training(sequences, labels, groups, n_folds=5, **kwargs):\n",
        "    print(f\"🚀 开始 {n_folds}-Fold 训练（简化版 - 仅修复零四元数）...\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\n📊 验证数据的四元数范数（已修复零四元数）...\")\n",
        "    is_valid, stats = check_quaternion_norm(sequences)\n",
        "    print(f\"   平均范数: {stats['mean']:.4f} (±{stats['std']:.4f})\")\n",
        "    print(f\"   范数范围: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
        "\n",
        "    kfold_results = []\n",
        "    sgkf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    sequences_np = np.array(sequences, dtype=object)\n",
        "\n",
        "    for fold_num, (train_idx, val_idx) in enumerate(sgkf.split(sequences_np, labels, groups), 1):\n",
        "        print(f\"\\n{'='*60}\\n🔄 训练 Fold {fold_num}/{n_folds}\\n{'='*60}\")\n",
        "        X_train_raw, X_val_raw = sequences_np[train_idx], sequences_np[val_idx]\n",
        "        y_train, y_val = labels[train_idx], labels[val_idx]\n",
        "\n",
        "        print(f\"📊 Fold {fold_num}: 标准化数据（四元数保持原始值）...\")\n",
        "        X_train_scaled, X_val_scaled, scaler = standardize_sequences_with_quaternion(X_train_raw.tolist(), X_val_raw.tolist(), fit_on_train=True)\n",
        "        is_valid_train, stats_train = check_quaternion_norm(X_train_scaled)\n",
        "        print(f\"   训练集四元数范数: {stats_train['mean']:.4f} (±{stats_train['std']:.4f})\")\n",
        "        scaler_path = f'{OUTPUT_DIR}/fold_{fold_num}_scaler.joblib'\n",
        "        joblib.dump(scaler, scaler_path)\n",
        "        print(f\"💾 保存 Fold {fold_num} 的 StandardScaler 到 {scaler_path}\")\n",
        "\n",
        "        train_ds = create_tf_dataset(X_train_scaled, y_train, kwargs['batch_size'], is_training=True, use_augmentation=True, use_mixup=kwargs['use_mixup'], mixup_alpha=kwargs.get('mixup_alpha', 0.2))\n",
        "        val_ds = create_tf_dataset(X_val_scaled, y_val, kwargs['batch_size'], is_training=False)\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "        model = build_model()\n",
        "        steps_per_epoch = len(X_train_scaled) // kwargs['batch_size']\n",
        "        if steps_per_epoch == 0:\n",
        "            print(f\"错误: Fold {fold_num} 的训练样本数小于batch_size\")\n",
        "            continue\n",
        "\n",
        "        total_steps = steps_per_epoch * kwargs['num_epochs']\n",
        "        lr_sched = WarmupCosine(base_lr=kwargs['learning_rate'], warmup_steps=int(0.05 * total_steps), total_steps=total_steps, min_lr=1e-5)\n",
        "        optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_sched, weight_decay=5e-3)\n",
        "        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=kwargs['label_smoothing'])\n",
        "        model.compile(optimizer=optimizer, loss=loss_fn, metrics=['categorical_accuracy'])\n",
        "\n",
        "        model_save_path = f'{OUTPUT_DIR}/fold_{fold_num}_model.keras'\n",
        "        callbacks = [\n",
        "            CompetitionScoreCallback(validation_data=val_ds),\n",
        "            tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path, save_best_only=True, monitor='val_score', mode='max', verbose=1),\n",
        "            tf.keras.callbacks.EarlyStopping(monitor='val_score', patience=kwargs['patience'], mode='max', verbose=1, restore_best_weights=True),\n",
        "        ]\n",
        "\n",
        "        print(f\"🏋️ 开始训练 Fold {fold_num}...\")\n",
        "        history = model.fit(train_ds, epochs=kwargs['num_epochs'], validation_data=val_ds, callbacks=callbacks, steps_per_epoch=steps_per_epoch, verbose=2)\n",
        "        plot_training_history(history, fold_num)\n",
        "\n",
        "        print(f\"\\n🎯 Fold {fold_num} 最终评估:\")\n",
        "        y_val_unbatched = np.concatenate([y for x, y in val_ds], axis=0)\n",
        "        y_val_labels = np.argmax(y_val_unbatched, axis=1)\n",
        "        val_preds_final = model.predict(val_ds)\n",
        "        val_preds_final_labels = np.argmax(val_preds_final, axis=1)\n",
        "        y_val_labels = y_val_labels[:len(val_preds_final_labels)]\n",
        "\n",
        "        final_score = get_competition_score(y_val_labels, val_preds_final_labels)\n",
        "        final_acc = np.mean(val_preds_final_labels == y_val_labels)\n",
        "        print(f\"       验证集 Accuracy: {final_acc:.4f}\")\n",
        "        print(f\"       验证集 Score: {final_score:.4f}\")\n",
        "\n",
        "        print(f\"\\n📊 Fold {fold_num} - 分类报告:\")\n",
        "        print(classification_report(\n",
        "            y_val_labels,\n",
        "            val_preds_final_labels,\n",
        "            target_names=list(IDX2LABEL.values()),\n",
        "            digits=4\n",
        "        ))\n",
        "\n",
        "        kfold_results.append({'fold': fold_num, 'val_accuracy': final_acc, 'val_score': final_score})\n",
        "\n",
        "    return kfold_results\n",
        "\n",
        "# ===================================================================\n",
        "# 11. 测试集预测函数（简化版 - 不归一化四元数）\n",
        "# ===================================================================\n",
        "def predict_test_data(test_sequences_raw, output_dir=OUTPUT_DIR, n_folds=10):  # 改为10\n",
        "    print(f\"🔮 开始测试集预测 ({n_folds}-fold ensemble)...\")\n",
        "    all_predictions = []\n",
        "    for fold_num in range(1, n_folds + 1):\n",
        "        print(f\"\\n处理 Fold {fold_num}...\")\n",
        "        scaler_path = f'{output_dir}/fold_{fold_num}_scaler.joblib'\n",
        "        scaler = joblib.load(scaler_path)\n",
        "        print(f\"   加载scaler: {scaler_path}\")\n",
        "        test_quat = [seq[:, :4] for seq in test_sequences_raw]\n",
        "        test_other = [seq[:, 4:] for seq in test_sequences_raw]\n",
        "        test_other_flat = np.vstack(test_other)\n",
        "        test_other_scaled = scaler.transform(test_other_flat)\n",
        "        test_sequences_scaled = []\n",
        "        start = 0\n",
        "        for i, seq in enumerate(test_sequences_raw):\n",
        "            seq_len = len(seq)\n",
        "            quat = test_quat[i]  # 直接使用，不归一化\n",
        "            other_scaled = test_other_scaled[start:start+seq_len]\n",
        "            seq_scaled = np.concatenate([quat, other_scaled], axis=1)\n",
        "            test_sequences_scaled.append(seq_scaled)\n",
        "            start += seq_len\n",
        "        model_path = f'{output_dir}/fold_{fold_num}_model.keras'\n",
        "        custom_objects = {'WarmupCosine': WarmupCosine, 'SumPooling1D': SumPooling1D}\n",
        "        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects, compile=False)\n",
        "        print(f\"   加载模型: {model_path}\")\n",
        "        dummy_labels = np.zeros(len(test_sequences_scaled), dtype=np.int32)\n",
        "        test_ds = create_tf_dataset(test_sequences_scaled, dummy_labels, batch_size=64, is_training=False)\n",
        "        fold_predictions = model.predict(test_ds, verbose=0)\n",
        "        all_predictions.append(fold_predictions)\n",
        "        print(f\"   完成Fold {fold_num}的预测\")\n",
        "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
        "    final_predictions = np.argmax(ensemble_predictions, axis=1)\n",
        "    print(f\"\\n✅ 测试集预测完成！\")\n",
        "    return final_predictions, ensemble_predictions\n",
        "\n",
        "# ===================================================================\n",
        "# 12. 主执行逻辑 - 10-FOLD版本\n",
        "# ===================================================================\n",
        "if __name__ == '__main__':\n",
        "    training_params = {\n",
        "        'num_epochs': 3 if DEBUG else 100,\n",
        "        'learning_rate': 0.001,\n",
        "        'patience': 24,\n",
        "        'batch_size': 64,\n",
        "        'label_smoothing': 0.1,\n",
        "        'use_mixup': True,\n",
        "        'mixup_alpha': 0.4\n",
        "    }\n",
        "\n",
        "    if TRAIN:\n",
        "        # 使用10-fold交叉验证\n",
        "        results = run_kfold_training(sequences, labels, groups, n_folds=10, **training_params)\n",
        "        print(\"\\n\\n\" + \"=\"*60 + \"\\n🎉 10-Fold 训练总结\\n\" + \"=\"*60)\n",
        "        if results:\n",
        "            val_scores = [r['val_score'] for r in results]\n",
        "            val_accs = [r['val_accuracy'] for r in results]\n",
        "            print(f\"平均验证集 Score: {np.mean(val_scores):.4f} ± {np.std(val_scores):.4f}\")\n",
        "            print(f\"平均验证集 Accuracy: {np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}\")\n",
        "            print(f\"最佳折 Score: {np.max(val_scores):.4f} (Fold {np.argmax(val_scores)+1})\")\n",
        "            print(f\"最差折 Score: {np.min(val_scores):.4f} (Fold {np.argmin(val_scores)+1})\")\n",
        "\n",
        "            feature_selection_info = {'all_features': ALL_FEATURE_NAMES, 'selected_features': FEATURE_NAMES, 'feature_indices': FEATURE_INDICES, 'num_features': len(FEATURE_NAMES), 'quaternion_safe': True, 'n_folds': 10}\n",
        "            results_with_features = {'kfold_results': results, 'feature_selection': feature_selection_info, 'preprocessing_note': '10-fold CV版本：仅修复零四元数，不进行阈值归一化', 'lr_schedule': 'WarmupCosine'}\n",
        "            with open(f'{OUTPUT_DIR}/kfold_results_10fold.json', 'w') as f:\n",
        "                json.dump(results_with_features, f, indent=2)\n",
        "\n",
        "    print(\"\\n📝 重要说明:\")\n",
        "    print(\"1. 使用10-fold交叉验证提高模型稳定性\")\n",
        "    print(\"2. 自动检测并修复零四元数（填充为单位四元数）\")\n",
        "    print(\"3. 四元数在标准化时保持原始值，不进行归一化\")\n",
        "    print(\"4. 数据增强后对四元数进行必要的归一化\")\n",
        "    print(\"5. MixUp使用SLERP进行四元数插值\")\n",
        "    print(\"6. 标准化仅应用于非四元数特征\")\n",
        "    print(\"7. 每折训练后会打印详细的分类报告\")\n",
        "\n",
        "# ===================================================================\n",
        "# 13. 生成提交文件 - 10-FOLD版本\n",
        "# ===================================================================\n",
        "def generate_submission(submission_output_path='submission.csv'):\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n📄 生成提交文件（10-fold ensemble）\" + \"\\n\" + \"=\"*60)\n",
        "    print(\"加载预处理的测试数据...\")\n",
        "    test_data = joblib.load(f'{PROCESSED_DATA_DIR}/processed_test_data_raw.joblib')\n",
        "    test_sequences_full = test_data['sequence'].tolist()\n",
        "    test_sequences = [seq[:, FEATURE_INDICES] for seq in test_sequences_full]\n",
        "\n",
        "    # 修复测试集的零四元数\n",
        "    print(\"\\n处理测试集数据...\")\n",
        "    test_sequences = fix_zero_quaternions(test_sequences)\n",
        "\n",
        "    test_sequence_ids = test_data['sequence_id'].values\n",
        "    # 使用10-fold ensemble预测\n",
        "    predictions, _ = predict_test_data(test_sequences, OUTPUT_DIR, n_folds=10)\n",
        "    predicted_labels = [IDX2LABEL[pred] for pred in predictions]\n",
        "    submission_df = pd.DataFrame({'sequence_id': test_sequence_ids, 'gesture': predicted_labels})\n",
        "    submission_df.to_csv(submission_output_path, index=False)\n",
        "    print(f\"✅ 提交文件已保存到: {submission_output_path}\")\n",
        "    print(\"\\n📊 预测分布:\")\n",
        "    print(submission_df['gesture'].value_counts())\n",
        "    return submission_df"
      ],
      "metadata": {
        "id": "sk8K9vYYUuy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}